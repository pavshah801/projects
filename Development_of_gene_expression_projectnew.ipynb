{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Development_of_gene_expression_projectnew.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TubqQvhHDAcz",
        "e3HeWPLUnpT_",
        "Di4262Vy_Gq6",
        "4SZw0BY_0tOw",
        "8kA50gnCExnP",
        "V5aIs_GIa2kP",
        "48Xhz-Euz3bd",
        "zkDD4uxNF2RX",
        "D82i11e3HUOU",
        "Lyty5Hd0HF1T",
        "hibQiD9SzX4h",
        "p6LgsZ7Kzlci",
        "XUugRh4Xv4_D",
        "uypysCj3wELK",
        "bY4egth3wELM",
        "n55V7LeqwELN",
        "nkFsMLAldDQ_",
        "AwsjrIZTQRD2",
        "z4cuzmYbQRD_",
        "-euUVB3WQRD_",
        "_ncij0wQQ0Fd",
        "8sp03pCjJ_Wn",
        "yDo7yuF8KGge",
        "kPitm5FJMXC5",
        "75V4AW8xLYxn",
        "rQrmEIL7MbZ-",
        "hQa0AtmiMhQG",
        "aUNrvH1uL0rR",
        "3YXPavDgo47J",
        "CVgx1gzyrfo6",
        "TfZiqVtQYocv",
        "2trVqPTRD4iQ",
        "Bo1I6NH3wSST",
        "6f5DywIewtqv",
        "MY4TsSpJwtqv",
        "1x4sO-sXwtqy",
        "DLVPwX39wtqz",
        "fiAYZ0XRwtqz",
        "kpp2RyXlwtqz",
        "QR02hqz8wtqz",
        "2s9SA4zDGEde",
        "trC3jauLMJTV",
        "Bg8e1GJzL68W",
        "pFop3KfwL68X",
        "GE90RLMIL68a",
        "Ph9TLTksL68b",
        "VvfVTKKHL68b",
        "F0qKnudIL68c",
        "gMSrNpPuL68c",
        "QFspPTh4UK4Z",
        "IFhDJWKEUK4d",
        "ccVK0RCmL68d"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TubqQvhHDAcz"
      },
      "source": [
        "# Libraries and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrjsAKMrg_Eu",
        "outputId": "928bee5e-917f-4a7c-e22f-af8c7f15f9e6"
      },
      "source": [
        "!pip install imblearn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3INu763yiwvO",
        "outputId": "58e129a2-af48-4d5b-e5ec-710dd08fd269"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_selection import SelectKBest,f_classif\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV,RepeatedStratifiedKFold,cross_val_score\n",
        "from sklearn.metrics import log_loss, accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import zipfile\n",
        "pd.set_option('precision', 2)\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.manifold import Isomap\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, LSTM, RepeatVector, TimeDistributed, Embedding\n",
        "from tensorflow.keras.optimizers import Adam, Adadelta\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA3yqiDizc2-"
      },
      "source": [
        "#with zipfile.ZipFile(\"drive/My Drive/dat_res_zip.zip\",\"r\") as dat_res_zip:\n",
        "#    dat_res_zip.extractall()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3HeWPLUnpT_"
      },
      "source": [
        "# Development"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBCMCPoA43xp",
        "outputId": "c84cc42f-60a3-4764-f810-99fa7b2440f9"
      },
      "source": [
        "cnt=int(input('Choose control conditions: input 1-for small groups without healthy,2-for small group with healthy,3-for wide cancer groups with healthy\\n'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Choose control conditions: input 1-for small groups without healthy,2-for small group with healthy,3-for wide cancer groups with healthy\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di4262Vy_Gq6"
      },
      "source": [
        "## Prepare for upload"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmIVVy3rnpUC"
      },
      "source": [
        "if cnt==4: \n",
        " dep_var=pd.read_csv('drive/My Drive/dep_var.csv')\n",
        " dep_var.x.value_counts().head(50)\n",
        " dep_var=dep_var.drop('Unnamed: 0',axis=1)\n",
        " dep_var\n",
        " with zipfile.ZipFile(\"drive/My Drive/dat_zip1.zip\",\"r\") as dat_zip1:\n",
        "    dat_zip1.extractall()\n",
        " with zipfile.ZipFile(\"drive/My Drive/dat_zip2.zip\",\"r\") as dat_zip2:\n",
        "    dat_zip2.extractall()\n",
        " with zipfile.ZipFile(\"drive/My Drive/dat_zip3.zip\",\"r\") as dat_zip3:\n",
        "    dat_zip3.extractall()\n",
        " with zipfile.ZipFile(\"drive/My Drive/dat_zip4.zip\",\"r\") as dat_zip4:\n",
        "    dat_zip4.extractall()\n",
        " with zipfile.ZipFile(\"drive/My Drive/dat_zip5.zip\",\"r\") as dat_zip5:\n",
        "    dat_zip5.extractall()\n",
        " pd.read_csv('49.csv').to_csv(' 49 .csv')\n",
        " pd.read_csv('74.csv').to_csv(' 74 .csv')\n",
        " pd.read_csv(' 49 .csv')\n",
        " dat_res1=pd.DataFrame()\n",
        " for s in range(1,55):\n",
        "  print(s)\n",
        "  dat_res_=0\n",
        "  dat_res_=pd.DataFrame()\n",
        "  dat_res_=pd.read_csv(' '+str(s)+' '+'.csv',index_col='V1').drop(['Unnamed: 0'],axis=1)\n",
        "  dat_res1=pd.concat([dat_res1,dat_res_],axis=0)\n",
        " dat_res1\n",
        " dat_res2=pd.DataFrame()\n",
        " for s in range(55,82):\n",
        "  print(s)\n",
        "  dat_res_=0\n",
        "  dat_res_=pd.DataFrame()\n",
        "  dat_res_=pd.read_csv(' '+str(s)+' '+'.csv',index_col='V1').drop(['Unnamed: 0'],axis=1)\n",
        "  dat_res2=pd.concat([dat_res2,dat_res_],axis=0)\n",
        " dat_res2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SZw0BY_0tOw"
      },
      "source": [
        "## Unzip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_dvh_hO0zSA"
      },
      "source": [
        "# with zipfile.ZipFile(\"drive/My Drive/dat_res_zip.zip\",\"r\") as dat_res_zip:\n",
        "    dat_res_zip.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgEI3sdh-mjO"
      },
      "source": [
        "## Upload dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNgi4qbXzZW9"
      },
      "source": [
        "### Core dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xZx7ARMsBYs",
        "outputId": "8e1af0f9-62dc-41f3-d38f-9a7a89ac59a5"
      },
      "source": [
        "dep_var=pd.read_csv('drive/My Drive/dep_var.csv')\n",
        "dep_var.x.value_counts().head(24)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "normal                                                9450\n",
              "acute myeloid leukaemia                                974\n",
              "chronic lymphocytic leukaemia                          757\n",
              "breast cancer                                          715\n",
              "breast adenocarcinoma                                  567\n",
              "multiple myeloma                                       532\n",
              "diffuse large B-cell lymphoma                          460\n",
              "colorectal adenocarcinoma                              366\n",
              "hepatocellular carcinoma                               362\n",
              "ovarian carcinoma                                      331\n",
              "melanoma                                               321\n",
              "neuroblastoma                                          283\n",
              "lung adenocarcinoma                                    273\n",
              "gastric cancer                                         260\n",
              "Pre-B-ALL/c-ALL                                        253\n",
              "cervical adenocarcinoma                                238\n",
              "multiple sclerosis or clinically isolated syndrome     237\n",
              "asthma                                                 234\n",
              "T-ALL                                                  230\n",
              "periodontitis                                          226\n",
              "NSCLC                                                  219\n",
              "glioblastoma                                           214\n",
              "myeloma                                                208\n",
              "obesity                                                205\n",
              "Name: x, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLm6K-K6yldQ"
      },
      "source": [
        "if cnt==4: \n",
        " dat_res=pd.read_csv('dat_res.csv',index_col='V1')\n",
        " dat_res=dat_res.drop(['Unnamed: 0.1'],axis=1)\n",
        " dat_res=dat_res.astype('float16').T\n",
        " #dat_res=dat_res.drop('Disease',axis=1)\n",
        " dat_res=dat_res.reset_index(drop=True)\n",
        " dat_res['Disease']=dep_var['x']\n",
        " dat_res\n",
        " dat_res.apply('max',axis=0)\n",
        " dat_res.apply('min',axis=0)\n",
        " dat_res.isna().sum().max()\n",
        " dat_res.info()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8P-dMUqEikX"
      },
      "source": [
        "### dataset1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyT_lyBzGR5m"
      },
      "source": [
        "if cnt==4: \n",
        " lsta=list(dep_var.x.value_counts().head(7).index)\n",
        " dat_resa=dat_res.set_index('Disease').loc[lsta].reset_index()\n",
        " dat_resa\n",
        " dat_resn=dat_resa[dat_resa.Disease=='normal']\n",
        " dat_resn\n",
        " dat_resn=dat_resn.loc[[np.random.randint(0,9450) for i in range(1,1001)]]\n",
        " dat_resn\n",
        " dat_resa[dat_resa.Disease!='normal']\n",
        " dat_resa=pd.concat([dat_resn,dat_resa[dat_resa.Disease!='normal']],axis=0)\n",
        " #dat_resa=dat_resa[dat_resa.Disease!='normal']\n",
        " dat_resa\n",
        " dat_resa.to_csv('dat_resa1.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cqVsi5KPBPL"
      },
      "source": [
        "if cnt<=2: \n",
        " dat_resa=pd.read_csv('drive/My Drive/dat_resa1.csv',index_col='Unnamed: 0')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeUcq3-hYQRI"
      },
      "source": [
        "if cnt==1:\n",
        " dat_resa=dat_resa[dat_resa.Disease!='normal']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "C-U6lvxLX6Nc",
        "outputId": "8fb00365-5b59-40e4-9bd7-9727149cc688"
      },
      "source": [
        "dat_resa"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Disease</th>\n",
              "      <th>1007_s_at</th>\n",
              "      <th>1053_at</th>\n",
              "      <th>117_at</th>\n",
              "      <th>121_at</th>\n",
              "      <th>1255_g_at</th>\n",
              "      <th>1294_at</th>\n",
              "      <th>1316_at</th>\n",
              "      <th>1320_at</th>\n",
              "      <th>1405_i_at</th>\n",
              "      <th>1431_at</th>\n",
              "      <th>1438_at</th>\n",
              "      <th>1487_at</th>\n",
              "      <th>1494_f_at</th>\n",
              "      <th>1552256_a_at</th>\n",
              "      <th>1552257_a_at</th>\n",
              "      <th>1552258_at</th>\n",
              "      <th>1552261_at</th>\n",
              "      <th>1552263_at</th>\n",
              "      <th>1552264_a_at</th>\n",
              "      <th>1552266_at</th>\n",
              "      <th>1552269_at</th>\n",
              "      <th>1552271_at</th>\n",
              "      <th>1552272_a_at</th>\n",
              "      <th>1552274_at</th>\n",
              "      <th>1552275_s_at</th>\n",
              "      <th>1552276_a_at</th>\n",
              "      <th>1552277_a_at</th>\n",
              "      <th>1552278_a_at</th>\n",
              "      <th>1552279_a_at</th>\n",
              "      <th>1552280_at</th>\n",
              "      <th>1552281_at</th>\n",
              "      <th>1552283_s_at</th>\n",
              "      <th>1552286_at</th>\n",
              "      <th>1552287_s_at</th>\n",
              "      <th>1552288_at</th>\n",
              "      <th>1552289_a_at</th>\n",
              "      <th>1552291_at</th>\n",
              "      <th>1552293_at</th>\n",
              "      <th>1552295_a_at</th>\n",
              "      <th>...</th>\n",
              "      <th>244709_at</th>\n",
              "      <th>244710_at</th>\n",
              "      <th>244711_at</th>\n",
              "      <th>244712_at</th>\n",
              "      <th>244713_at</th>\n",
              "      <th>244714_at</th>\n",
              "      <th>244715_at</th>\n",
              "      <th>244716_x_at</th>\n",
              "      <th>244717_x_at</th>\n",
              "      <th>244718_at</th>\n",
              "      <th>244719_at</th>\n",
              "      <th>244720_at</th>\n",
              "      <th>244721_at</th>\n",
              "      <th>244722_at</th>\n",
              "      <th>244723_at</th>\n",
              "      <th>244724_at</th>\n",
              "      <th>244725_at</th>\n",
              "      <th>244726_at</th>\n",
              "      <th>244727_at</th>\n",
              "      <th>244728_at</th>\n",
              "      <th>244729_at</th>\n",
              "      <th>244730_x_at</th>\n",
              "      <th>244731_at</th>\n",
              "      <th>244732_at</th>\n",
              "      <th>244733_at</th>\n",
              "      <th>244734_at</th>\n",
              "      <th>244735_at</th>\n",
              "      <th>244736_at</th>\n",
              "      <th>244737_at</th>\n",
              "      <th>244738_at</th>\n",
              "      <th>244739_at</th>\n",
              "      <th>244740_at</th>\n",
              "      <th>244741_s_at</th>\n",
              "      <th>244742_at</th>\n",
              "      <th>244743_x_at</th>\n",
              "      <th>244744_at</th>\n",
              "      <th>244745_at</th>\n",
              "      <th>244746_at</th>\n",
              "      <th>244747_at</th>\n",
              "      <th>244748_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9450</th>\n",
              "      <td>acute myeloid leukaemia</td>\n",
              "      <td>7.17</td>\n",
              "      <td>6.51</td>\n",
              "      <td>6.52</td>\n",
              "      <td>8.02</td>\n",
              "      <td>3.42</td>\n",
              "      <td>8.77</td>\n",
              "      <td>6.21</td>\n",
              "      <td>4.02</td>\n",
              "      <td>10.20</td>\n",
              "      <td>4.40</td>\n",
              "      <td>5.56</td>\n",
              "      <td>7.71</td>\n",
              "      <td>5.82</td>\n",
              "      <td>8.62</td>\n",
              "      <td>8.54</td>\n",
              "      <td>4.02</td>\n",
              "      <td>4.45</td>\n",
              "      <td>7.70</td>\n",
              "      <td>7.87</td>\n",
              "      <td>3.96</td>\n",
              "      <td>3.47</td>\n",
              "      <td>6.70</td>\n",
              "      <td>6.84</td>\n",
              "      <td>8.20</td>\n",
              "      <td>9.16</td>\n",
              "      <td>5.77</td>\n",
              "      <td>8.16</td>\n",
              "      <td>4.43</td>\n",
              "      <td>6.91</td>\n",
              "      <td>4.57</td>\n",
              "      <td>7.01</td>\n",
              "      <td>4.49</td>\n",
              "      <td>5.64</td>\n",
              "      <td>6.18</td>\n",
              "      <td>4.55</td>\n",
              "      <td>6.81</td>\n",
              "      <td>5.86</td>\n",
              "      <td>4.28</td>\n",
              "      <td>7.23</td>\n",
              "      <td>...</td>\n",
              "      <td>5.80</td>\n",
              "      <td>4.12</td>\n",
              "      <td>6.14</td>\n",
              "      <td>5.26</td>\n",
              "      <td>6.19</td>\n",
              "      <td>3.54</td>\n",
              "      <td>4.43</td>\n",
              "      <td>7.29</td>\n",
              "      <td>4.85</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.16</td>\n",
              "      <td>3.10</td>\n",
              "      <td>5.73</td>\n",
              "      <td>4.01</td>\n",
              "      <td>3.43</td>\n",
              "      <td>3.44</td>\n",
              "      <td>5.10</td>\n",
              "      <td>5.22</td>\n",
              "      <td>4.04</td>\n",
              "      <td>4.30</td>\n",
              "      <td>5.12</td>\n",
              "      <td>5.17</td>\n",
              "      <td>2.97</td>\n",
              "      <td>5.80</td>\n",
              "      <td>4.36</td>\n",
              "      <td>3.48</td>\n",
              "      <td>3.63</td>\n",
              "      <td>3.40</td>\n",
              "      <td>4.42</td>\n",
              "      <td>5.61</td>\n",
              "      <td>3.46</td>\n",
              "      <td>6.72</td>\n",
              "      <td>5.91</td>\n",
              "      <td>3.35</td>\n",
              "      <td>5.47</td>\n",
              "      <td>4.82</td>\n",
              "      <td>3.28</td>\n",
              "      <td>4.79</td>\n",
              "      <td>3.94</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9451</th>\n",
              "      <td>acute myeloid leukaemia</td>\n",
              "      <td>6.64</td>\n",
              "      <td>6.50</td>\n",
              "      <td>5.81</td>\n",
              "      <td>8.41</td>\n",
              "      <td>3.42</td>\n",
              "      <td>8.48</td>\n",
              "      <td>5.34</td>\n",
              "      <td>3.99</td>\n",
              "      <td>6.50</td>\n",
              "      <td>3.74</td>\n",
              "      <td>5.52</td>\n",
              "      <td>7.66</td>\n",
              "      <td>5.94</td>\n",
              "      <td>8.56</td>\n",
              "      <td>8.50</td>\n",
              "      <td>4.07</td>\n",
              "      <td>4.47</td>\n",
              "      <td>7.27</td>\n",
              "      <td>7.97</td>\n",
              "      <td>3.34</td>\n",
              "      <td>3.51</td>\n",
              "      <td>5.81</td>\n",
              "      <td>5.54</td>\n",
              "      <td>9.95</td>\n",
              "      <td>9.58</td>\n",
              "      <td>6.36</td>\n",
              "      <td>6.82</td>\n",
              "      <td>4.46</td>\n",
              "      <td>6.94</td>\n",
              "      <td>4.23</td>\n",
              "      <td>7.64</td>\n",
              "      <td>4.21</td>\n",
              "      <td>6.05</td>\n",
              "      <td>6.80</td>\n",
              "      <td>3.40</td>\n",
              "      <td>4.70</td>\n",
              "      <td>4.83</td>\n",
              "      <td>4.43</td>\n",
              "      <td>7.32</td>\n",
              "      <td>...</td>\n",
              "      <td>5.38</td>\n",
              "      <td>4.38</td>\n",
              "      <td>5.54</td>\n",
              "      <td>4.57</td>\n",
              "      <td>6.29</td>\n",
              "      <td>3.55</td>\n",
              "      <td>3.98</td>\n",
              "      <td>6.98</td>\n",
              "      <td>4.05</td>\n",
              "      <td>4.61</td>\n",
              "      <td>4.50</td>\n",
              "      <td>3.20</td>\n",
              "      <td>7.07</td>\n",
              "      <td>4.39</td>\n",
              "      <td>3.24</td>\n",
              "      <td>3.70</td>\n",
              "      <td>4.32</td>\n",
              "      <td>7.95</td>\n",
              "      <td>3.69</td>\n",
              "      <td>4.36</td>\n",
              "      <td>4.54</td>\n",
              "      <td>5.09</td>\n",
              "      <td>3.52</td>\n",
              "      <td>4.91</td>\n",
              "      <td>5.41</td>\n",
              "      <td>3.18</td>\n",
              "      <td>3.89</td>\n",
              "      <td>3.27</td>\n",
              "      <td>5.32</td>\n",
              "      <td>5.45</td>\n",
              "      <td>3.35</td>\n",
              "      <td>7.32</td>\n",
              "      <td>7.70</td>\n",
              "      <td>3.44</td>\n",
              "      <td>6.19</td>\n",
              "      <td>7.11</td>\n",
              "      <td>3.55</td>\n",
              "      <td>4.06</td>\n",
              "      <td>3.73</td>\n",
              "      <td>3.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9452</th>\n",
              "      <td>acute myeloid leukaemia</td>\n",
              "      <td>7.11</td>\n",
              "      <td>6.75</td>\n",
              "      <td>6.72</td>\n",
              "      <td>8.36</td>\n",
              "      <td>3.50</td>\n",
              "      <td>8.77</td>\n",
              "      <td>6.06</td>\n",
              "      <td>3.83</td>\n",
              "      <td>10.02</td>\n",
              "      <td>3.52</td>\n",
              "      <td>5.51</td>\n",
              "      <td>7.16</td>\n",
              "      <td>5.54</td>\n",
              "      <td>7.75</td>\n",
              "      <td>7.86</td>\n",
              "      <td>5.36</td>\n",
              "      <td>4.39</td>\n",
              "      <td>7.16</td>\n",
              "      <td>6.92</td>\n",
              "      <td>3.49</td>\n",
              "      <td>3.61</td>\n",
              "      <td>5.91</td>\n",
              "      <td>5.50</td>\n",
              "      <td>8.96</td>\n",
              "      <td>8.55</td>\n",
              "      <td>5.52</td>\n",
              "      <td>7.38</td>\n",
              "      <td>4.39</td>\n",
              "      <td>6.89</td>\n",
              "      <td>4.07</td>\n",
              "      <td>7.34</td>\n",
              "      <td>4.20</td>\n",
              "      <td>6.09</td>\n",
              "      <td>6.22</td>\n",
              "      <td>3.41</td>\n",
              "      <td>4.79</td>\n",
              "      <td>5.65</td>\n",
              "      <td>4.41</td>\n",
              "      <td>6.93</td>\n",
              "      <td>...</td>\n",
              "      <td>5.42</td>\n",
              "      <td>4.04</td>\n",
              "      <td>5.61</td>\n",
              "      <td>5.25</td>\n",
              "      <td>5.73</td>\n",
              "      <td>3.92</td>\n",
              "      <td>4.98</td>\n",
              "      <td>6.55</td>\n",
              "      <td>4.62</td>\n",
              "      <td>4.40</td>\n",
              "      <td>4.78</td>\n",
              "      <td>3.26</td>\n",
              "      <td>5.41</td>\n",
              "      <td>4.12</td>\n",
              "      <td>3.38</td>\n",
              "      <td>3.69</td>\n",
              "      <td>4.30</td>\n",
              "      <td>8.90</td>\n",
              "      <td>4.25</td>\n",
              "      <td>4.73</td>\n",
              "      <td>5.21</td>\n",
              "      <td>4.90</td>\n",
              "      <td>3.51</td>\n",
              "      <td>5.31</td>\n",
              "      <td>4.61</td>\n",
              "      <td>3.62</td>\n",
              "      <td>3.37</td>\n",
              "      <td>3.98</td>\n",
              "      <td>5.02</td>\n",
              "      <td>5.80</td>\n",
              "      <td>3.56</td>\n",
              "      <td>5.72</td>\n",
              "      <td>5.34</td>\n",
              "      <td>3.07</td>\n",
              "      <td>4.47</td>\n",
              "      <td>4.89</td>\n",
              "      <td>3.26</td>\n",
              "      <td>4.46</td>\n",
              "      <td>3.83</td>\n",
              "      <td>3.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9453</th>\n",
              "      <td>acute myeloid leukaemia</td>\n",
              "      <td>6.77</td>\n",
              "      <td>7.24</td>\n",
              "      <td>6.37</td>\n",
              "      <td>7.71</td>\n",
              "      <td>3.62</td>\n",
              "      <td>8.91</td>\n",
              "      <td>5.52</td>\n",
              "      <td>3.87</td>\n",
              "      <td>6.21</td>\n",
              "      <td>4.84</td>\n",
              "      <td>5.38</td>\n",
              "      <td>7.52</td>\n",
              "      <td>5.72</td>\n",
              "      <td>8.13</td>\n",
              "      <td>8.43</td>\n",
              "      <td>4.31</td>\n",
              "      <td>4.12</td>\n",
              "      <td>7.97</td>\n",
              "      <td>8.15</td>\n",
              "      <td>3.45</td>\n",
              "      <td>3.72</td>\n",
              "      <td>5.30</td>\n",
              "      <td>5.47</td>\n",
              "      <td>9.17</td>\n",
              "      <td>8.59</td>\n",
              "      <td>5.85</td>\n",
              "      <td>6.21</td>\n",
              "      <td>4.16</td>\n",
              "      <td>6.29</td>\n",
              "      <td>4.14</td>\n",
              "      <td>6.96</td>\n",
              "      <td>4.16</td>\n",
              "      <td>5.52</td>\n",
              "      <td>6.73</td>\n",
              "      <td>3.56</td>\n",
              "      <td>5.41</td>\n",
              "      <td>6.08</td>\n",
              "      <td>4.10</td>\n",
              "      <td>6.04</td>\n",
              "      <td>...</td>\n",
              "      <td>5.42</td>\n",
              "      <td>3.86</td>\n",
              "      <td>5.59</td>\n",
              "      <td>4.79</td>\n",
              "      <td>5.93</td>\n",
              "      <td>3.53</td>\n",
              "      <td>4.55</td>\n",
              "      <td>7.87</td>\n",
              "      <td>4.46</td>\n",
              "      <td>4.59</td>\n",
              "      <td>4.55</td>\n",
              "      <td>3.38</td>\n",
              "      <td>5.71</td>\n",
              "      <td>3.88</td>\n",
              "      <td>3.59</td>\n",
              "      <td>3.60</td>\n",
              "      <td>4.18</td>\n",
              "      <td>9.39</td>\n",
              "      <td>4.35</td>\n",
              "      <td>5.19</td>\n",
              "      <td>4.66</td>\n",
              "      <td>4.85</td>\n",
              "      <td>3.35</td>\n",
              "      <td>5.09</td>\n",
              "      <td>5.34</td>\n",
              "      <td>3.85</td>\n",
              "      <td>4.30</td>\n",
              "      <td>3.68</td>\n",
              "      <td>5.07</td>\n",
              "      <td>4.84</td>\n",
              "      <td>3.70</td>\n",
              "      <td>6.46</td>\n",
              "      <td>6.50</td>\n",
              "      <td>3.76</td>\n",
              "      <td>5.86</td>\n",
              "      <td>4.16</td>\n",
              "      <td>3.33</td>\n",
              "      <td>4.40</td>\n",
              "      <td>3.75</td>\n",
              "      <td>4.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9454</th>\n",
              "      <td>acute myeloid leukaemia</td>\n",
              "      <td>7.23</td>\n",
              "      <td>8.17</td>\n",
              "      <td>5.84</td>\n",
              "      <td>8.21</td>\n",
              "      <td>3.52</td>\n",
              "      <td>9.05</td>\n",
              "      <td>5.52</td>\n",
              "      <td>3.85</td>\n",
              "      <td>8.59</td>\n",
              "      <td>3.82</td>\n",
              "      <td>5.29</td>\n",
              "      <td>7.20</td>\n",
              "      <td>5.64</td>\n",
              "      <td>8.50</td>\n",
              "      <td>8.66</td>\n",
              "      <td>5.02</td>\n",
              "      <td>4.21</td>\n",
              "      <td>6.22</td>\n",
              "      <td>7.18</td>\n",
              "      <td>3.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>5.96</td>\n",
              "      <td>6.00</td>\n",
              "      <td>8.74</td>\n",
              "      <td>7.62</td>\n",
              "      <td>5.39</td>\n",
              "      <td>6.45</td>\n",
              "      <td>4.05</td>\n",
              "      <td>6.76</td>\n",
              "      <td>4.34</td>\n",
              "      <td>7.32</td>\n",
              "      <td>3.95</td>\n",
              "      <td>6.14</td>\n",
              "      <td>6.18</td>\n",
              "      <td>3.55</td>\n",
              "      <td>5.83</td>\n",
              "      <td>5.92</td>\n",
              "      <td>4.20</td>\n",
              "      <td>7.07</td>\n",
              "      <td>...</td>\n",
              "      <td>5.13</td>\n",
              "      <td>4.02</td>\n",
              "      <td>5.68</td>\n",
              "      <td>5.05</td>\n",
              "      <td>5.96</td>\n",
              "      <td>3.65</td>\n",
              "      <td>3.97</td>\n",
              "      <td>8.19</td>\n",
              "      <td>4.32</td>\n",
              "      <td>4.86</td>\n",
              "      <td>4.61</td>\n",
              "      <td>3.67</td>\n",
              "      <td>5.50</td>\n",
              "      <td>3.79</td>\n",
              "      <td>3.60</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.78</td>\n",
              "      <td>10.12</td>\n",
              "      <td>3.74</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.07</td>\n",
              "      <td>5.68</td>\n",
              "      <td>3.84</td>\n",
              "      <td>4.94</td>\n",
              "      <td>5.69</td>\n",
              "      <td>3.37</td>\n",
              "      <td>4.10</td>\n",
              "      <td>3.29</td>\n",
              "      <td>5.25</td>\n",
              "      <td>5.25</td>\n",
              "      <td>3.44</td>\n",
              "      <td>6.04</td>\n",
              "      <td>5.77</td>\n",
              "      <td>3.43</td>\n",
              "      <td>6.06</td>\n",
              "      <td>5.33</td>\n",
              "      <td>3.35</td>\n",
              "      <td>3.73</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13450</th>\n",
              "      <td>diffuse large B-cell lymphoma</td>\n",
              "      <td>7.12</td>\n",
              "      <td>6.54</td>\n",
              "      <td>7.12</td>\n",
              "      <td>8.17</td>\n",
              "      <td>3.43</td>\n",
              "      <td>8.11</td>\n",
              "      <td>5.62</td>\n",
              "      <td>4.02</td>\n",
              "      <td>9.48</td>\n",
              "      <td>4.00</td>\n",
              "      <td>5.18</td>\n",
              "      <td>7.31</td>\n",
              "      <td>5.56</td>\n",
              "      <td>8.64</td>\n",
              "      <td>7.75</td>\n",
              "      <td>4.84</td>\n",
              "      <td>4.34</td>\n",
              "      <td>7.04</td>\n",
              "      <td>7.14</td>\n",
              "      <td>3.44</td>\n",
              "      <td>3.43</td>\n",
              "      <td>5.59</td>\n",
              "      <td>5.44</td>\n",
              "      <td>8.73</td>\n",
              "      <td>7.94</td>\n",
              "      <td>5.48</td>\n",
              "      <td>6.60</td>\n",
              "      <td>4.54</td>\n",
              "      <td>6.44</td>\n",
              "      <td>7.09</td>\n",
              "      <td>6.90</td>\n",
              "      <td>4.36</td>\n",
              "      <td>5.62</td>\n",
              "      <td>7.28</td>\n",
              "      <td>3.81</td>\n",
              "      <td>4.77</td>\n",
              "      <td>4.93</td>\n",
              "      <td>4.43</td>\n",
              "      <td>6.76</td>\n",
              "      <td>...</td>\n",
              "      <td>5.61</td>\n",
              "      <td>4.01</td>\n",
              "      <td>5.44</td>\n",
              "      <td>4.79</td>\n",
              "      <td>6.07</td>\n",
              "      <td>3.75</td>\n",
              "      <td>4.00</td>\n",
              "      <td>6.95</td>\n",
              "      <td>4.37</td>\n",
              "      <td>4.42</td>\n",
              "      <td>3.84</td>\n",
              "      <td>3.10</td>\n",
              "      <td>5.92</td>\n",
              "      <td>4.24</td>\n",
              "      <td>3.42</td>\n",
              "      <td>3.69</td>\n",
              "      <td>4.34</td>\n",
              "      <td>5.52</td>\n",
              "      <td>4.07</td>\n",
              "      <td>4.61</td>\n",
              "      <td>5.04</td>\n",
              "      <td>5.00</td>\n",
              "      <td>3.31</td>\n",
              "      <td>5.22</td>\n",
              "      <td>4.66</td>\n",
              "      <td>3.52</td>\n",
              "      <td>3.71</td>\n",
              "      <td>3.38</td>\n",
              "      <td>4.84</td>\n",
              "      <td>5.54</td>\n",
              "      <td>3.63</td>\n",
              "      <td>6.22</td>\n",
              "      <td>6.11</td>\n",
              "      <td>3.25</td>\n",
              "      <td>5.55</td>\n",
              "      <td>4.82</td>\n",
              "      <td>3.81</td>\n",
              "      <td>4.32</td>\n",
              "      <td>4.12</td>\n",
              "      <td>3.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13451</th>\n",
              "      <td>diffuse large B-cell lymphoma</td>\n",
              "      <td>7.09</td>\n",
              "      <td>7.85</td>\n",
              "      <td>6.21</td>\n",
              "      <td>7.62</td>\n",
              "      <td>3.91</td>\n",
              "      <td>7.32</td>\n",
              "      <td>5.42</td>\n",
              "      <td>3.83</td>\n",
              "      <td>10.44</td>\n",
              "      <td>3.52</td>\n",
              "      <td>4.84</td>\n",
              "      <td>7.77</td>\n",
              "      <td>5.48</td>\n",
              "      <td>8.59</td>\n",
              "      <td>7.91</td>\n",
              "      <td>3.92</td>\n",
              "      <td>4.14</td>\n",
              "      <td>7.70</td>\n",
              "      <td>7.60</td>\n",
              "      <td>3.35</td>\n",
              "      <td>3.31</td>\n",
              "      <td>5.36</td>\n",
              "      <td>4.70</td>\n",
              "      <td>8.48</td>\n",
              "      <td>7.73</td>\n",
              "      <td>5.23</td>\n",
              "      <td>7.85</td>\n",
              "      <td>4.61</td>\n",
              "      <td>6.19</td>\n",
              "      <td>9.29</td>\n",
              "      <td>6.75</td>\n",
              "      <td>4.03</td>\n",
              "      <td>6.76</td>\n",
              "      <td>6.28</td>\n",
              "      <td>3.48</td>\n",
              "      <td>4.61</td>\n",
              "      <td>6.24</td>\n",
              "      <td>4.43</td>\n",
              "      <td>6.50</td>\n",
              "      <td>...</td>\n",
              "      <td>5.28</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.82</td>\n",
              "      <td>4.58</td>\n",
              "      <td>5.48</td>\n",
              "      <td>3.54</td>\n",
              "      <td>4.37</td>\n",
              "      <td>6.64</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.21</td>\n",
              "      <td>3.97</td>\n",
              "      <td>2.93</td>\n",
              "      <td>5.47</td>\n",
              "      <td>4.41</td>\n",
              "      <td>3.16</td>\n",
              "      <td>3.63</td>\n",
              "      <td>4.76</td>\n",
              "      <td>4.61</td>\n",
              "      <td>3.80</td>\n",
              "      <td>4.33</td>\n",
              "      <td>4.62</td>\n",
              "      <td>5.40</td>\n",
              "      <td>3.12</td>\n",
              "      <td>5.11</td>\n",
              "      <td>4.30</td>\n",
              "      <td>3.47</td>\n",
              "      <td>3.45</td>\n",
              "      <td>3.22</td>\n",
              "      <td>4.51</td>\n",
              "      <td>5.50</td>\n",
              "      <td>3.48</td>\n",
              "      <td>6.04</td>\n",
              "      <td>6.19</td>\n",
              "      <td>3.12</td>\n",
              "      <td>6.68</td>\n",
              "      <td>4.32</td>\n",
              "      <td>6.28</td>\n",
              "      <td>4.32</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13452</th>\n",
              "      <td>diffuse large B-cell lymphoma</td>\n",
              "      <td>8.18</td>\n",
              "      <td>8.74</td>\n",
              "      <td>5.04</td>\n",
              "      <td>7.25</td>\n",
              "      <td>3.69</td>\n",
              "      <td>7.88</td>\n",
              "      <td>5.48</td>\n",
              "      <td>3.73</td>\n",
              "      <td>8.38</td>\n",
              "      <td>3.70</td>\n",
              "      <td>4.71</td>\n",
              "      <td>6.89</td>\n",
              "      <td>5.28</td>\n",
              "      <td>6.72</td>\n",
              "      <td>7.15</td>\n",
              "      <td>4.03</td>\n",
              "      <td>4.33</td>\n",
              "      <td>6.64</td>\n",
              "      <td>7.31</td>\n",
              "      <td>3.44</td>\n",
              "      <td>3.37</td>\n",
              "      <td>4.95</td>\n",
              "      <td>4.82</td>\n",
              "      <td>9.95</td>\n",
              "      <td>9.34</td>\n",
              "      <td>5.08</td>\n",
              "      <td>8.09</td>\n",
              "      <td>4.42</td>\n",
              "      <td>6.11</td>\n",
              "      <td>6.05</td>\n",
              "      <td>6.04</td>\n",
              "      <td>3.68</td>\n",
              "      <td>5.73</td>\n",
              "      <td>7.47</td>\n",
              "      <td>3.54</td>\n",
              "      <td>4.70</td>\n",
              "      <td>5.23</td>\n",
              "      <td>4.57</td>\n",
              "      <td>6.25</td>\n",
              "      <td>...</td>\n",
              "      <td>4.73</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.48</td>\n",
              "      <td>4.38</td>\n",
              "      <td>5.31</td>\n",
              "      <td>3.69</td>\n",
              "      <td>4.04</td>\n",
              "      <td>5.95</td>\n",
              "      <td>4.62</td>\n",
              "      <td>4.61</td>\n",
              "      <td>4.32</td>\n",
              "      <td>3.11</td>\n",
              "      <td>5.25</td>\n",
              "      <td>4.45</td>\n",
              "      <td>3.35</td>\n",
              "      <td>3.52</td>\n",
              "      <td>4.24</td>\n",
              "      <td>4.90</td>\n",
              "      <td>3.87</td>\n",
              "      <td>4.39</td>\n",
              "      <td>4.53</td>\n",
              "      <td>5.93</td>\n",
              "      <td>3.46</td>\n",
              "      <td>5.16</td>\n",
              "      <td>5.22</td>\n",
              "      <td>3.49</td>\n",
              "      <td>3.48</td>\n",
              "      <td>3.31</td>\n",
              "      <td>4.84</td>\n",
              "      <td>5.04</td>\n",
              "      <td>3.54</td>\n",
              "      <td>6.80</td>\n",
              "      <td>6.91</td>\n",
              "      <td>3.23</td>\n",
              "      <td>7.43</td>\n",
              "      <td>4.29</td>\n",
              "      <td>3.85</td>\n",
              "      <td>4.48</td>\n",
              "      <td>3.79</td>\n",
              "      <td>4.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13453</th>\n",
              "      <td>diffuse large B-cell lymphoma</td>\n",
              "      <td>7.02</td>\n",
              "      <td>6.83</td>\n",
              "      <td>5.20</td>\n",
              "      <td>7.64</td>\n",
              "      <td>6.78</td>\n",
              "      <td>7.60</td>\n",
              "      <td>5.41</td>\n",
              "      <td>4.02</td>\n",
              "      <td>9.30</td>\n",
              "      <td>3.63</td>\n",
              "      <td>5.02</td>\n",
              "      <td>7.68</td>\n",
              "      <td>5.69</td>\n",
              "      <td>8.23</td>\n",
              "      <td>8.52</td>\n",
              "      <td>3.82</td>\n",
              "      <td>4.26</td>\n",
              "      <td>6.95</td>\n",
              "      <td>7.71</td>\n",
              "      <td>3.34</td>\n",
              "      <td>3.35</td>\n",
              "      <td>5.49</td>\n",
              "      <td>5.39</td>\n",
              "      <td>8.52</td>\n",
              "      <td>7.89</td>\n",
              "      <td>5.09</td>\n",
              "      <td>7.61</td>\n",
              "      <td>4.14</td>\n",
              "      <td>6.21</td>\n",
              "      <td>4.74</td>\n",
              "      <td>6.73</td>\n",
              "      <td>3.68</td>\n",
              "      <td>5.59</td>\n",
              "      <td>7.55</td>\n",
              "      <td>3.51</td>\n",
              "      <td>4.54</td>\n",
              "      <td>6.46</td>\n",
              "      <td>4.41</td>\n",
              "      <td>6.46</td>\n",
              "      <td>...</td>\n",
              "      <td>5.41</td>\n",
              "      <td>4.13</td>\n",
              "      <td>5.02</td>\n",
              "      <td>4.43</td>\n",
              "      <td>5.79</td>\n",
              "      <td>3.43</td>\n",
              "      <td>4.04</td>\n",
              "      <td>6.04</td>\n",
              "      <td>4.47</td>\n",
              "      <td>4.21</td>\n",
              "      <td>4.21</td>\n",
              "      <td>2.87</td>\n",
              "      <td>5.69</td>\n",
              "      <td>4.70</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.41</td>\n",
              "      <td>4.28</td>\n",
              "      <td>4.67</td>\n",
              "      <td>3.98</td>\n",
              "      <td>4.34</td>\n",
              "      <td>4.72</td>\n",
              "      <td>6.06</td>\n",
              "      <td>3.23</td>\n",
              "      <td>5.23</td>\n",
              "      <td>4.79</td>\n",
              "      <td>3.36</td>\n",
              "      <td>3.51</td>\n",
              "      <td>3.41</td>\n",
              "      <td>4.79</td>\n",
              "      <td>5.31</td>\n",
              "      <td>3.58</td>\n",
              "      <td>5.23</td>\n",
              "      <td>4.73</td>\n",
              "      <td>3.21</td>\n",
              "      <td>6.48</td>\n",
              "      <td>4.74</td>\n",
              "      <td>3.55</td>\n",
              "      <td>4.30</td>\n",
              "      <td>3.58</td>\n",
              "      <td>3.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13454</th>\n",
              "      <td>diffuse large B-cell lymphoma</td>\n",
              "      <td>6.70</td>\n",
              "      <td>7.40</td>\n",
              "      <td>6.77</td>\n",
              "      <td>7.90</td>\n",
              "      <td>3.32</td>\n",
              "      <td>7.73</td>\n",
              "      <td>5.77</td>\n",
              "      <td>3.87</td>\n",
              "      <td>8.92</td>\n",
              "      <td>3.73</td>\n",
              "      <td>5.33</td>\n",
              "      <td>7.64</td>\n",
              "      <td>5.63</td>\n",
              "      <td>8.30</td>\n",
              "      <td>7.88</td>\n",
              "      <td>4.26</td>\n",
              "      <td>3.88</td>\n",
              "      <td>7.41</td>\n",
              "      <td>7.53</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.62</td>\n",
              "      <td>5.52</td>\n",
              "      <td>5.28</td>\n",
              "      <td>8.30</td>\n",
              "      <td>7.20</td>\n",
              "      <td>5.55</td>\n",
              "      <td>6.89</td>\n",
              "      <td>4.68</td>\n",
              "      <td>6.79</td>\n",
              "      <td>5.12</td>\n",
              "      <td>7.39</td>\n",
              "      <td>5.56</td>\n",
              "      <td>5.49</td>\n",
              "      <td>7.03</td>\n",
              "      <td>3.63</td>\n",
              "      <td>5.26</td>\n",
              "      <td>5.88</td>\n",
              "      <td>4.30</td>\n",
              "      <td>6.45</td>\n",
              "      <td>...</td>\n",
              "      <td>5.20</td>\n",
              "      <td>4.69</td>\n",
              "      <td>5.25</td>\n",
              "      <td>4.02</td>\n",
              "      <td>5.69</td>\n",
              "      <td>3.70</td>\n",
              "      <td>3.78</td>\n",
              "      <td>6.46</td>\n",
              "      <td>3.59</td>\n",
              "      <td>4.41</td>\n",
              "      <td>3.82</td>\n",
              "      <td>3.33</td>\n",
              "      <td>5.30</td>\n",
              "      <td>4.42</td>\n",
              "      <td>3.26</td>\n",
              "      <td>3.65</td>\n",
              "      <td>4.00</td>\n",
              "      <td>5.31</td>\n",
              "      <td>4.07</td>\n",
              "      <td>4.61</td>\n",
              "      <td>4.66</td>\n",
              "      <td>5.39</td>\n",
              "      <td>3.30</td>\n",
              "      <td>5.29</td>\n",
              "      <td>4.73</td>\n",
              "      <td>3.33</td>\n",
              "      <td>4.30</td>\n",
              "      <td>3.81</td>\n",
              "      <td>4.86</td>\n",
              "      <td>5.33</td>\n",
              "      <td>3.77</td>\n",
              "      <td>8.48</td>\n",
              "      <td>8.61</td>\n",
              "      <td>3.42</td>\n",
              "      <td>6.54</td>\n",
              "      <td>5.13</td>\n",
              "      <td>3.67</td>\n",
              "      <td>4.67</td>\n",
              "      <td>3.97</td>\n",
              "      <td>3.94</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4005 rows × 54000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Disease  1007_s_at  ...  244747_at  244748_at\n",
              "9450         acute myeloid leukaemia       7.17  ...       3.94       4.03\n",
              "9451         acute myeloid leukaemia       6.64  ...       3.73       3.76\n",
              "9452         acute myeloid leukaemia       7.11  ...       3.83       3.77\n",
              "9453         acute myeloid leukaemia       6.77  ...       3.75       4.30\n",
              "9454         acute myeloid leukaemia       7.23  ...       3.67       3.84\n",
              "...                              ...        ...  ...        ...        ...\n",
              "13450  diffuse large B-cell lymphoma       7.12  ...       4.12       3.96\n",
              "13451  diffuse large B-cell lymphoma       7.09  ...       3.67       3.82\n",
              "13452  diffuse large B-cell lymphoma       8.18  ...       3.79       4.01\n",
              "13453  diffuse large B-cell lymphoma       7.02  ...       3.58       3.78\n",
              "13454  diffuse large B-cell lymphoma       6.70  ...       3.97       3.94\n",
              "\n",
              "[4005 rows x 54000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kA50gnCExnP"
      },
      "source": [
        "### dataset2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Du-v2Gmfavv"
      },
      "source": [
        "if cnt==4:\n",
        " dat_resa=dat_res[dat_res['Disease'].str.contains('lung|breast|leukaemia|lymphoma|myeloma|colon|ALL|AML|normal',regex=True)]\n",
        " dat_resa\n",
        " dat_resa.replace(to_replace=r'[^leukaemia$','^ALL$','^AML$'],value='leukaemia',regex=True)\n",
        " dat_resa.replace(regex=r'^lung$',value='lung')\n",
        " dat_resa.replace(to_replace=r'^myeloma$',value='myeloma',regex=True)\n",
        " dat_resa['Disease'].str.replace(r'^breast$','breast',regex=True)\n",
        " dat_resa.replace(to_replace=[r'^lymphoma$'],value='lymphoma',regex=True)\n",
        " dat_resa.replace(to_replace=[r'^colon$'],value='colon',regex=True)\n",
        " dat_resn=dat_resa[dat_resa.Disease=='normal']\n",
        " dat_resn\n",
        " dat_resn=dat_resn.set_index('Disease').reset_index()\n",
        " dat_resn=dat_resn.loc[[np.random.randint(0,9450) for i in range(1,2401)]]\n",
        " dat_resn\n",
        " dat_resa[dat_resa.Disease!='normal']\n",
        " dat_resa=pd.concat([dat_resn,dat_resa[dat_resa.Disease!='normal']],axis=0)\n",
        " #dat_resa=dat_resa[dat_resa.Disease!='normal']\n",
        " dat_resa\n",
        " dat_resa.to_csv('dat_resa2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjr23i4MfE4D"
      },
      "source": [
        "if cnt==3:\n",
        " dat_resa=pd.read_csv('drive/My Drive/dat_resa2.csv',index_col='Unnamed: 0')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su4bA4nrfE4D",
        "outputId": "62707110-da2a-4616-99ed-dd77507789ef"
      },
      "source": [
        "dat_resa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Disease</th>\n",
              "      <th>1007_s_at</th>\n",
              "      <th>1053_at</th>\n",
              "      <th>117_at</th>\n",
              "      <th>121_at</th>\n",
              "      <th>1255_g_at</th>\n",
              "      <th>1294_at</th>\n",
              "      <th>1316_at</th>\n",
              "      <th>1320_at</th>\n",
              "      <th>1405_i_at</th>\n",
              "      <th>...</th>\n",
              "      <th>244739_at</th>\n",
              "      <th>244740_at</th>\n",
              "      <th>244741_s_at</th>\n",
              "      <th>244742_at</th>\n",
              "      <th>244743_x_at</th>\n",
              "      <th>244744_at</th>\n",
              "      <th>244745_at</th>\n",
              "      <th>244746_at</th>\n",
              "      <th>244747_at</th>\n",
              "      <th>244748_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7022</th>\n",
              "      <td>normal</td>\n",
              "      <td>8.66</td>\n",
              "      <td>6.40</td>\n",
              "      <td>5.96</td>\n",
              "      <td>8.30</td>\n",
              "      <td>3.50</td>\n",
              "      <td>6.97</td>\n",
              "      <td>5.85</td>\n",
              "      <td>5.78</td>\n",
              "      <td>4.41</td>\n",
              "      <td>...</td>\n",
              "      <td>3.38</td>\n",
              "      <td>6.36</td>\n",
              "      <td>7.10</td>\n",
              "      <td>3.34</td>\n",
              "      <td>4.47</td>\n",
              "      <td>5.16</td>\n",
              "      <td>3.56</td>\n",
              "      <td>3.98</td>\n",
              "      <td>3.55</td>\n",
              "      <td>4.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4001</th>\n",
              "      <td>normal</td>\n",
              "      <td>10.00</td>\n",
              "      <td>8.34</td>\n",
              "      <td>5.16</td>\n",
              "      <td>7.41</td>\n",
              "      <td>3.59</td>\n",
              "      <td>6.33</td>\n",
              "      <td>5.16</td>\n",
              "      <td>4.46</td>\n",
              "      <td>3.49</td>\n",
              "      <td>...</td>\n",
              "      <td>3.57</td>\n",
              "      <td>7.50</td>\n",
              "      <td>7.68</td>\n",
              "      <td>3.28</td>\n",
              "      <td>5.18</td>\n",
              "      <td>4.71</td>\n",
              "      <td>3.36</td>\n",
              "      <td>4.59</td>\n",
              "      <td>4.26</td>\n",
              "      <td>3.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>normal</td>\n",
              "      <td>9.10</td>\n",
              "      <td>7.23</td>\n",
              "      <td>6.45</td>\n",
              "      <td>7.89</td>\n",
              "      <td>3.70</td>\n",
              "      <td>7.95</td>\n",
              "      <td>5.65</td>\n",
              "      <td>3.98</td>\n",
              "      <td>6.36</td>\n",
              "      <td>...</td>\n",
              "      <td>3.87</td>\n",
              "      <td>5.41</td>\n",
              "      <td>4.33</td>\n",
              "      <td>3.36</td>\n",
              "      <td>5.10</td>\n",
              "      <td>4.55</td>\n",
              "      <td>3.33</td>\n",
              "      <td>4.24</td>\n",
              "      <td>3.71</td>\n",
              "      <td>4.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7105</th>\n",
              "      <td>normal</td>\n",
              "      <td>6.63</td>\n",
              "      <td>8.60</td>\n",
              "      <td>5.78</td>\n",
              "      <td>7.88</td>\n",
              "      <td>3.50</td>\n",
              "      <td>8.15</td>\n",
              "      <td>5.23</td>\n",
              "      <td>4.11</td>\n",
              "      <td>6.49</td>\n",
              "      <td>...</td>\n",
              "      <td>3.66</td>\n",
              "      <td>5.74</td>\n",
              "      <td>4.55</td>\n",
              "      <td>3.19</td>\n",
              "      <td>6.13</td>\n",
              "      <td>4.59</td>\n",
              "      <td>3.51</td>\n",
              "      <td>4.80</td>\n",
              "      <td>4.00</td>\n",
              "      <td>3.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8894</th>\n",
              "      <td>normal</td>\n",
              "      <td>10.05</td>\n",
              "      <td>6.26</td>\n",
              "      <td>5.58</td>\n",
              "      <td>8.16</td>\n",
              "      <td>3.43</td>\n",
              "      <td>7.44</td>\n",
              "      <td>5.45</td>\n",
              "      <td>4.70</td>\n",
              "      <td>8.84</td>\n",
              "      <td>...</td>\n",
              "      <td>3.26</td>\n",
              "      <td>5.96</td>\n",
              "      <td>5.04</td>\n",
              "      <td>3.10</td>\n",
              "      <td>5.47</td>\n",
              "      <td>5.16</td>\n",
              "      <td>3.74</td>\n",
              "      <td>5.04</td>\n",
              "      <td>4.21</td>\n",
              "      <td>4.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27868</th>\n",
              "      <td>breast</td>\n",
              "      <td>10.66</td>\n",
              "      <td>8.46</td>\n",
              "      <td>5.15</td>\n",
              "      <td>7.86</td>\n",
              "      <td>3.46</td>\n",
              "      <td>5.87</td>\n",
              "      <td>5.85</td>\n",
              "      <td>4.24</td>\n",
              "      <td>6.19</td>\n",
              "      <td>...</td>\n",
              "      <td>4.09</td>\n",
              "      <td>4.72</td>\n",
              "      <td>3.89</td>\n",
              "      <td>3.33</td>\n",
              "      <td>7.39</td>\n",
              "      <td>4.82</td>\n",
              "      <td>7.34</td>\n",
              "      <td>4.57</td>\n",
              "      <td>5.37</td>\n",
              "      <td>4.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27869</th>\n",
              "      <td>breast</td>\n",
              "      <td>10.82</td>\n",
              "      <td>8.33</td>\n",
              "      <td>5.34</td>\n",
              "      <td>7.89</td>\n",
              "      <td>3.44</td>\n",
              "      <td>5.82</td>\n",
              "      <td>5.66</td>\n",
              "      <td>4.22</td>\n",
              "      <td>5.18</td>\n",
              "      <td>...</td>\n",
              "      <td>3.83</td>\n",
              "      <td>4.92</td>\n",
              "      <td>3.72</td>\n",
              "      <td>3.15</td>\n",
              "      <td>7.14</td>\n",
              "      <td>4.60</td>\n",
              "      <td>7.41</td>\n",
              "      <td>4.70</td>\n",
              "      <td>5.64</td>\n",
              "      <td>3.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27872</th>\n",
              "      <td>leukaemia</td>\n",
              "      <td>7.46</td>\n",
              "      <td>5.31</td>\n",
              "      <td>6.89</td>\n",
              "      <td>8.06</td>\n",
              "      <td>3.46</td>\n",
              "      <td>7.20</td>\n",
              "      <td>5.65</td>\n",
              "      <td>3.82</td>\n",
              "      <td>6.73</td>\n",
              "      <td>...</td>\n",
              "      <td>3.57</td>\n",
              "      <td>5.45</td>\n",
              "      <td>4.04</td>\n",
              "      <td>3.22</td>\n",
              "      <td>5.42</td>\n",
              "      <td>5.09</td>\n",
              "      <td>3.43</td>\n",
              "      <td>4.97</td>\n",
              "      <td>3.56</td>\n",
              "      <td>3.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27873</th>\n",
              "      <td>leukaemia</td>\n",
              "      <td>8.05</td>\n",
              "      <td>5.96</td>\n",
              "      <td>5.58</td>\n",
              "      <td>7.83</td>\n",
              "      <td>3.47</td>\n",
              "      <td>8.41</td>\n",
              "      <td>5.57</td>\n",
              "      <td>3.88</td>\n",
              "      <td>8.15</td>\n",
              "      <td>...</td>\n",
              "      <td>3.64</td>\n",
              "      <td>5.30</td>\n",
              "      <td>3.96</td>\n",
              "      <td>3.04</td>\n",
              "      <td>6.07</td>\n",
              "      <td>4.73</td>\n",
              "      <td>3.27</td>\n",
              "      <td>4.77</td>\n",
              "      <td>3.72</td>\n",
              "      <td>3.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27874</th>\n",
              "      <td>leukaemia</td>\n",
              "      <td>8.42</td>\n",
              "      <td>5.58</td>\n",
              "      <td>5.92</td>\n",
              "      <td>7.74</td>\n",
              "      <td>3.49</td>\n",
              "      <td>7.06</td>\n",
              "      <td>5.96</td>\n",
              "      <td>3.76</td>\n",
              "      <td>8.28</td>\n",
              "      <td>...</td>\n",
              "      <td>3.50</td>\n",
              "      <td>8.48</td>\n",
              "      <td>9.06</td>\n",
              "      <td>3.30</td>\n",
              "      <td>5.41</td>\n",
              "      <td>4.75</td>\n",
              "      <td>3.20</td>\n",
              "      <td>4.62</td>\n",
              "      <td>3.75</td>\n",
              "      <td>4.15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8908 rows × 54000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Disease  1007_s_at  1053_at  117_at  121_at  1255_g_at  1294_at  \\\n",
              "7022      normal       8.66     6.40    5.96    8.30       3.50     6.97   \n",
              "4001      normal      10.00     8.34    5.16    7.41       3.59     6.33   \n",
              "284       normal       9.10     7.23    6.45    7.89       3.70     7.95   \n",
              "7105      normal       6.63     8.60    5.78    7.88       3.50     8.15   \n",
              "8894      normal      10.05     6.26    5.58    8.16       3.43     7.44   \n",
              "...          ...        ...      ...     ...     ...        ...      ...   \n",
              "27868     breast      10.66     8.46    5.15    7.86       3.46     5.87   \n",
              "27869     breast      10.82     8.33    5.34    7.89       3.44     5.82   \n",
              "27872  leukaemia       7.46     5.31    6.89    8.06       3.46     7.20   \n",
              "27873  leukaemia       8.05     5.96    5.58    7.83       3.47     8.41   \n",
              "27874  leukaemia       8.42     5.58    5.92    7.74       3.49     7.06   \n",
              "\n",
              "       1316_at  1320_at  1405_i_at  ...  244739_at  244740_at  244741_s_at  \\\n",
              "7022      5.85     5.78       4.41  ...       3.38       6.36         7.10   \n",
              "4001      5.16     4.46       3.49  ...       3.57       7.50         7.68   \n",
              "284       5.65     3.98       6.36  ...       3.87       5.41         4.33   \n",
              "7105      5.23     4.11       6.49  ...       3.66       5.74         4.55   \n",
              "8894      5.45     4.70       8.84  ...       3.26       5.96         5.04   \n",
              "...        ...      ...        ...  ...        ...        ...          ...   \n",
              "27868     5.85     4.24       6.19  ...       4.09       4.72         3.89   \n",
              "27869     5.66     4.22       5.18  ...       3.83       4.92         3.72   \n",
              "27872     5.65     3.82       6.73  ...       3.57       5.45         4.04   \n",
              "27873     5.57     3.88       8.15  ...       3.64       5.30         3.96   \n",
              "27874     5.96     3.76       8.28  ...       3.50       8.48         9.06   \n",
              "\n",
              "       244742_at  244743_x_at  244744_at  244745_at  244746_at  244747_at  \\\n",
              "7022        3.34         4.47       5.16       3.56       3.98       3.55   \n",
              "4001        3.28         5.18       4.71       3.36       4.59       4.26   \n",
              "284         3.36         5.10       4.55       3.33       4.24       3.71   \n",
              "7105        3.19         6.13       4.59       3.51       4.80       4.00   \n",
              "8894        3.10         5.47       5.16       3.74       5.04       4.21   \n",
              "...          ...          ...        ...        ...        ...        ...   \n",
              "27868       3.33         7.39       4.82       7.34       4.57       5.37   \n",
              "27869       3.15         7.14       4.60       7.41       4.70       5.64   \n",
              "27872       3.22         5.42       5.09       3.43       4.97       3.56   \n",
              "27873       3.04         6.07       4.73       3.27       4.77       3.72   \n",
              "27874       3.30         5.41       4.75       3.20       4.62       3.75   \n",
              "\n",
              "       244748_at  \n",
              "7022        4.17  \n",
              "4001        3.81  \n",
              "284         4.12  \n",
              "7105        3.85  \n",
              "8894        4.09  \n",
              "...          ...  \n",
              "27868       4.02  \n",
              "27869       3.82  \n",
              "27872       3.88  \n",
              "27873       3.77  \n",
              "27874       4.15  \n",
              "\n",
              "[8908 rows x 54000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5aIs_GIa2kP"
      },
      "source": [
        "### dataset3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih7qN31Xc9EY"
      },
      "source": [
        "if cnt==4: \n",
        " ss='multiple sclerosis '+'or'+' clinically isolated syndrome'\n",
        " lsta_=list(dep_var.x.value_counts().head(14).index)\n",
        " lsta_ex=['irritable bowel syndrome','sepsis','kidney transplant recipient','ulcerative colitis',\"Crohn's disease\",'septic shock','myelodysplastic syndrome','no rejection','obesity','periodontitis','asthma',ss]\n",
        " lsta_new=[ll for ll in lsta_ if ll not in lsta_ex]\n",
        " lsta_\n",
        " dat_resa=dat_res.set_index('Disease').loc[lsta_new].reset_index()\n",
        " dat_resn=dat_resa[dat_resa.Disease=='normal']\n",
        " dat_resn\n",
        " dat_resn=dat_resn.loc[[np.random.randint(0,9450) for i in range(1,6001)]]\n",
        " dat_resn\n",
        " dat_resa[dat_resa.Disease!='normal']\n",
        " dat_resa=pd.concat([dat_resn,dat_resa[dat_resa.Disease!='normal']],axis=0)\n",
        " #dat_resa=dat_resa[dat_resa.Disease!='normal']\n",
        " dat_resa\n",
        " dat_resa.to_csv('dat_resa3.csv')\n",
        " dat_resa=pd.read_csv('drive/My Drive/dat_resa3.csv',index_col='Unnamed: 0')\n",
        " #dat_resa=dat_resa.drop(['Unnamed: 0'],axis=1)\n",
        " dat_resa=dat_resa[dat_resa.Disease!='normal']\n",
        " dat_resa"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Xhz-Euz3bd"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkDD4uxNF2RX"
      },
      "source": [
        "### dataset1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D82i11e3HUOU"
      },
      "source": [
        "#### full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh6nS3rMBcME"
      },
      "source": [
        "if cnt==1: \n",
        " scaler=MinMaxScaler()\n",
        " dat_resaX=scaler.fit_transform(dat_resa.drop(['Disease'],axis=1))\n",
        " over_sampler = SMOTE(k_neighbors=10, sampling_strategy=\"auto\")\n",
        " dat_resaX, dat_resaY = over_sampler.fit_resample(dat_resaX, dat_resa['Disease'])\n",
        " selector = SelectKBest(score_func=f_classif, k=60)\n",
        " dat_resaXred=selector.fit_transform(dat_resaX,dat_resaY)\n",
        " dat_resaXred\n",
        " selector.pvalues_\n",
        " selector.scores_\n",
        " dtheat=pd.DataFrame(dat_resaXred)\n",
        " dtheat['Disease']=dat_resaY\n",
        " dtheat.set_index('Disease')\n",
        " plt.figure(figsize=(16,9))\n",
        " sns.heatmap(dtheat.set_index('Disease'))\n",
        " dbscana1 = DBSCAN(eps=0.77, min_samples=18)\n",
        " dbscana1.fit_predict(dat_resaXred)\n",
        " lab1=list(dbscana1.labels_)\n",
        " lab1_=list(filter(lambda x:x>-1,lab1))\n",
        " len(lab1_)\n",
        " len(set(lab1_))\n",
        " lab1.count(-1)\n",
        " #dat_resa.drop(['dbscan'],axis=1)\n",
        " dat_resaXred_=pd.DataFrame(dat_resaXred)\n",
        " dat_resaXred_\n",
        " dat_resaXred_['Disease']=pd.DataFrame(dat_resaY)[0]\n",
        " dat_resaXred_['dbscan']=pd.DataFrame(lab1)[0]\n",
        " dat_resaXred_[dat_resaXred_['dbscan']==0]['Disease'].value_counts()\n",
        " pca_transformer = PCA(n_components=60)\n",
        " dat_resaXpca1=pca_transformer.fit(dat_resaX)\n",
        " dat_resaXpca=dat_resaXpca1.transform(dat_resaX)\n",
        " dat_resaXpca1.explained_variance_ratio_\n",
        " dat_resaXpca1.explained_variance_ratio_.cumsum()\n",
        " dat_resaXpca.shape\n",
        " dbscana4 = DBSCAN(eps=20, min_samples=100)\n",
        " dbscana4.fit_predict(dat_resaXpca)\n",
        " lab4=list(dbscana4.labels_)\n",
        " lab4_=list(filter(lambda x:x>-1,lab4))\n",
        " len(lab4_)\n",
        " len(set(lab4_))\n",
        " lab4.count(-1)\n",
        " dat_resaXpca_=pd.DataFrame(dat_resaXpca)\n",
        " dat_resaXpca_\n",
        " dat_resaXpca_['Disease']=pd.DataFrame(dat_resaY)[0]\n",
        " dat_resaXpca_['dbscan']=pd.DataFrame(lab4)[0]\n",
        " dat_resaXpca_[dat_resaXpca_['dbscan']==1]['Disease'].value_counts()\n",
        " isomap = Isomap(n_components=60, n_neighbors=30,n_jobs=-1)\n",
        " dat_resaXiso=isomap.fit_transform(dat_resaX)\n",
        " isomap.reconstruction_error()\n",
        " dbscana2 = DBSCAN(eps=88, min_samples=200)\n",
        " dbscana2.fit_predict(dat_resaXiso)\n",
        " lab2=list(dbscana2.labels_)\n",
        " lab2_=list(filter(lambda x:x>-1,lab2))\n",
        " len(lab2_)\n",
        " len(set(lab2_))\n",
        " lab2.count(-1)\n",
        " dat_resaXiso_=pd.DataFrame(dat_resaXiso)\n",
        " dat_resaXiso_\n",
        " dat_resaXiso_['Disease']=pd.DataFrame(dat_resaY)[0]\n",
        " at_resaXiso_['dbscan']=pd.DataFrame(lab2)[0]\n",
        " dat_resaXiso_[dat_resaXiso_['dbscan']==0]['Disease'].value_counts()\n",
        " input_dim = 53999\n",
        " latent_dim1 =5000\n",
        " latent_dim2=500\n",
        " latent_dim3=60\n",
        " input_layer = Input(shape=(input_dim,), name='input')\n",
        "\n",
        " encoded1 = Dense(latent_dim1, \n",
        "                activation='relu', name='features1')(input_layer)\n",
        " encoded2 = Dense(latent_dim2, \n",
        "                activation='relu', name='features2')(encoded1)\n",
        " encoded3 = Dense(latent_dim3, \n",
        "                activation='relu', name='features3')(encoded2)\n",
        "\n",
        " decoded1 = Dense(latent_dim2, activation='relu', name='reconstructed1')(encoded3)\n",
        " decoded2 = Dense(latent_dim1, activation='relu', name='reconstructed2')(decoded1)\n",
        " decoded3 = Dense(input_dim, activation='sigmoid', name='reconstructed3')(decoded2)\n",
        "\n",
        " autoencoder = Model(inputs=[input_layer], outputs=[decoded3])\n",
        "\n",
        " autoencoder.compile(optimizer='adam', \n",
        "                    loss='binary_crossentropy')\n",
        " autoencoder.fit(dat_resaX,dat_resaX ,\n",
        "                epochs=1000,\n",
        "                batch_size=3000,\n",
        "                shuffle=True,\n",
        "                validation_split=0.2)\n",
        " encoder = Model(input_layer, encoded3)\n",
        " dat_resaXauto = encoder.predict(dat_resaX)\n",
        " pd.DataFrame(dat_resaXauto)\n",
        " dbscana3 = DBSCAN(eps=36, min_samples=130)\n",
        " dbscana3.fit_predict(dat_resaXauto)\n",
        " lab3=list(dbscana3.labels_)\n",
        " lab3_=list(filter(lambda x:x>-1,lab3))\n",
        " len(lab3_)\n",
        " len(set(lab3_))\n",
        " lab3.count(-1)\n",
        " #dat_resa.drop(['dbscan'],axis=1)\n",
        " dat_resaXauto_=pd.DataFrame(dat_resaXauto)\n",
        " dat_resaXauto_\n",
        " dat_resaXauto_['Disease']=pd.DataFrame(dat_resaY)[0]\n",
        " dat_resaXauto_['dbscan']=pd.DataFrame(lab3)[0]\n",
        " dat_resaXauto_[dat_resaXauto_['dbscan']==0]['Disease'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u1BecOiE3yG",
        "outputId": "7041e723-5919-49c7-cbb3-b78ad28c37fa"
      },
      "source": [
        "#over_sampler = SMOTE(k_neighbors=10, sampling_strategy=\"auto\")\n",
        "#dat_resaX, dat_resaY = over_sampler.fit_resample(dat_resaX, dat_resa['Disease'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGKFlTOBRJLy",
        "outputId": "3491e7e4-d7f3-4196-cb67-9ec6acb48cfa"
      },
      "source": [
        "#selector = SelectKBest(score_func=f_classif, k=60)\n",
        "#dat_resaXred=selector.fit_transform(dat_resaX,dat_resaY)\n",
        "#dat_resaXred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.16082512, 0.22851822, 0.28833297, ..., 0.78715729, 0.20503282,\n",
              "        0.14917261],\n",
              "       [0.03386272, 0.12343708, 0.09262454, ..., 0.81120731, 0.0916849 ,\n",
              "        0.11897572],\n",
              "       [0.05029208, 0.13141793, 0.14103947, ..., 0.81902357, 0.08555799,\n",
              "        0.24036719],\n",
              "       ...,\n",
              "       [0.20688506, 0.17921181, 0.34550779, ..., 0.5271126 , 0.10034334,\n",
              "        0.13448381],\n",
              "       [0.04730578, 0.15377759, 0.26785841, ..., 0.4305738 , 0.09390353,\n",
              "        0.12993037],\n",
              "       [0.04835649, 0.29835878, 0.26041321, ..., 0.4694953 , 0.05003477,\n",
              "        0.10858165]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIjNhF6g7MVC",
        "outputId": "11a64ab6-1047-439e-f79e-8fa13614e93e"
      },
      "source": [
        "#selector.pvalues_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00000000e+000, 0.00000000e+000, 2.05148213e-195, ...,\n",
              "       1.44361475e-309, 0.00000000e+000, 3.96595615e-079])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zanz3L5l7krm",
        "outputId": "2b4238f1-cf0c-45fc-daf6-5bee92cab68c"
      },
      "source": [
        "#selector.scores_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4369.60278868,  760.49896301,  197.92206802, ...,  326.90421279,\n",
              "        664.5035621 ,   77.69889259])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "tGjC5TLgvUaX",
        "outputId": "0292706b-c008-44a4-f445-d19ea22d198b"
      },
      "source": [
        "#dtheat=pd.DataFrame(dat_resaXred)\n",
        "#dtheat['Disease']=dat_resaY\n",
        "#dtheat.set_index('Disease')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Disease</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>acute myeloid leukaemia</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.13</td>\n",
              "      <td>1.03e-01</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>acute myeloid leukaemia</th>\n",
              "      <td>0.03</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.13</td>\n",
              "      <td>4.17e-02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>acute myeloid leukaemia</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1.19e-01</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>acute myeloid leukaemia</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.43</td>\n",
              "      <td>4.65e-02</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>acute myeloid leukaemia</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.08</td>\n",
              "      <td>8.45e-03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multiple myeloma</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.25e-01</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multiple myeloma</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.25</td>\n",
              "      <td>9.06e-02</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multiple myeloma</th>\n",
              "      <td>0.32</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.25</td>\n",
              "      <td>4.88e-02</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multiple myeloma</th>\n",
              "      <td>0.41</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.13</td>\n",
              "      <td>7.40e-02</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>multiple myeloma</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>9.48e-02</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5844 rows × 60 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                           0     1     2     3   ...    56    57    58    59\n",
              "Disease                                          ...                        \n",
              "acute myeloid leukaemia  0.16  0.23  0.29  0.17  ...  0.38  0.79  0.21  0.15\n",
              "acute myeloid leukaemia  0.03  0.12  0.09  0.10  ...  0.18  0.81  0.09  0.12\n",
              "acute myeloid leukaemia  0.05  0.13  0.14  0.11  ...  0.35  0.82  0.09  0.24\n",
              "acute myeloid leukaemia  0.22  0.12  0.13  0.09  ...  0.53  0.72  0.07  0.11\n",
              "acute myeloid leukaemia  0.08  0.08  0.21  0.08  ...  0.43  0.77  0.03  0.16\n",
              "...                       ...   ...   ...   ...  ...   ...   ...   ...   ...\n",
              "multiple myeloma         0.16  0.18  0.32  0.19  ...  0.20  0.65  0.08  0.09\n",
              "multiple myeloma         0.07  0.21  0.48  0.21  ...  0.22  0.63  0.07  0.13\n",
              "multiple myeloma         0.32  0.11  0.26  0.10  ...  0.32  0.60  0.10  0.08\n",
              "multiple myeloma         0.41  0.20  0.52  0.17  ...  0.18  0.65  0.03  0.10\n",
              "multiple myeloma         0.20  0.27  0.45  0.22  ...  0.21  0.60  0.05  0.10\n",
              "\n",
              "[5844 rows x 60 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "KL-lm_Pr33DX",
        "outputId": "eeca7f64-f4fd-48e8-a4bd-a5e73556a7a4"
      },
      "source": [
        "#plt.figure(figsize=(16,9))\n",
        "#sns.heatmap(dtheat.set_index('Disease'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0c257b8780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zGZRGpnL8Er",
        "outputId": "3f1961b6-aef1-474b-8721-207e0064345e"
      },
      "source": [
        "#dbscana1 = DBSCAN(eps=0.77, min_samples=18)\n",
        "#dbscana1.fit_predict(dat_resaXred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 4, 4, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoi3TH2x_nW5"
      },
      "source": [
        "#lab1=list(dbscana1.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKNFBHbIl4Zz"
      },
      "source": [
        "#lab1_=list(filter(lambda x:x>-1,lab1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9vbYkW1N9vI",
        "outputId": "1e499946-9694-42be-f905-aab0347df60a"
      },
      "source": [
        "#len(lab1_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5680"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5xMSKjB_7sO",
        "outputId": "6186a8f3-133b-4eae-ecf6-afdd6c111ad6"
      },
      "source": [
        "#len(set(lab1_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "158AuuyIn25Q",
        "outputId": "77f269c8-5607-437b-8148-38b4d2f244b9"
      },
      "source": [
        "#lab1.count(-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uo08TfL0wpQ"
      },
      "source": [
        "#dat_resa.drop(['dbscan'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "HjSMScTb5Giv",
        "outputId": "5d216e22-492c-4bfe-971b-54558c9d4944"
      },
      "source": [
        "#dat_resaXred_=pd.DataFrame(dat_resaXred)\n",
        "#dat_resaXred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.13</td>\n",
              "      <td>1.03e-01</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.03</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.13</td>\n",
              "      <td>4.17e-02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1.19e-01</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.43</td>\n",
              "      <td>4.65e-02</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.08</td>\n",
              "      <td>8.45e-03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5839</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.28</td>\n",
              "      <td>7.89e-02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5840</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.13</td>\n",
              "      <td>1.38e-01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5841</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.09</td>\n",
              "      <td>1.10e-01</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5842</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.15</td>\n",
              "      <td>8.42e-02</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5843</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.68e-02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5844 rows × 60 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0     1     2     3     4     5   ...    54    55    56    57    58    59\n",
              "0     0.16  0.23  0.29  0.17  0.17  0.11  ...  0.06  0.66  0.38  0.79  0.21  0.15\n",
              "1     0.03  0.12  0.09  0.10  0.08  0.17  ...  0.05  0.83  0.18  0.81  0.09  0.12\n",
              "2     0.05  0.13  0.14  0.11  0.02  0.11  ...  0.03  0.62  0.35  0.82  0.09  0.24\n",
              "3     0.22  0.12  0.13  0.09  0.18  0.11  ...  0.24  0.78  0.53  0.72  0.07  0.11\n",
              "4     0.08  0.08  0.21  0.08  0.10  0.07  ...  0.07  0.77  0.43  0.77  0.03  0.16\n",
              "...    ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
              "5839  0.17  0.18  0.06  0.13  0.18  0.67  ...  0.06  0.51  0.19  0.50  0.06  0.13\n",
              "5840  0.16  0.17  0.42  0.21  0.10  0.91  ...  0.04  0.75  0.27  0.52  0.08  0.13\n",
              "5841  0.21  0.18  0.35  0.19  0.05  0.78  ...  0.03  0.76  0.25  0.53  0.10  0.13\n",
              "5842  0.05  0.15  0.27  0.24  0.18  0.85  ...  0.06  0.68  0.23  0.43  0.09  0.13\n",
              "5843  0.05  0.30  0.26  0.18  0.13  0.71  ...  0.04  0.73  0.23  0.47  0.05  0.11\n",
              "\n",
              "[5844 rows x 60 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY3w93mOzsNw"
      },
      "source": [
        "#dat_resaXred_['Disease']=pd.DataFrame(dat_resaY)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TJGTJZf2r36"
      },
      "source": [
        "#dat_resaXred_['dbscan']=pd.DataFrame(lab1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi187Zk6z_Cs",
        "outputId": "da13d15e-7af1-47db-ad16-b90859f25e7e"
      },
      "source": [
        "#dat_resaXred_[dat_resaXred_['dbscan']==0]['Disease'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "acute myeloid leukaemia          961\n",
              "chronic lymphocytic leukaemia    959\n",
              "multiple myeloma                  34\n",
              "Name: Disease, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw1pnIBGcpCg"
      },
      "source": [
        "#pca_transformer = PCA(n_components=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9ZntGZYczqX"
      },
      "source": [
        "#dat_resaXpca1=pca_transformer.fit(dat_resaX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoKAUDIb1t8G"
      },
      "source": [
        "#dat_resaXpca=dat_resaXpca1.transform(dat_resaX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmDgTsrzcz1d",
        "outputId": "98693999-62a6-4161-81ac-835d23154b7c"
      },
      "source": [
        "#dat_resaXpca1.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.16015719, 0.09452069, 0.0712438 , 0.05569434, 0.05316155,\n",
              "       0.03862302, 0.03431632, 0.02204739, 0.02066261, 0.01815106,\n",
              "       0.0151661 , 0.01434258, 0.01120848, 0.00947587, 0.00884937,\n",
              "       0.00785274, 0.006887  , 0.00653459, 0.00574301, 0.00530054,\n",
              "       0.00449575, 0.00413886, 0.00401434, 0.00374498, 0.00366923,\n",
              "       0.00324254, 0.00309565, 0.0029897 , 0.00276695, 0.0027208 ,\n",
              "       0.00257569, 0.00252548, 0.00243606, 0.00233934, 0.00219201,\n",
              "       0.00212691, 0.00202736, 0.00199982, 0.00184769, 0.00183177,\n",
              "       0.0018061 , 0.00174148, 0.00170855, 0.00163619, 0.00161672,\n",
              "       0.00159111, 0.00155572, 0.00153339, 0.00150943, 0.00143526,\n",
              "       0.00139247, 0.00138493, 0.0013686 , 0.00131671, 0.00131278,\n",
              "       0.00125324, 0.00124259, 0.0012174 , 0.00117895, 0.0011451 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCF1JHQlt7YE",
        "outputId": "ada4206a-0258-4806-f7c0-3657ffe6228e"
      },
      "source": [
        "#dat_resaXpca1.explained_variance_ratio_.cumsum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.16015719, 0.25467788, 0.32592168, 0.38161602, 0.43477757,\n",
              "       0.47340059, 0.50771691, 0.52976429, 0.5504269 , 0.56857796,\n",
              "       0.58374405, 0.59808663, 0.60929511, 0.61877098, 0.62762035,\n",
              "       0.6354731 , 0.64236009, 0.64889468, 0.65463769, 0.65993823,\n",
              "       0.66443398, 0.66857285, 0.67258719, 0.67633217, 0.6800014 ,\n",
              "       0.68324394, 0.68633959, 0.68932929, 0.69209624, 0.69481704,\n",
              "       0.69739273, 0.69991822, 0.70235427, 0.70469362, 0.70688562,\n",
              "       0.70901253, 0.71103989, 0.71303971, 0.71488741, 0.71671917,\n",
              "       0.71852527, 0.72026675, 0.7219753 , 0.72361149, 0.72522822,\n",
              "       0.72681933, 0.72837505, 0.72990844, 0.73141787, 0.73285313,\n",
              "       0.7342456 , 0.73563053, 0.73699913, 0.73831584, 0.73962862,\n",
              "       0.74088186, 0.74212446, 0.74334186, 0.74452081, 0.74566591])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHBE7Elc8BG5",
        "outputId": "f26daf91-fcc9-4a2b-ed80-fae16105df0a"
      },
      "source": [
        "#dat_resaXpca.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5844, 60)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9m4N8s7wSP6",
        "outputId": "f809125b-b64f-45a8-efdc-baaac43972a2"
      },
      "source": [
        "#dbscana4 = DBSCAN(eps=20, min_samples=100)\n",
        "#dbscana4.fit_predict(dat_resaXpca)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 5, 5, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppu4JMM6v5bz"
      },
      "source": [
        "#lab4=list(dbscana4.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyLOOaZkv5b3"
      },
      "source": [
        "#lab4_=list(filter(lambda x:x>-1,lab4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7HfdctBv5b6",
        "outputId": "5e21b061-8b25-4914-cbdd-40bb0648ea12"
      },
      "source": [
        "#len(lab4_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5409"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCQ0VPIiv5b-",
        "outputId": "10f8b14f-892c-422a-c8a9-d425a9777a1a"
      },
      "source": [
        "#len(set(lab4_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjnGXfRGv5cB",
        "outputId": "4e0325a2-a849-48d1-9902-2e1ac80b1223"
      },
      "source": [
        "#lab4.count(-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "435"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "l0qTIzyQv5cF",
        "outputId": "a3c789ff-30a0-4516-d2bf-2c7f30e4eaa6"
      },
      "source": [
        "#dat_resaXpca_=pd.DataFrame(dat_resaXpca)\n",
        "#dat_resaXpca_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.09</td>\n",
              "      <td>9.49</td>\n",
              "      <td>-1.18</td>\n",
              "      <td>-8.44</td>\n",
              "      <td>-1.61e+00</td>\n",
              "      <td>16.57</td>\n",
              "      <td>5.33</td>\n",
              "      <td>-2.82</td>\n",
              "      <td>-2.33</td>\n",
              "      <td>-0.84</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>5.26</td>\n",
              "      <td>1.86</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-1.25</td>\n",
              "      <td>-1.51</td>\n",
              "      <td>3.59</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>-3.38</td>\n",
              "      <td>-2.90</td>\n",
              "      <td>1.30e+00</td>\n",
              "      <td>-3.81</td>\n",
              "      <td>2.34</td>\n",
              "      <td>-1.22</td>\n",
              "      <td>1.16</td>\n",
              "      <td>-1.55</td>\n",
              "      <td>2.69</td>\n",
              "      <td>-1.44</td>\n",
              "      <td>-2.25</td>\n",
              "      <td>-3.18</td>\n",
              "      <td>-4.01</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>-1.06</td>\n",
              "      <td>-0.85</td>\n",
              "      <td>1.16</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-3.74</td>\n",
              "      <td>0.41</td>\n",
              "      <td>-3.57</td>\n",
              "      <td>-2.26</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.05</td>\n",
              "      <td>2.36</td>\n",
              "      <td>5.56</td>\n",
              "      <td>2.92</td>\n",
              "      <td>-1.33</td>\n",
              "      <td>-0.62</td>\n",
              "      <td>-2.26</td>\n",
              "      <td>1.74</td>\n",
              "      <td>2.07</td>\n",
              "      <td>-0.36</td>\n",
              "      <td>-2.83</td>\n",
              "      <td>-1.64</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>1.05</td>\n",
              "      <td>2.18</td>\n",
              "      <td>1.76</td>\n",
              "      <td>0.75</td>\n",
              "      <td>2.27</td>\n",
              "      <td>-1.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.93</td>\n",
              "      <td>0.16</td>\n",
              "      <td>11.65</td>\n",
              "      <td>-5.41</td>\n",
              "      <td>-1.88e+00</td>\n",
              "      <td>-6.80</td>\n",
              "      <td>10.33</td>\n",
              "      <td>-4.32</td>\n",
              "      <td>-3.72</td>\n",
              "      <td>3.86</td>\n",
              "      <td>2.36</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>5.94</td>\n",
              "      <td>-0.82</td>\n",
              "      <td>-2.86</td>\n",
              "      <td>-1.58</td>\n",
              "      <td>4.22</td>\n",
              "      <td>5.31</td>\n",
              "      <td>-3.53</td>\n",
              "      <td>-5.72</td>\n",
              "      <td>4.37e+00</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>0.11</td>\n",
              "      <td>1.34</td>\n",
              "      <td>2.06</td>\n",
              "      <td>-3.74</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>-2.08</td>\n",
              "      <td>-2.47</td>\n",
              "      <td>1.59</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2.55</td>\n",
              "      <td>-4.68</td>\n",
              "      <td>-1.10</td>\n",
              "      <td>-0.16</td>\n",
              "      <td>-3.86</td>\n",
              "      <td>-0.72</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>0.86</td>\n",
              "      <td>-0.16</td>\n",
              "      <td>1.24</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>-2.74</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.42</td>\n",
              "      <td>2.26</td>\n",
              "      <td>-1.05</td>\n",
              "      <td>-0.91</td>\n",
              "      <td>-1.37</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.64</td>\n",
              "      <td>-2.51</td>\n",
              "      <td>0.68</td>\n",
              "      <td>1.99</td>\n",
              "      <td>-0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15.03</td>\n",
              "      <td>-1.31</td>\n",
              "      <td>6.67</td>\n",
              "      <td>-7.80</td>\n",
              "      <td>8.13e+00</td>\n",
              "      <td>4.99</td>\n",
              "      <td>9.43</td>\n",
              "      <td>-3.02</td>\n",
              "      <td>-1.16</td>\n",
              "      <td>-8.72</td>\n",
              "      <td>0.11</td>\n",
              "      <td>-2.49</td>\n",
              "      <td>4.68</td>\n",
              "      <td>1.45</td>\n",
              "      <td>2.65</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.83</td>\n",
              "      <td>-2.43</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.29</td>\n",
              "      <td>1.41e+00</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>0.92</td>\n",
              "      <td>2.63</td>\n",
              "      <td>-1.12</td>\n",
              "      <td>0.74</td>\n",
              "      <td>-1.26</td>\n",
              "      <td>-2.40</td>\n",
              "      <td>-1.01</td>\n",
              "      <td>-0.37</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>1.46</td>\n",
              "      <td>2.41</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.69</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-0.51</td>\n",
              "      <td>0.26</td>\n",
              "      <td>2.40</td>\n",
              "      <td>2.71</td>\n",
              "      <td>1.39</td>\n",
              "      <td>-1.39</td>\n",
              "      <td>1.70</td>\n",
              "      <td>-3.25</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>2.14</td>\n",
              "      <td>1.81</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>-1.36</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>1.08</td>\n",
              "      <td>-0.38</td>\n",
              "      <td>0.29</td>\n",
              "      <td>2.06</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.30</td>\n",
              "      <td>-6.56</td>\n",
              "      <td>7.00</td>\n",
              "      <td>-2.90</td>\n",
              "      <td>6.88e-03</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>13.98</td>\n",
              "      <td>-5.73</td>\n",
              "      <td>0.37</td>\n",
              "      <td>1.02</td>\n",
              "      <td>2.64</td>\n",
              "      <td>-5.58</td>\n",
              "      <td>5.03</td>\n",
              "      <td>2.61</td>\n",
              "      <td>0.75</td>\n",
              "      <td>2.43</td>\n",
              "      <td>5.83</td>\n",
              "      <td>4.30</td>\n",
              "      <td>1.74</td>\n",
              "      <td>-1.69</td>\n",
              "      <td>1.76e+00</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2.94</td>\n",
              "      <td>-1.46</td>\n",
              "      <td>2.99</td>\n",
              "      <td>1.32</td>\n",
              "      <td>-3.46</td>\n",
              "      <td>-2.73</td>\n",
              "      <td>-1.06</td>\n",
              "      <td>1.94</td>\n",
              "      <td>-1.32</td>\n",
              "      <td>-2.81</td>\n",
              "      <td>1.51</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>3.31</td>\n",
              "      <td>-3.01</td>\n",
              "      <td>-1.90</td>\n",
              "      <td>-2.31</td>\n",
              "      <td>-2.15</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.54</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>-2.45</td>\n",
              "      <td>2.66</td>\n",
              "      <td>-0.59</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.82</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.02</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>0.42</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>1.45</td>\n",
              "      <td>-0.46</td>\n",
              "      <td>-2.40</td>\n",
              "      <td>0.83</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>2.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.84</td>\n",
              "      <td>-5.83</td>\n",
              "      <td>8.44</td>\n",
              "      <td>-5.05</td>\n",
              "      <td>2.15e+00</td>\n",
              "      <td>-2.98</td>\n",
              "      <td>12.98</td>\n",
              "      <td>-3.83</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-2.36</td>\n",
              "      <td>1.16</td>\n",
              "      <td>-2.56</td>\n",
              "      <td>4.24</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>2.42</td>\n",
              "      <td>2.76</td>\n",
              "      <td>4.69</td>\n",
              "      <td>-2.07</td>\n",
              "      <td>-2.04</td>\n",
              "      <td>1.92e-01</td>\n",
              "      <td>-3.27</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>3.59</td>\n",
              "      <td>-0.35</td>\n",
              "      <td>2.35</td>\n",
              "      <td>2.58</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>-4.82</td>\n",
              "      <td>1.57</td>\n",
              "      <td>1.86</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>2.33</td>\n",
              "      <td>-0.37</td>\n",
              "      <td>2.36</td>\n",
              "      <td>-1.36</td>\n",
              "      <td>-1.68</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>-1.85</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.86</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>-5.32</td>\n",
              "      <td>2.28</td>\n",
              "      <td>-1.18</td>\n",
              "      <td>0.18</td>\n",
              "      <td>1.13</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>0.37</td>\n",
              "      <td>-1.18</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>1.06</td>\n",
              "      <td>-1.46</td>\n",
              "      <td>0.26</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-2.76</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.23</td>\n",
              "      <td>2.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5839</th>\n",
              "      <td>8.20</td>\n",
              "      <td>13.90</td>\n",
              "      <td>-4.45</td>\n",
              "      <td>9.86</td>\n",
              "      <td>9.52e+00</td>\n",
              "      <td>-4.20</td>\n",
              "      <td>3.14</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.18</td>\n",
              "      <td>-2.53</td>\n",
              "      <td>-4.60</td>\n",
              "      <td>0.81</td>\n",
              "      <td>1.93</td>\n",
              "      <td>3.40</td>\n",
              "      <td>-6.29</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.16</td>\n",
              "      <td>-1.91</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>0.93</td>\n",
              "      <td>-7.85e-01</td>\n",
              "      <td>0.37</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>-0.35</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>1.14</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.51</td>\n",
              "      <td>1.64</td>\n",
              "      <td>-0.23</td>\n",
              "      <td>-0.28</td>\n",
              "      <td>0.22</td>\n",
              "      <td>-0.76</td>\n",
              "      <td>-0.89</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>1.91</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.32</td>\n",
              "      <td>-1.55</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-0.61</td>\n",
              "      <td>-0.52</td>\n",
              "      <td>-0.97</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>0.57</td>\n",
              "      <td>-0.61</td>\n",
              "      <td>0.44</td>\n",
              "      <td>-1.05</td>\n",
              "      <td>0.83</td>\n",
              "      <td>-1.16</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>-0.94</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5840</th>\n",
              "      <td>5.47</td>\n",
              "      <td>18.52</td>\n",
              "      <td>-7.71</td>\n",
              "      <td>15.76</td>\n",
              "      <td>4.46e+00</td>\n",
              "      <td>1.03</td>\n",
              "      <td>-1.98</td>\n",
              "      <td>0.21</td>\n",
              "      <td>-2.11</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4.16</td>\n",
              "      <td>2.05</td>\n",
              "      <td>-1.69</td>\n",
              "      <td>1.76</td>\n",
              "      <td>6.30</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>-2.24</td>\n",
              "      <td>0.61</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>0.60</td>\n",
              "      <td>4.45e-01</td>\n",
              "      <td>-3.24</td>\n",
              "      <td>-1.21</td>\n",
              "      <td>0.41</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.52</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>-0.78</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.53</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-1.83</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-2.32</td>\n",
              "      <td>1.02</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-3.39</td>\n",
              "      <td>1.29</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>0.33</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1.06</td>\n",
              "      <td>-3.16</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.61</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>0.12</td>\n",
              "      <td>-1.33</td>\n",
              "      <td>1.24</td>\n",
              "      <td>0.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5841</th>\n",
              "      <td>4.26</td>\n",
              "      <td>18.85</td>\n",
              "      <td>-9.18</td>\n",
              "      <td>16.30</td>\n",
              "      <td>3.87e+00</td>\n",
              "      <td>-0.73</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.17</td>\n",
              "      <td>-0.63</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.95</td>\n",
              "      <td>3.63</td>\n",
              "      <td>-1.81</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.68</td>\n",
              "      <td>-0.94</td>\n",
              "      <td>-3.20</td>\n",
              "      <td>1.05e+00</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>-1.14</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-1.33</td>\n",
              "      <td>0.90</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>2.59</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>1.08</td>\n",
              "      <td>1.17</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-0.98</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>-0.97</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.40</td>\n",
              "      <td>-0.35</td>\n",
              "      <td>-0.83</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.37</td>\n",
              "      <td>1.15</td>\n",
              "      <td>-1.02</td>\n",
              "      <td>1.95</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.41</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>-1.82</td>\n",
              "      <td>-0.97</td>\n",
              "      <td>-0.73</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.18</td>\n",
              "      <td>2.27</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5842</th>\n",
              "      <td>2.13</td>\n",
              "      <td>18.51</td>\n",
              "      <td>-4.90</td>\n",
              "      <td>14.89</td>\n",
              "      <td>-2.43e+00</td>\n",
              "      <td>-2.88</td>\n",
              "      <td>-1.83</td>\n",
              "      <td>-2.50</td>\n",
              "      <td>-7.94</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1.70</td>\n",
              "      <td>1.35</td>\n",
              "      <td>3.03</td>\n",
              "      <td>7.05</td>\n",
              "      <td>-1.29</td>\n",
              "      <td>-3.35</td>\n",
              "      <td>2.54</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>1.41</td>\n",
              "      <td>1.43e+00</td>\n",
              "      <td>-2.56</td>\n",
              "      <td>-2.05</td>\n",
              "      <td>1.64</td>\n",
              "      <td>0.33</td>\n",
              "      <td>1.37</td>\n",
              "      <td>2.34</td>\n",
              "      <td>0.12</td>\n",
              "      <td>-2.07</td>\n",
              "      <td>-1.16</td>\n",
              "      <td>3.21</td>\n",
              "      <td>-2.51</td>\n",
              "      <td>-3.19</td>\n",
              "      <td>2.48</td>\n",
              "      <td>-0.87</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>-3.39</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1.02</td>\n",
              "      <td>-1.15</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.14</td>\n",
              "      <td>-1.73</td>\n",
              "      <td>-1.92</td>\n",
              "      <td>1.14</td>\n",
              "      <td>-1.08</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.72</td>\n",
              "      <td>-0.78</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.66</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.44</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.95</td>\n",
              "      <td>0.98</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.29</td>\n",
              "      <td>-1.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5843</th>\n",
              "      <td>4.77</td>\n",
              "      <td>12.84</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>7.06</td>\n",
              "      <td>4.44e+00</td>\n",
              "      <td>-5.65</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.66</td>\n",
              "      <td>7.61</td>\n",
              "      <td>4.97</td>\n",
              "      <td>1.67</td>\n",
              "      <td>1.05</td>\n",
              "      <td>-2.78</td>\n",
              "      <td>-3.92</td>\n",
              "      <td>-4.65</td>\n",
              "      <td>-2.75</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.11e-03</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>0.56</td>\n",
              "      <td>-2.01</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>-1.38</td>\n",
              "      <td>4.11</td>\n",
              "      <td>-4.47</td>\n",
              "      <td>1.65</td>\n",
              "      <td>-0.37</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1.73</td>\n",
              "      <td>-2.06</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>2.77</td>\n",
              "      <td>1.81</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.66</td>\n",
              "      <td>1.02</td>\n",
              "      <td>2.06</td>\n",
              "      <td>2.25</td>\n",
              "      <td>-0.95</td>\n",
              "      <td>2.21</td>\n",
              "      <td>1.08</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.79</td>\n",
              "      <td>2.03</td>\n",
              "      <td>-1.36</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.47</td>\n",
              "      <td>-1.32</td>\n",
              "      <td>1.09</td>\n",
              "      <td>1.86</td>\n",
              "      <td>2.45</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5844 rows × 60 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0      1      2      3         4   ...    55    56    57    58    59\n",
              "0      8.09   9.49  -1.18  -8.44 -1.61e+00  ...  2.18  1.76  0.75  2.27 -1.79\n",
              "1      4.93   0.16  11.65  -5.41 -1.88e+00  ...  0.64 -2.51  0.68  1.99 -0.43\n",
              "2     15.03  -1.31   6.67  -7.80  8.13e+00  ...  1.08 -0.38  0.29  2.06  0.50\n",
              "3      6.30  -6.56   7.00  -2.90  6.88e-03  ... -0.46 -2.40  0.83 -0.13  2.38\n",
              "4      7.84  -5.83   8.44  -5.05  2.15e+00  ... -0.11 -2.76  0.50  0.23  2.70\n",
              "...     ...    ...    ...    ...       ...  ...   ...   ...   ...   ...   ...\n",
              "5839   8.20  13.90  -4.45   9.86  9.52e+00  ... -1.16 -1.35 -0.94  0.23  0.33\n",
              "5840   5.47  18.52  -7.71  15.76  4.46e+00  ... -0.24  0.12 -1.33  1.24  0.84\n",
              "5841   4.26  18.85  -9.18  16.30  3.87e+00  ...  0.57  1.18  2.27  0.24  0.45\n",
              "5842   2.13  18.51  -4.90  14.89 -2.43e+00  ... -0.95  0.98  1.60  0.29 -1.57\n",
              "5843   4.77  12.84  -1.04   7.06  4.44e+00  ...  1.09  1.86  2.45 -0.22  0.20\n",
              "\n",
              "[5844 rows x 60 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr2nCML5v5cI"
      },
      "source": [
        "#dat_resaXpca_['Disease']=pd.DataFrame(dat_resaY)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5j-bJB2v5cK"
      },
      "source": [
        "#dat_resaXpca_['dbscan']=pd.DataFrame(lab4)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuCJ97cVv5cN",
        "outputId": "e4a31607-0edf-462a-c277-05a23a76062b"
      },
      "source": [
        "#dat_resaXpca_[dat_resaXpca_['dbscan']==1]['Disease'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "chronic lymphocytic leukaemia    959\n",
              "acute myeloid leukaemia          351\n",
              "Name: Disease, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww_MrVU9R7-D"
      },
      "source": [
        "#isomap = Isomap(n_components=60, n_neighbors=30,n_jobs=-1)\n",
        "#dat_resaXiso=isomap.fit_transform(dat_resaX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRZrmuki-MLM",
        "outputId": "34e4a004-2c2c-4736-826f-0b4fd74b6633"
      },
      "source": [
        "#isomap.reconstruction_error()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "758.7973986977535"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHeBNXiTEZpy",
        "outputId": "68a98105-aeaa-4db7-bbc8-c6ba81f38837"
      },
      "source": [
        "#dbscana2 = DBSCAN(eps=88, min_samples=200)\n",
        "#dbscana2.fit_predict(dat_resaXiso)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  0, -1, ...,  4,  4,  4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSzPzlTYm6mt"
      },
      "source": [
        "#lab2=list(dbscana2.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsizpqhCoU2_"
      },
      "source": [
        "#lab2_=list(filter(lambda x:x>-1,lab2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izf69Rovxq5R",
        "outputId": "9647434b-8bc4-43c5-ba06-b404b0a37e28"
      },
      "source": [
        "#len(lab2_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4645"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDVejMp3A7kS",
        "outputId": "a8d2bf49-77c7-4184-e7ae-5feba94291a0"
      },
      "source": [
        "#len(set(lab2_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYnWX6c_A7s_",
        "outputId": "10071bb2-300a-48d7-9abd-c5372b2c3d36"
      },
      "source": [
        "#lab2.count(-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "NfLr9MYbvPPC",
        "outputId": "eb2d353e-06dc-4877-bc41-244084dae55a"
      },
      "source": [
        "#dat_resaXiso_=pd.DataFrame(dat_resaXiso)\n",
        "#dat_resaXiso_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-71.53</td>\n",
              "      <td>38.63</td>\n",
              "      <td>20.87</td>\n",
              "      <td>36.94</td>\n",
              "      <td>-57.42</td>\n",
              "      <td>-4.21</td>\n",
              "      <td>-6.76</td>\n",
              "      <td>-2.18</td>\n",
              "      <td>-28.79</td>\n",
              "      <td>0.23</td>\n",
              "      <td>21.91</td>\n",
              "      <td>7.38</td>\n",
              "      <td>0.55</td>\n",
              "      <td>-13.87</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>9.65</td>\n",
              "      <td>2.17</td>\n",
              "      <td>24.18</td>\n",
              "      <td>-6.63</td>\n",
              "      <td>7.61</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-9.95</td>\n",
              "      <td>-16.51</td>\n",
              "      <td>8.58</td>\n",
              "      <td>-0.52</td>\n",
              "      <td>1.79</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-14.49</td>\n",
              "      <td>4.37</td>\n",
              "      <td>1.96</td>\n",
              "      <td>-2.58</td>\n",
              "      <td>8.72</td>\n",
              "      <td>-7.34</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>-2.91</td>\n",
              "      <td>1.02</td>\n",
              "      <td>-8.07</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>-9.13</td>\n",
              "      <td>-1.40e+01</td>\n",
              "      <td>4.58</td>\n",
              "      <td>7.16</td>\n",
              "      <td>-4.76</td>\n",
              "      <td>2.91</td>\n",
              "      <td>1.31</td>\n",
              "      <td>-8.36</td>\n",
              "      <td>3.47</td>\n",
              "      <td>-3.79</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>1.73</td>\n",
              "      <td>-7.18</td>\n",
              "      <td>7.26</td>\n",
              "      <td>1.08</td>\n",
              "      <td>-3.49</td>\n",
              "      <td>-2.88</td>\n",
              "      <td>0.70</td>\n",
              "      <td>6.99</td>\n",
              "      <td>4.05</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>7.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-44.48</td>\n",
              "      <td>13.78</td>\n",
              "      <td>53.00</td>\n",
              "      <td>39.41</td>\n",
              "      <td>19.06</td>\n",
              "      <td>-9.08</td>\n",
              "      <td>-27.76</td>\n",
              "      <td>0.24</td>\n",
              "      <td>13.73</td>\n",
              "      <td>-5.85</td>\n",
              "      <td>-21.44</td>\n",
              "      <td>-16.84</td>\n",
              "      <td>14.58</td>\n",
              "      <td>24.44</td>\n",
              "      <td>-12.66</td>\n",
              "      <td>-3.99</td>\n",
              "      <td>1.51</td>\n",
              "      <td>-5.42</td>\n",
              "      <td>-11.62</td>\n",
              "      <td>2.28</td>\n",
              "      <td>8.10</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>3.88</td>\n",
              "      <td>-4.57</td>\n",
              "      <td>-10.48</td>\n",
              "      <td>10.10</td>\n",
              "      <td>4.13</td>\n",
              "      <td>-19.00</td>\n",
              "      <td>-2.13</td>\n",
              "      <td>-1.94</td>\n",
              "      <td>2.83</td>\n",
              "      <td>3.94</td>\n",
              "      <td>-11.74</td>\n",
              "      <td>4.53</td>\n",
              "      <td>-5.05</td>\n",
              "      <td>6.45</td>\n",
              "      <td>-6.83</td>\n",
              "      <td>-2.36</td>\n",
              "      <td>1.89</td>\n",
              "      <td>-1.15e+01</td>\n",
              "      <td>-2.13</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.66</td>\n",
              "      <td>-11.98</td>\n",
              "      <td>-6.89</td>\n",
              "      <td>-7.13</td>\n",
              "      <td>9.94</td>\n",
              "      <td>7.08</td>\n",
              "      <td>7.53</td>\n",
              "      <td>-3.58</td>\n",
              "      <td>-5.54</td>\n",
              "      <td>1.48</td>\n",
              "      <td>-4.20</td>\n",
              "      <td>5.63</td>\n",
              "      <td>1.14</td>\n",
              "      <td>-6.02</td>\n",
              "      <td>5.22</td>\n",
              "      <td>9.46</td>\n",
              "      <td>1.31</td>\n",
              "      <td>-6.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-81.07</td>\n",
              "      <td>15.33</td>\n",
              "      <td>70.66</td>\n",
              "      <td>52.10</td>\n",
              "      <td>-7.90</td>\n",
              "      <td>11.38</td>\n",
              "      <td>4.65</td>\n",
              "      <td>-18.76</td>\n",
              "      <td>17.11</td>\n",
              "      <td>14.58</td>\n",
              "      <td>-3.45</td>\n",
              "      <td>5.34</td>\n",
              "      <td>-23.96</td>\n",
              "      <td>15.67</td>\n",
              "      <td>19.77</td>\n",
              "      <td>5.88</td>\n",
              "      <td>-6.82</td>\n",
              "      <td>-7.03</td>\n",
              "      <td>0.34</td>\n",
              "      <td>10.49</td>\n",
              "      <td>6.87</td>\n",
              "      <td>-6.09</td>\n",
              "      <td>1.22</td>\n",
              "      <td>-18.05</td>\n",
              "      <td>-7.09</td>\n",
              "      <td>16.11</td>\n",
              "      <td>-6.08</td>\n",
              "      <td>2.43</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>-9.25</td>\n",
              "      <td>-15.92</td>\n",
              "      <td>12.34</td>\n",
              "      <td>4.60</td>\n",
              "      <td>3.16</td>\n",
              "      <td>5.60</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.72</td>\n",
              "      <td>-3.43</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>8.28e+00</td>\n",
              "      <td>-7.16</td>\n",
              "      <td>4.31</td>\n",
              "      <td>1.38</td>\n",
              "      <td>-3.82</td>\n",
              "      <td>5.23</td>\n",
              "      <td>8.62</td>\n",
              "      <td>10.20</td>\n",
              "      <td>11.25</td>\n",
              "      <td>3.39</td>\n",
              "      <td>-11.91</td>\n",
              "      <td>-8.39</td>\n",
              "      <td>-9.76</td>\n",
              "      <td>-17.47</td>\n",
              "      <td>-2.28</td>\n",
              "      <td>-7.31</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>-10.82</td>\n",
              "      <td>7.68</td>\n",
              "      <td>4.58</td>\n",
              "      <td>-1.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-57.82</td>\n",
              "      <td>2.64</td>\n",
              "      <td>38.71</td>\n",
              "      <td>43.82</td>\n",
              "      <td>2.17</td>\n",
              "      <td>5.25</td>\n",
              "      <td>-20.24</td>\n",
              "      <td>6.39</td>\n",
              "      <td>19.50</td>\n",
              "      <td>15.54</td>\n",
              "      <td>-17.56</td>\n",
              "      <td>-11.67</td>\n",
              "      <td>26.90</td>\n",
              "      <td>17.72</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>-13.76</td>\n",
              "      <td>-7.66</td>\n",
              "      <td>5.36</td>\n",
              "      <td>-9.34</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>10.75</td>\n",
              "      <td>4.77</td>\n",
              "      <td>-3.46</td>\n",
              "      <td>-5.69</td>\n",
              "      <td>-12.70</td>\n",
              "      <td>7.78</td>\n",
              "      <td>18.28</td>\n",
              "      <td>-3.74</td>\n",
              "      <td>11.90</td>\n",
              "      <td>0.37</td>\n",
              "      <td>4.60</td>\n",
              "      <td>-11.47</td>\n",
              "      <td>-12.55</td>\n",
              "      <td>5.60</td>\n",
              "      <td>-8.74</td>\n",
              "      <td>-5.54</td>\n",
              "      <td>-22.42</td>\n",
              "      <td>-3.94</td>\n",
              "      <td>4.64</td>\n",
              "      <td>-1.53e+01</td>\n",
              "      <td>3.44</td>\n",
              "      <td>-8.06</td>\n",
              "      <td>-7.95</td>\n",
              "      <td>1.38</td>\n",
              "      <td>-16.99</td>\n",
              "      <td>-14.63</td>\n",
              "      <td>-1.08</td>\n",
              "      <td>10.72</td>\n",
              "      <td>11.10</td>\n",
              "      <td>-6.59</td>\n",
              "      <td>-2.38</td>\n",
              "      <td>1.44</td>\n",
              "      <td>-5.59</td>\n",
              "      <td>5.19</td>\n",
              "      <td>8.00</td>\n",
              "      <td>-8.85</td>\n",
              "      <td>16.60</td>\n",
              "      <td>-1.40</td>\n",
              "      <td>2.02</td>\n",
              "      <td>2.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-54.03</td>\n",
              "      <td>12.04</td>\n",
              "      <td>43.23</td>\n",
              "      <td>45.89</td>\n",
              "      <td>20.66</td>\n",
              "      <td>5.91</td>\n",
              "      <td>-25.03</td>\n",
              "      <td>7.48</td>\n",
              "      <td>20.13</td>\n",
              "      <td>11.84</td>\n",
              "      <td>-18.51</td>\n",
              "      <td>-21.11</td>\n",
              "      <td>26.01</td>\n",
              "      <td>17.24</td>\n",
              "      <td>-2.47</td>\n",
              "      <td>-5.15</td>\n",
              "      <td>-9.15</td>\n",
              "      <td>-11.21</td>\n",
              "      <td>-14.24</td>\n",
              "      <td>4.59</td>\n",
              "      <td>10.05</td>\n",
              "      <td>13.42</td>\n",
              "      <td>6.53</td>\n",
              "      <td>-21.46</td>\n",
              "      <td>-8.65</td>\n",
              "      <td>10.07</td>\n",
              "      <td>4.82</td>\n",
              "      <td>-15.60</td>\n",
              "      <td>-7.54</td>\n",
              "      <td>5.53</td>\n",
              "      <td>14.20</td>\n",
              "      <td>2.95</td>\n",
              "      <td>-9.84</td>\n",
              "      <td>5.96</td>\n",
              "      <td>-4.40</td>\n",
              "      <td>-6.71</td>\n",
              "      <td>-13.86</td>\n",
              "      <td>0.68</td>\n",
              "      <td>4.57</td>\n",
              "      <td>-1.41e+01</td>\n",
              "      <td>-2.78</td>\n",
              "      <td>-2.95</td>\n",
              "      <td>-3.11</td>\n",
              "      <td>-9.21</td>\n",
              "      <td>-11.95</td>\n",
              "      <td>-6.53</td>\n",
              "      <td>8.32</td>\n",
              "      <td>9.06</td>\n",
              "      <td>9.28</td>\n",
              "      <td>-2.64</td>\n",
              "      <td>-8.10</td>\n",
              "      <td>7.08</td>\n",
              "      <td>-5.30</td>\n",
              "      <td>2.80</td>\n",
              "      <td>4.92</td>\n",
              "      <td>-12.23</td>\n",
              "      <td>2.18</td>\n",
              "      <td>-1.07</td>\n",
              "      <td>0.84</td>\n",
              "      <td>-2.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5839</th>\n",
              "      <td>-45.05</td>\n",
              "      <td>103.74</td>\n",
              "      <td>-12.66</td>\n",
              "      <td>-46.17</td>\n",
              "      <td>-11.52</td>\n",
              "      <td>7.95</td>\n",
              "      <td>6.57</td>\n",
              "      <td>0.54</td>\n",
              "      <td>22.62</td>\n",
              "      <td>-3.32</td>\n",
              "      <td>8.39</td>\n",
              "      <td>2.53</td>\n",
              "      <td>5.20</td>\n",
              "      <td>-10.67</td>\n",
              "      <td>-11.24</td>\n",
              "      <td>6.08</td>\n",
              "      <td>-5.23</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>3.73</td>\n",
              "      <td>3.95</td>\n",
              "      <td>-2.82</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>8.83</td>\n",
              "      <td>3.29</td>\n",
              "      <td>-1.81</td>\n",
              "      <td>2.98</td>\n",
              "      <td>4.02</td>\n",
              "      <td>-3.31</td>\n",
              "      <td>2.38</td>\n",
              "      <td>-2.46</td>\n",
              "      <td>-3.31</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>-3.79</td>\n",
              "      <td>1.68</td>\n",
              "      <td>2.70</td>\n",
              "      <td>0.91</td>\n",
              "      <td>5.81</td>\n",
              "      <td>1.04</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>-9.94e-01</td>\n",
              "      <td>-4.22</td>\n",
              "      <td>3.24</td>\n",
              "      <td>-2.57</td>\n",
              "      <td>1.20</td>\n",
              "      <td>-3.70</td>\n",
              "      <td>-4.96</td>\n",
              "      <td>7.31</td>\n",
              "      <td>-5.32</td>\n",
              "      <td>4.03</td>\n",
              "      <td>-1.58</td>\n",
              "      <td>-2.09</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-3.79</td>\n",
              "      <td>-4.61</td>\n",
              "      <td>-3.15</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5840</th>\n",
              "      <td>17.10</td>\n",
              "      <td>68.20</td>\n",
              "      <td>-41.24</td>\n",
              "      <td>-18.55</td>\n",
              "      <td>24.07</td>\n",
              "      <td>48.25</td>\n",
              "      <td>-18.36</td>\n",
              "      <td>-15.96</td>\n",
              "      <td>-24.63</td>\n",
              "      <td>11.70</td>\n",
              "      <td>-18.65</td>\n",
              "      <td>2.38</td>\n",
              "      <td>-1.91</td>\n",
              "      <td>14.54</td>\n",
              "      <td>11.48</td>\n",
              "      <td>-1.99</td>\n",
              "      <td>-1.85</td>\n",
              "      <td>-5.14</td>\n",
              "      <td>1.58</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.90</td>\n",
              "      <td>1.56</td>\n",
              "      <td>-9.01</td>\n",
              "      <td>5.44</td>\n",
              "      <td>-10.75</td>\n",
              "      <td>-8.80</td>\n",
              "      <td>-8.00</td>\n",
              "      <td>-1.81</td>\n",
              "      <td>10.85</td>\n",
              "      <td>-3.79</td>\n",
              "      <td>-7.28</td>\n",
              "      <td>-0.67</td>\n",
              "      <td>0.93</td>\n",
              "      <td>3.71</td>\n",
              "      <td>1.54</td>\n",
              "      <td>-5.81</td>\n",
              "      <td>10.40</td>\n",
              "      <td>6.41</td>\n",
              "      <td>2.97</td>\n",
              "      <td>3.95e+00</td>\n",
              "      <td>-4.70</td>\n",
              "      <td>0.47</td>\n",
              "      <td>-5.16</td>\n",
              "      <td>-2.43</td>\n",
              "      <td>-9.68</td>\n",
              "      <td>-9.17</td>\n",
              "      <td>4.74</td>\n",
              "      <td>-5.42</td>\n",
              "      <td>-1.75</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-5.06</td>\n",
              "      <td>0.97</td>\n",
              "      <td>-3.97</td>\n",
              "      <td>2.54</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>7.50</td>\n",
              "      <td>3.35</td>\n",
              "      <td>-0.84</td>\n",
              "      <td>3.53</td>\n",
              "      <td>-3.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5841</th>\n",
              "      <td>13.23</td>\n",
              "      <td>72.09</td>\n",
              "      <td>-37.87</td>\n",
              "      <td>-19.53</td>\n",
              "      <td>1.48</td>\n",
              "      <td>30.26</td>\n",
              "      <td>-8.16</td>\n",
              "      <td>-15.01</td>\n",
              "      <td>-10.25</td>\n",
              "      <td>14.55</td>\n",
              "      <td>-19.89</td>\n",
              "      <td>-9.68</td>\n",
              "      <td>-9.83</td>\n",
              "      <td>4.48</td>\n",
              "      <td>-4.79</td>\n",
              "      <td>-7.61</td>\n",
              "      <td>-1.87</td>\n",
              "      <td>4.65</td>\n",
              "      <td>11.45</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.69</td>\n",
              "      <td>-7.02</td>\n",
              "      <td>-5.33</td>\n",
              "      <td>15.42</td>\n",
              "      <td>-9.95</td>\n",
              "      <td>-6.92</td>\n",
              "      <td>4.54</td>\n",
              "      <td>-2.05</td>\n",
              "      <td>2.38</td>\n",
              "      <td>1.88</td>\n",
              "      <td>-14.51</td>\n",
              "      <td>3.70</td>\n",
              "      <td>-12.41</td>\n",
              "      <td>3.79</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.27</td>\n",
              "      <td>11.22</td>\n",
              "      <td>5.76</td>\n",
              "      <td>2.31</td>\n",
              "      <td>1.22e+01</td>\n",
              "      <td>-17.02</td>\n",
              "      <td>5.73</td>\n",
              "      <td>1.01</td>\n",
              "      <td>0.57</td>\n",
              "      <td>-5.44</td>\n",
              "      <td>7.18</td>\n",
              "      <td>5.06</td>\n",
              "      <td>-10.65</td>\n",
              "      <td>-1.84</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>-4.21</td>\n",
              "      <td>4.58</td>\n",
              "      <td>-3.06</td>\n",
              "      <td>-2.12</td>\n",
              "      <td>-5.60</td>\n",
              "      <td>-8.17</td>\n",
              "      <td>-12.35</td>\n",
              "      <td>-8.51</td>\n",
              "      <td>-0.58</td>\n",
              "      <td>-2.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5842</th>\n",
              "      <td>18.80</td>\n",
              "      <td>80.12</td>\n",
              "      <td>-49.56</td>\n",
              "      <td>8.60</td>\n",
              "      <td>19.49</td>\n",
              "      <td>8.64</td>\n",
              "      <td>-33.13</td>\n",
              "      <td>-20.51</td>\n",
              "      <td>-19.83</td>\n",
              "      <td>20.86</td>\n",
              "      <td>-12.63</td>\n",
              "      <td>6.37</td>\n",
              "      <td>21.27</td>\n",
              "      <td>0.21</td>\n",
              "      <td>-22.21</td>\n",
              "      <td>-0.92</td>\n",
              "      <td>-4.32</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-0.82</td>\n",
              "      <td>4.03</td>\n",
              "      <td>-5.64</td>\n",
              "      <td>-15.39</td>\n",
              "      <td>-6.69</td>\n",
              "      <td>-3.60</td>\n",
              "      <td>-9.78</td>\n",
              "      <td>2.52</td>\n",
              "      <td>-3.51</td>\n",
              "      <td>1.86</td>\n",
              "      <td>-8.99</td>\n",
              "      <td>9.05</td>\n",
              "      <td>2.74</td>\n",
              "      <td>-3.36</td>\n",
              "      <td>3.54</td>\n",
              "      <td>-2.14</td>\n",
              "      <td>2.83</td>\n",
              "      <td>3.65</td>\n",
              "      <td>-5.40</td>\n",
              "      <td>-9.60</td>\n",
              "      <td>0.74</td>\n",
              "      <td>7.01e-03</td>\n",
              "      <td>6.09</td>\n",
              "      <td>-2.23</td>\n",
              "      <td>4.91</td>\n",
              "      <td>-3.70</td>\n",
              "      <td>5.60</td>\n",
              "      <td>-1.80</td>\n",
              "      <td>1.49</td>\n",
              "      <td>-4.29</td>\n",
              "      <td>-0.70</td>\n",
              "      <td>-2.66</td>\n",
              "      <td>-3.04</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.81</td>\n",
              "      <td>-1.55</td>\n",
              "      <td>-1.27</td>\n",
              "      <td>0.12</td>\n",
              "      <td>1.27</td>\n",
              "      <td>5.84</td>\n",
              "      <td>1.42</td>\n",
              "      <td>2.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5843</th>\n",
              "      <td>-2.12</td>\n",
              "      <td>86.55</td>\n",
              "      <td>-10.17</td>\n",
              "      <td>-49.20</td>\n",
              "      <td>0.43</td>\n",
              "      <td>-13.67</td>\n",
              "      <td>28.16</td>\n",
              "      <td>-25.36</td>\n",
              "      <td>-3.38</td>\n",
              "      <td>-3.02</td>\n",
              "      <td>5.96</td>\n",
              "      <td>-4.02</td>\n",
              "      <td>-7.93</td>\n",
              "      <td>-13.49</td>\n",
              "      <td>9.55</td>\n",
              "      <td>11.33</td>\n",
              "      <td>10.40</td>\n",
              "      <td>9.07</td>\n",
              "      <td>21.43</td>\n",
              "      <td>1.45</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-6.85</td>\n",
              "      <td>-10.83</td>\n",
              "      <td>4.43</td>\n",
              "      <td>5.06</td>\n",
              "      <td>13.45</td>\n",
              "      <td>-11.04</td>\n",
              "      <td>3.98</td>\n",
              "      <td>-2.85</td>\n",
              "      <td>7.20</td>\n",
              "      <td>-8.76</td>\n",
              "      <td>-1.01</td>\n",
              "      <td>-0.78</td>\n",
              "      <td>8.21</td>\n",
              "      <td>-2.01</td>\n",
              "      <td>-0.54</td>\n",
              "      <td>0.56</td>\n",
              "      <td>-1.71</td>\n",
              "      <td>11.21</td>\n",
              "      <td>3.92e+00</td>\n",
              "      <td>-1.10</td>\n",
              "      <td>-11.10</td>\n",
              "      <td>0.87</td>\n",
              "      <td>5.63</td>\n",
              "      <td>-2.58</td>\n",
              "      <td>-12.62</td>\n",
              "      <td>-10.50</td>\n",
              "      <td>4.90</td>\n",
              "      <td>2.42</td>\n",
              "      <td>2.89</td>\n",
              "      <td>8.14</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>-3.03</td>\n",
              "      <td>3.51</td>\n",
              "      <td>1.60</td>\n",
              "      <td>9.82</td>\n",
              "      <td>-8.85</td>\n",
              "      <td>6.31</td>\n",
              "      <td>-5.43</td>\n",
              "      <td>6.90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5844 rows × 60 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0       1      2      3      4   ...     55     56    57    58    59\n",
              "0    -71.53   38.63  20.87  36.94 -57.42  ...   0.70   6.99  4.05 -0.20  7.96\n",
              "1    -44.48   13.78  53.00  39.41  19.06  ...  -6.02   5.22  9.46  1.31 -6.62\n",
              "2    -81.07   15.33  70.66  52.10  -7.90  ...  -0.13 -10.82  7.68  4.58 -1.82\n",
              "3    -57.82    2.64  38.71  43.82   2.17  ...  -8.85  16.60 -1.40  2.02  2.79\n",
              "4    -54.03   12.04  43.23  45.89  20.66  ... -12.23   2.18 -1.07  0.84 -2.19\n",
              "...     ...     ...    ...    ...    ...  ...    ...    ...   ...   ...   ...\n",
              "5839 -45.05  103.74 -12.66 -46.17 -11.52  ...   0.79   0.30  0.05  0.40  1.71\n",
              "5840  17.10   68.20 -41.24 -18.55  24.07  ...   7.50   3.35 -0.84  3.53 -3.99\n",
              "5841  13.23   72.09 -37.87 -19.53   1.48  ...  -8.17 -12.35 -8.51 -0.58 -2.57\n",
              "5842  18.80   80.12 -49.56   8.60  19.49  ...   0.12   1.27  5.84  1.42  2.82\n",
              "5843  -2.12   86.55 -10.17 -49.20   0.43  ...   9.82  -8.85  6.31 -5.43  6.90\n",
              "\n",
              "[5844 rows x 60 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMKDuTR1vPPJ"
      },
      "source": [
        "#dat_resaXiso_['Disease']=pd.DataFrame(dat_resaY)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTLXdqMDvPPM"
      },
      "source": [
        "#dat_resaXiso_['dbscan']=pd.DataFrame(lab2)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w22INQ7hvPPO",
        "outputId": "d5b49047-d593-4264-cfae-d846550be8d0"
      },
      "source": [
        "#dat_resaXiso_[dat_resaXiso_['dbscan']==0]['Disease'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "acute myeloid leukaemia          505\n",
              "multiple myeloma                  27\n",
              "chronic lymphocytic leukaemia      1\n",
              "Name: Disease, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFV9wyeBYozp"
      },
      "source": [
        "#input_dim = 53999\n",
        "#latent_dim1 =5000\n",
        "#latent_dim2=500\n",
        "#latent_dim3=60\n",
        "#input_layer = Input(shape=(input_dim,), name='input')\n",
        "\n",
        "#encoded1 = Dense(latent_dim1, \n",
        "#                activation='relu', name='features1')(input_layer)\n",
        "#encoded2 = Dense(latent_dim2, \n",
        "#                activation='relu', name='features2')(encoded1)\n",
        "#encoded3 = Dense(latent_dim3, \n",
        "#                activation='relu', name='features3')(encoded2)\n",
        "\n",
        "#decoded1 = Dense(latent_dim2, activation='relu', name='reconstructed1')(encoded3)\n",
        "#decoded2 = Dense(latent_dim1, activation='relu', name='reconstructed2')(decoded1)\n",
        "#decoded3 = Dense(input_dim, activation='sigmoid', name='reconstructed3')(decoded2)\n",
        "\n",
        "#autoencoder = Model(inputs=[input_layer], outputs=[decoded3])\n",
        "\n",
        "#autoencoder.compile(optimizer='adam', \n",
        "#                    loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrXA4fl2-MuI",
        "outputId": "6b5db532-fc81-4cad-b94a-3cab4701e1b7"
      },
      "source": [
        "#autoencoder.fit(dat_resaX,dat_resaX ,\n",
        "#                epochs=1000,\n",
        "#                batch_size=3000,\n",
        "#                shuffle=True,\n",
        "#                validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "2/2 [==============================] - 2s 857ms/step - loss: 0.6925 - val_loss: 0.6456\n",
            "Epoch 2/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.6433 - val_loss: 0.9775\n",
            "Epoch 3/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.8606 - val_loss: 0.6570\n",
            "Epoch 4/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.6501 - val_loss: 0.6312\n",
            "Epoch 5/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.6249 - val_loss: 0.6089\n",
            "Epoch 6/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.6110 - val_loss: 0.6116\n",
            "Epoch 7/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.6112 - val_loss: 0.6069\n",
            "Epoch 8/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.6061 - val_loss: 0.6050\n",
            "Epoch 9/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.6051 - val_loss: 0.6043\n",
            "Epoch 10/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.6045 - val_loss: 0.6033\n",
            "Epoch 11/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.6035 - val_loss: 0.6035\n",
            "Epoch 12/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.6035 - val_loss: 0.6032\n",
            "Epoch 13/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.6033 - val_loss: 0.6029\n",
            "Epoch 14/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.6030 - val_loss: 0.6027\n",
            "Epoch 15/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.6030 - val_loss: 0.6028\n",
            "Epoch 16/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.6029 - val_loss: 0.6028\n",
            "Epoch 17/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.6028 - val_loss: 0.6027\n",
            "Epoch 18/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.6027 - val_loss: 0.6028\n",
            "Epoch 19/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.6027 - val_loss: 0.6026\n",
            "Epoch 20/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.6027 - val_loss: 0.6026\n",
            "Epoch 21/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.6026 - val_loss: 0.6025\n",
            "Epoch 22/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.6026 - val_loss: 0.6025\n",
            "Epoch 23/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.6025 - val_loss: 0.6025\n",
            "Epoch 24/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.6025 - val_loss: 0.6025\n",
            "Epoch 25/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.6025 - val_loss: 0.6025\n",
            "Epoch 26/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.6025 - val_loss: 0.6025\n",
            "Epoch 27/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.6024 - val_loss: 0.6025\n",
            "Epoch 28/1000\n",
            "2/2 [==============================] - 2s 924ms/step - loss: 0.6023 - val_loss: 0.6024\n",
            "Epoch 29/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.6022 - val_loss: 0.6023\n",
            "Epoch 30/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.6019 - val_loss: 0.6022\n",
            "Epoch 31/1000\n",
            "2/2 [==============================] - 2s 949ms/step - loss: 0.6015 - val_loss: 0.6018\n",
            "Epoch 32/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.6006 - val_loss: 0.6010\n",
            "Epoch 33/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5990 - val_loss: 0.5999\n",
            "Epoch 34/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5968 - val_loss: 0.5991\n",
            "Epoch 35/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5955 - val_loss: 0.5994\n",
            "Epoch 36/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5957 - val_loss: 0.6007\n",
            "Epoch 37/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5958 - val_loss: 0.5986\n",
            "Epoch 38/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5950 - val_loss: 0.5985\n",
            "Epoch 39/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5952 - val_loss: 0.5981\n",
            "Epoch 40/1000\n",
            "2/2 [==============================] - 2s 923ms/step - loss: 0.5948 - val_loss: 0.5978\n",
            "Epoch 41/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5944 - val_loss: 0.5975\n",
            "Epoch 42/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5942 - val_loss: 0.5969\n",
            "Epoch 43/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5938 - val_loss: 0.5964\n",
            "Epoch 44/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5934 - val_loss: 0.5959\n",
            "Epoch 45/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5931 - val_loss: 0.5955\n",
            "Epoch 46/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5928 - val_loss: 0.5951\n",
            "Epoch 47/1000\n",
            "2/2 [==============================] - 2s 974ms/step - loss: 0.5925 - val_loss: 0.5952\n",
            "Epoch 48/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5924 - val_loss: 0.5951\n",
            "Epoch 49/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5922 - val_loss: 0.5951\n",
            "Epoch 50/1000\n",
            "2/2 [==============================] - 2s 969ms/step - loss: 0.5922 - val_loss: 0.5952\n",
            "Epoch 51/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5921 - val_loss: 0.5951\n",
            "Epoch 52/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5920 - val_loss: 0.5952\n",
            "Epoch 53/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5917 - val_loss: 0.5952\n",
            "Epoch 54/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5913 - val_loss: 0.5952\n",
            "Epoch 55/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5910 - val_loss: 0.5952\n",
            "Epoch 56/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5907 - val_loss: 0.5953\n",
            "Epoch 57/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5902 - val_loss: 0.5958\n",
            "Epoch 58/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5909 - val_loss: 0.6014\n",
            "Epoch 59/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5959 - val_loss: 0.6030\n",
            "Epoch 60/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5944 - val_loss: 0.6002\n",
            "Epoch 61/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5930 - val_loss: 0.5977\n",
            "Epoch 62/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5918 - val_loss: 0.5960\n",
            "Epoch 63/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5906 - val_loss: 0.5953\n",
            "Epoch 64/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5896 - val_loss: 0.5950\n",
            "Epoch 65/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5891 - val_loss: 0.5959\n",
            "Epoch 66/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5892 - val_loss: 0.5940\n",
            "Epoch 67/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5884 - val_loss: 0.5936\n",
            "Epoch 68/1000\n",
            "2/2 [==============================] - 2s 935ms/step - loss: 0.5881 - val_loss: 0.5933\n",
            "Epoch 69/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5879 - val_loss: 0.5926\n",
            "Epoch 70/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5873 - val_loss: 0.5921\n",
            "Epoch 71/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5870 - val_loss: 0.5915\n",
            "Epoch 72/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5865 - val_loss: 0.5907\n",
            "Epoch 73/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5861 - val_loss: 0.5903\n",
            "Epoch 74/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5858 - val_loss: 0.5903\n",
            "Epoch 75/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5856 - val_loss: 0.5897\n",
            "Epoch 76/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5853 - val_loss: 0.5893\n",
            "Epoch 77/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5850 - val_loss: 0.5890\n",
            "Epoch 78/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5848 - val_loss: 0.5887\n",
            "Epoch 79/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5846 - val_loss: 0.5884\n",
            "Epoch 80/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5843 - val_loss: 0.5882\n",
            "Epoch 81/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5841 - val_loss: 0.5878\n",
            "Epoch 82/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5838 - val_loss: 0.5874\n",
            "Epoch 83/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5837 - val_loss: 0.5873\n",
            "Epoch 84/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5841 - val_loss: 0.5871\n",
            "Epoch 85/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5836 - val_loss: 0.5864\n",
            "Epoch 86/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5831 - val_loss: 0.5860\n",
            "Epoch 87/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5828 - val_loss: 0.5855\n",
            "Epoch 88/1000\n",
            "2/2 [==============================] - 2s 971ms/step - loss: 0.5827 - val_loss: 0.5850\n",
            "Epoch 89/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5822 - val_loss: 0.5851\n",
            "Epoch 90/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5821 - val_loss: 0.5842\n",
            "Epoch 91/1000\n",
            "2/2 [==============================] - 2s 972ms/step - loss: 0.5817 - val_loss: 0.5837\n",
            "Epoch 92/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5812 - val_loss: 0.5837\n",
            "Epoch 93/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5810 - val_loss: 0.5833\n",
            "Epoch 94/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5809 - val_loss: 0.5846\n",
            "Epoch 95/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5822 - val_loss: 0.5853\n",
            "Epoch 96/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5821 - val_loss: 0.5848\n",
            "Epoch 97/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5819 - val_loss: 0.5840\n",
            "Epoch 98/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5812 - val_loss: 0.5829\n",
            "Epoch 99/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5806 - val_loss: 0.5821\n",
            "Epoch 100/1000\n",
            "2/2 [==============================] - 2s 970ms/step - loss: 0.5799 - val_loss: 0.5820\n",
            "Epoch 101/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5796 - val_loss: 0.5819\n",
            "Epoch 102/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5795 - val_loss: 0.5819\n",
            "Epoch 103/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5793 - val_loss: 0.5814\n",
            "Epoch 104/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5791 - val_loss: 0.5813\n",
            "Epoch 105/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5788 - val_loss: 0.5809\n",
            "Epoch 106/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5785 - val_loss: 0.5808\n",
            "Epoch 107/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5784 - val_loss: 0.5806\n",
            "Epoch 108/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5782 - val_loss: 0.5804\n",
            "Epoch 109/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5781 - val_loss: 0.5806\n",
            "Epoch 110/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5783 - val_loss: 0.5805\n",
            "Epoch 111/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5781 - val_loss: 0.5800\n",
            "Epoch 112/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5777 - val_loss: 0.5801\n",
            "Epoch 113/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5778 - val_loss: 0.5797\n",
            "Epoch 114/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5774 - val_loss: 0.5798\n",
            "Epoch 115/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5774 - val_loss: 0.5795\n",
            "Epoch 116/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5772 - val_loss: 0.5795\n",
            "Epoch 117/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5772 - val_loss: 0.5793\n",
            "Epoch 118/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5770 - val_loss: 0.5792\n",
            "Epoch 119/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5769 - val_loss: 0.5791\n",
            "Epoch 120/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5768 - val_loss: 0.5789\n",
            "Epoch 121/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5766 - val_loss: 0.5789\n",
            "Epoch 122/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5766 - val_loss: 0.5788\n",
            "Epoch 123/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5766 - val_loss: 0.5787\n",
            "Epoch 124/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5764 - val_loss: 0.5786\n",
            "Epoch 125/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5764 - val_loss: 0.5786\n",
            "Epoch 126/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5764 - val_loss: 0.5786\n",
            "Epoch 127/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5763 - val_loss: 0.5784\n",
            "Epoch 128/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5761 - val_loss: 0.5782\n",
            "Epoch 129/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5760 - val_loss: 0.5782\n",
            "Epoch 130/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5760 - val_loss: 0.5781\n",
            "Epoch 131/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5759 - val_loss: 0.5781\n",
            "Epoch 132/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5759 - val_loss: 0.5781\n",
            "Epoch 133/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5759 - val_loss: 0.5781\n",
            "Epoch 134/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5759 - val_loss: 0.5778\n",
            "Epoch 135/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5756 - val_loss: 0.5778\n",
            "Epoch 136/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5755 - val_loss: 0.5776\n",
            "Epoch 137/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5753 - val_loss: 0.5775\n",
            "Epoch 138/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5753 - val_loss: 0.5775\n",
            "Epoch 139/1000\n",
            "2/2 [==============================] - 2s 922ms/step - loss: 0.5753 - val_loss: 0.5776\n",
            "Epoch 140/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5754 - val_loss: 0.5780\n",
            "Epoch 141/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5758 - val_loss: 0.5777\n",
            "Epoch 142/1000\n",
            "2/2 [==============================] - 2s 926ms/step - loss: 0.5754 - val_loss: 0.5776\n",
            "Epoch 143/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5753 - val_loss: 0.5774\n",
            "Epoch 144/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5750 - val_loss: 0.5774\n",
            "Epoch 145/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5750 - val_loss: 0.5771\n",
            "Epoch 146/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5747 - val_loss: 0.5772\n",
            "Epoch 147/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5748 - val_loss: 0.5770\n",
            "Epoch 148/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5746 - val_loss: 0.5770\n",
            "Epoch 149/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5745 - val_loss: 0.5769\n",
            "Epoch 150/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5744 - val_loss: 0.5769\n",
            "Epoch 151/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5744 - val_loss: 0.5771\n",
            "Epoch 152/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5746 - val_loss: 0.5774\n",
            "Epoch 153/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5749 - val_loss: 0.5778\n",
            "Epoch 154/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5751 - val_loss: 0.5775\n",
            "Epoch 155/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5748 - val_loss: 0.5768\n",
            "Epoch 156/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5744 - val_loss: 0.5769\n",
            "Epoch 157/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5743 - val_loss: 0.5771\n",
            "Epoch 158/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5743 - val_loss: 0.5766\n",
            "Epoch 159/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5741 - val_loss: 0.5765\n",
            "Epoch 160/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5739 - val_loss: 0.5765\n",
            "Epoch 161/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5738 - val_loss: 0.5765\n",
            "Epoch 162/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5738 - val_loss: 0.5764\n",
            "Epoch 163/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5737 - val_loss: 0.5763\n",
            "Epoch 164/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5736 - val_loss: 0.5764\n",
            "Epoch 165/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5736 - val_loss: 0.5764\n",
            "Epoch 166/1000\n",
            "2/2 [==============================] - 2s 924ms/step - loss: 0.5736 - val_loss: 0.5764\n",
            "Epoch 167/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5736 - val_loss: 0.5762\n",
            "Epoch 168/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5734 - val_loss: 0.5762\n",
            "Epoch 169/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5734 - val_loss: 0.5764\n",
            "Epoch 170/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5735 - val_loss: 0.5763\n",
            "Epoch 171/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5734 - val_loss: 0.5761\n",
            "Epoch 172/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5732 - val_loss: 0.5761\n",
            "Epoch 173/1000\n",
            "2/2 [==============================] - 2s 934ms/step - loss: 0.5733 - val_loss: 0.5764\n",
            "Epoch 174/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5735 - val_loss: 0.5763\n",
            "Epoch 175/1000\n",
            "2/2 [==============================] - 2s 929ms/step - loss: 0.5734 - val_loss: 0.5761\n",
            "Epoch 176/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5733 - val_loss: 0.5763\n",
            "Epoch 177/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5732 - val_loss: 0.5759\n",
            "Epoch 178/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5730 - val_loss: 0.5758\n",
            "Epoch 179/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5729 - val_loss: 0.5758\n",
            "Epoch 180/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5729 - val_loss: 0.5757\n",
            "Epoch 181/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5727 - val_loss: 0.5757\n",
            "Epoch 182/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5726 - val_loss: 0.5756\n",
            "Epoch 183/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5726 - val_loss: 0.5757\n",
            "Epoch 184/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5727 - val_loss: 0.5756\n",
            "Epoch 185/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5726 - val_loss: 0.5759\n",
            "Epoch 186/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5730 - val_loss: 0.5763\n",
            "Epoch 187/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5736 - val_loss: 0.5760\n",
            "Epoch 188/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5730 - val_loss: 0.5762\n",
            "Epoch 189/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5730 - val_loss: 0.5757\n",
            "Epoch 190/1000\n",
            "2/2 [==============================] - 2s 918ms/step - loss: 0.5727 - val_loss: 0.5755\n",
            "Epoch 191/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5724 - val_loss: 0.5755\n",
            "Epoch 192/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5724 - val_loss: 0.5755\n",
            "Epoch 193/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5723 - val_loss: 0.5754\n",
            "Epoch 194/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5723 - val_loss: 0.5752\n",
            "Epoch 195/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5721 - val_loss: 0.5753\n",
            "Epoch 196/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5721 - val_loss: 0.5753\n",
            "Epoch 197/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5721 - val_loss: 0.5754\n",
            "Epoch 198/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5723 - val_loss: 0.5762\n",
            "Epoch 199/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5732 - val_loss: 0.5765\n",
            "Epoch 200/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5727 - val_loss: 0.5754\n",
            "Epoch 201/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5724 - val_loss: 0.5751\n",
            "Epoch 202/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5721 - val_loss: 0.5754\n",
            "Epoch 203/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5721 - val_loss: 0.5754\n",
            "Epoch 204/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5721 - val_loss: 0.5751\n",
            "Epoch 205/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5719 - val_loss: 0.5752\n",
            "Epoch 206/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5719 - val_loss: 0.5751\n",
            "Epoch 207/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5718 - val_loss: 0.5750\n",
            "Epoch 208/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5717 - val_loss: 0.5749\n",
            "Epoch 209/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5716 - val_loss: 0.5749\n",
            "Epoch 210/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5715 - val_loss: 0.5750\n",
            "Epoch 211/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5715 - val_loss: 0.5748\n",
            "Epoch 212/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5714 - val_loss: 0.5747\n",
            "Epoch 213/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5714 - val_loss: 0.5748\n",
            "Epoch 214/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5713 - val_loss: 0.5747\n",
            "Epoch 215/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5713 - val_loss: 0.5747\n",
            "Epoch 216/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5713 - val_loss: 0.5747\n",
            "Epoch 217/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5713 - val_loss: 0.5750\n",
            "Epoch 218/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5717 - val_loss: 0.5756\n",
            "Epoch 219/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5719 - val_loss: 0.5748\n",
            "Epoch 220/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5715 - val_loss: 0.5754\n",
            "Epoch 221/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5720 - val_loss: 0.5753\n",
            "Epoch 222/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5717 - val_loss: 0.5748\n",
            "Epoch 223/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5714 - val_loss: 0.5748\n",
            "Epoch 224/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5713 - val_loss: 0.5746\n",
            "Epoch 225/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5712 - val_loss: 0.5746\n",
            "Epoch 226/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5711 - val_loss: 0.5746\n",
            "Epoch 227/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5711 - val_loss: 0.5746\n",
            "Epoch 228/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5710 - val_loss: 0.5744\n",
            "Epoch 229/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5710 - val_loss: 0.5744\n",
            "Epoch 230/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5709 - val_loss: 0.5744\n",
            "Epoch 231/1000\n",
            "2/2 [==============================] - 2s 924ms/step - loss: 0.5709 - val_loss: 0.5744\n",
            "Epoch 232/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5708 - val_loss: 0.5744\n",
            "Epoch 233/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5708 - val_loss: 0.5743\n",
            "Epoch 234/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5707 - val_loss: 0.5743\n",
            "Epoch 235/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5707 - val_loss: 0.5742\n",
            "Epoch 236/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5707 - val_loss: 0.5742\n",
            "Epoch 237/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5706 - val_loss: 0.5742\n",
            "Epoch 238/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5706 - val_loss: 0.5744\n",
            "Epoch 239/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5709 - val_loss: 0.5756\n",
            "Epoch 240/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5723 - val_loss: 0.5755\n",
            "Epoch 241/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5718 - val_loss: 0.5756\n",
            "Epoch 242/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5716 - val_loss: 0.5750\n",
            "Epoch 243/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5712 - val_loss: 0.5747\n",
            "Epoch 244/1000\n",
            "2/2 [==============================] - 2s 950ms/step - loss: 0.5711 - val_loss: 0.5748\n",
            "Epoch 245/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5710 - val_loss: 0.5746\n",
            "Epoch 246/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5709 - val_loss: 0.5745\n",
            "Epoch 247/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5708 - val_loss: 0.5744\n",
            "Epoch 248/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5707 - val_loss: 0.5742\n",
            "Epoch 249/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5706 - val_loss: 0.5742\n",
            "Epoch 250/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5705 - val_loss: 0.5741\n",
            "Epoch 251/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5705 - val_loss: 0.5741\n",
            "Epoch 252/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5704 - val_loss: 0.5740\n",
            "Epoch 253/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5704 - val_loss: 0.5741\n",
            "Epoch 254/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5704 - val_loss: 0.5742\n",
            "Epoch 255/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5707 - val_loss: 0.5746\n",
            "Epoch 256/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5710 - val_loss: 0.5743\n",
            "Epoch 257/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5706 - val_loss: 0.5746\n",
            "Epoch 258/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5708 - val_loss: 0.5740\n",
            "Epoch 259/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5703 - val_loss: 0.5742\n",
            "Epoch 260/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5705 - val_loss: 0.5739\n",
            "Epoch 261/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5703 - val_loss: 0.5739\n",
            "Epoch 262/1000\n",
            "2/2 [==============================] - 2s 972ms/step - loss: 0.5702 - val_loss: 0.5740\n",
            "Epoch 263/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5702 - val_loss: 0.5738\n",
            "Epoch 264/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5701 - val_loss: 0.5738\n",
            "Epoch 265/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5701 - val_loss: 0.5739\n",
            "Epoch 266/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5702 - val_loss: 0.5739\n",
            "Epoch 267/1000\n",
            "2/2 [==============================] - 2s 946ms/step - loss: 0.5703 - val_loss: 0.5739\n",
            "Epoch 268/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5702 - val_loss: 0.5737\n",
            "Epoch 269/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5700 - val_loss: 0.5738\n",
            "Epoch 270/1000\n",
            "2/2 [==============================] - 2s 971ms/step - loss: 0.5700 - val_loss: 0.5738\n",
            "Epoch 271/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5700 - val_loss: 0.5737\n",
            "Epoch 272/1000\n",
            "2/2 [==============================] - 2s 928ms/step - loss: 0.5700 - val_loss: 0.5736\n",
            "Epoch 273/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5698 - val_loss: 0.5736\n",
            "Epoch 274/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5698 - val_loss: 0.5736\n",
            "Epoch 275/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5698 - val_loss: 0.5737\n",
            "Epoch 276/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5699 - val_loss: 0.5737\n",
            "Epoch 277/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5699 - val_loss: 0.5739\n",
            "Epoch 278/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5703 - val_loss: 0.5742\n",
            "Epoch 279/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5703 - val_loss: 0.5736\n",
            "Epoch 280/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5698 - val_loss: 0.5737\n",
            "Epoch 281/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5699 - val_loss: 0.5738\n",
            "Epoch 282/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5700 - val_loss: 0.5740\n",
            "Epoch 283/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5703 - val_loss: 0.5743\n",
            "Epoch 284/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5704 - val_loss: 0.5737\n",
            "Epoch 285/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5699 - val_loss: 0.5738\n",
            "Epoch 286/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5699 - val_loss: 0.5737\n",
            "Epoch 287/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5699 - val_loss: 0.5735\n",
            "Epoch 288/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5697 - val_loss: 0.5736\n",
            "Epoch 289/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5696 - val_loss: 0.5736\n",
            "Epoch 290/1000\n",
            "2/2 [==============================] - 2s 969ms/step - loss: 0.5697 - val_loss: 0.5734\n",
            "Epoch 291/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5695 - val_loss: 0.5735\n",
            "Epoch 292/1000\n",
            "2/2 [==============================] - 2s 928ms/step - loss: 0.5696 - val_loss: 0.5735\n",
            "Epoch 293/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5696 - val_loss: 0.5734\n",
            "Epoch 294/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5694 - val_loss: 0.5734\n",
            "Epoch 295/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5694 - val_loss: 0.5734\n",
            "Epoch 296/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5695 - val_loss: 0.5733\n",
            "Epoch 297/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5694 - val_loss: 0.5734\n",
            "Epoch 298/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5694 - val_loss: 0.5734\n",
            "Epoch 299/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5694 - val_loss: 0.5734\n",
            "Epoch 300/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5694 - val_loss: 0.5735\n",
            "Epoch 301/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5695 - val_loss: 0.5737\n",
            "Epoch 302/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5699 - val_loss: 0.5749\n",
            "Epoch 303/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5710 - val_loss: 0.5754\n",
            "Epoch 304/1000\n",
            "2/2 [==============================] - 2s 933ms/step - loss: 0.5707 - val_loss: 0.5744\n",
            "Epoch 305/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5707 - val_loss: 0.5734\n",
            "Epoch 306/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5698 - val_loss: 0.5737\n",
            "Epoch 307/1000\n",
            "2/2 [==============================] - 2s 928ms/step - loss: 0.5699 - val_loss: 0.5736\n",
            "Epoch 308/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5697 - val_loss: 0.5735\n",
            "Epoch 309/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5696 - val_loss: 0.5734\n",
            "Epoch 310/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5695 - val_loss: 0.5733\n",
            "Epoch 311/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5694 - val_loss: 0.5733\n",
            "Epoch 312/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5694 - val_loss: 0.5733\n",
            "Epoch 313/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5693 - val_loss: 0.5732\n",
            "Epoch 314/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5692 - val_loss: 0.5731\n",
            "Epoch 315/1000\n",
            "2/2 [==============================] - 2s 895ms/step - loss: 0.5692 - val_loss: 0.5732\n",
            "Epoch 316/1000\n",
            "2/2 [==============================] - 2s 923ms/step - loss: 0.5692 - val_loss: 0.5732\n",
            "Epoch 317/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5691 - val_loss: 0.5731\n",
            "Epoch 318/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5691 - val_loss: 0.5731\n",
            "Epoch 319/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5691 - val_loss: 0.5732\n",
            "Epoch 320/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5691 - val_loss: 0.5732\n",
            "Epoch 321/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5691 - val_loss: 0.5732\n",
            "Epoch 322/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5691 - val_loss: 0.5731\n",
            "Epoch 323/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5690 - val_loss: 0.5731\n",
            "Epoch 324/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5690 - val_loss: 0.5731\n",
            "Epoch 325/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5689 - val_loss: 0.5731\n",
            "Epoch 326/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5689 - val_loss: 0.5730\n",
            "Epoch 327/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5689 - val_loss: 0.5731\n",
            "Epoch 328/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5689 - val_loss: 0.5731\n",
            "Epoch 329/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5690 - val_loss: 0.5733\n",
            "Epoch 330/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5692 - val_loss: 0.5735\n",
            "Epoch 331/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5695 - val_loss: 0.5737\n",
            "Epoch 332/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5696 - val_loss: 0.5734\n",
            "Epoch 333/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5694 - val_loss: 0.5733\n",
            "Epoch 334/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5692 - val_loss: 0.5735\n",
            "Epoch 335/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5692 - val_loss: 0.5731\n",
            "Epoch 336/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5691 - val_loss: 0.5730\n",
            "Epoch 337/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5689 - val_loss: 0.5732\n",
            "Epoch 338/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5690 - val_loss: 0.5730\n",
            "Epoch 339/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5689 - val_loss: 0.5729\n",
            "Epoch 340/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5687 - val_loss: 0.5729\n",
            "Epoch 341/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5687 - val_loss: 0.5729\n",
            "Epoch 342/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5687 - val_loss: 0.5729\n",
            "Epoch 343/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5686 - val_loss: 0.5728\n",
            "Epoch 344/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5686 - val_loss: 0.5728\n",
            "Epoch 345/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5686 - val_loss: 0.5728\n",
            "Epoch 346/1000\n",
            "2/2 [==============================] - 2s 927ms/step - loss: 0.5686 - val_loss: 0.5728\n",
            "Epoch 347/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5685 - val_loss: 0.5728\n",
            "Epoch 348/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5685 - val_loss: 0.5729\n",
            "Epoch 349/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5687 - val_loss: 0.5735\n",
            "Epoch 350/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5696 - val_loss: 0.5740\n",
            "Epoch 351/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5697 - val_loss: 0.5732\n",
            "Epoch 352/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5693 - val_loss: 0.5733\n",
            "Epoch 353/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5690 - val_loss: 0.5734\n",
            "Epoch 354/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5691 - val_loss: 0.5730\n",
            "Epoch 355/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5688 - val_loss: 0.5730\n",
            "Epoch 356/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5688 - val_loss: 0.5728\n",
            "Epoch 357/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5686 - val_loss: 0.5728\n",
            "Epoch 358/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5686 - val_loss: 0.5728\n",
            "Epoch 359/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5685 - val_loss: 0.5728\n",
            "Epoch 360/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5685 - val_loss: 0.5729\n",
            "Epoch 361/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5686 - val_loss: 0.5727\n",
            "Epoch 362/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5684 - val_loss: 0.5726\n",
            "Epoch 363/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5684 - val_loss: 0.5726\n",
            "Epoch 364/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5683 - val_loss: 0.5727\n",
            "Epoch 365/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5683 - val_loss: 0.5727\n",
            "Epoch 366/1000\n",
            "2/2 [==============================] - 2s 925ms/step - loss: 0.5683 - val_loss: 0.5726\n",
            "Epoch 367/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5683 - val_loss: 0.5726\n",
            "Epoch 368/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5683 - val_loss: 0.5727\n",
            "Epoch 369/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5684 - val_loss: 0.5728\n",
            "Epoch 370/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5685 - val_loss: 0.5726\n",
            "Epoch 371/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5683 - val_loss: 0.5727\n",
            "Epoch 372/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5684 - val_loss: 0.5729\n",
            "Epoch 373/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5686 - val_loss: 0.5727\n",
            "Epoch 374/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5683 - val_loss: 0.5727\n",
            "Epoch 375/1000\n",
            "2/2 [==============================] - 2s 939ms/step - loss: 0.5684 - val_loss: 0.5727\n",
            "Epoch 376/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5683 - val_loss: 0.5726\n",
            "Epoch 377/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5683 - val_loss: 0.5727\n",
            "Epoch 378/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5683 - val_loss: 0.5725\n",
            "Epoch 379/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5681 - val_loss: 0.5725\n",
            "Epoch 380/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5682 - val_loss: 0.5724\n",
            "Epoch 381/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5680 - val_loss: 0.5725\n",
            "Epoch 382/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5681 - val_loss: 0.5724\n",
            "Epoch 383/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5680 - val_loss: 0.5725\n",
            "Epoch 384/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5681 - val_loss: 0.5725\n",
            "Epoch 385/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5682 - val_loss: 0.5728\n",
            "Epoch 386/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5686 - val_loss: 0.5730\n",
            "Epoch 387/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5687 - val_loss: 0.5727\n",
            "Epoch 388/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5684 - val_loss: 0.5728\n",
            "Epoch 389/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5683 - val_loss: 0.5725\n",
            "Epoch 390/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5682 - val_loss: 0.5725\n",
            "Epoch 391/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5681 - val_loss: 0.5726\n",
            "Epoch 392/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5681 - val_loss: 0.5724\n",
            "Epoch 393/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5680 - val_loss: 0.5724\n",
            "Epoch 394/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5680 - val_loss: 0.5724\n",
            "Epoch 395/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5680 - val_loss: 0.5723\n",
            "Epoch 396/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5679 - val_loss: 0.5725\n",
            "Epoch 397/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5680 - val_loss: 0.5727\n",
            "Epoch 398/1000\n",
            "2/2 [==============================] - 2s 928ms/step - loss: 0.5683 - val_loss: 0.5731\n",
            "Epoch 399/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5685 - val_loss: 0.5725\n",
            "Epoch 400/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5681 - val_loss: 0.5726\n",
            "Epoch 401/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5682 - val_loss: 0.5727\n",
            "Epoch 402/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5681 - val_loss: 0.5725\n",
            "Epoch 403/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5681 - val_loss: 0.5723\n",
            "Epoch 404/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5679 - val_loss: 0.5725\n",
            "Epoch 405/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5679 - val_loss: 0.5723\n",
            "Epoch 406/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5678 - val_loss: 0.5722\n",
            "Epoch 407/1000\n",
            "2/2 [==============================] - 2s 930ms/step - loss: 0.5678 - val_loss: 0.5723\n",
            "Epoch 408/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5678 - val_loss: 0.5723\n",
            "Epoch 409/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5678 - val_loss: 0.5722\n",
            "Epoch 410/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5678 - val_loss: 0.5724\n",
            "Epoch 411/1000\n",
            "2/2 [==============================] - 2s 924ms/step - loss: 0.5679 - val_loss: 0.5723\n",
            "Epoch 412/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5678 - val_loss: 0.5722\n",
            "Epoch 413/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5677 - val_loss: 0.5723\n",
            "Epoch 414/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5678 - val_loss: 0.5723\n",
            "Epoch 415/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5678 - val_loss: 0.5725\n",
            "Epoch 416/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5680 - val_loss: 0.5725\n",
            "Epoch 417/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5678 - val_loss: 0.5722\n",
            "Epoch 418/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5678 - val_loss: 0.5725\n",
            "Epoch 419/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5680 - val_loss: 0.5724\n",
            "Epoch 420/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5679 - val_loss: 0.5723\n",
            "Epoch 421/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5677 - val_loss: 0.5722\n",
            "Epoch 422/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5677 - val_loss: 0.5722\n",
            "Epoch 423/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5676 - val_loss: 0.5721\n",
            "Epoch 424/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5676 - val_loss: 0.5721\n",
            "Epoch 425/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5675 - val_loss: 0.5721\n",
            "Epoch 426/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5676 - val_loss: 0.5721\n",
            "Epoch 427/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5676 - val_loss: 0.5723\n",
            "Epoch 428/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5678 - val_loss: 0.5724\n",
            "Epoch 429/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5678 - val_loss: 0.5723\n",
            "Epoch 430/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5678 - val_loss: 0.5725\n",
            "Epoch 431/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5680 - val_loss: 0.5725\n",
            "Epoch 432/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5678 - val_loss: 0.5723\n",
            "Epoch 433/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5677 - val_loss: 0.5723\n",
            "Epoch 434/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5677 - val_loss: 0.5723\n",
            "Epoch 435/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5677 - val_loss: 0.5722\n",
            "Epoch 436/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5676 - val_loss: 0.5721\n",
            "Epoch 437/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5675 - val_loss: 0.5721\n",
            "Epoch 438/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5675 - val_loss: 0.5721\n",
            "Epoch 439/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5675 - val_loss: 0.5721\n",
            "Epoch 440/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5675 - val_loss: 0.5722\n",
            "Epoch 441/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5675 - val_loss: 0.5720\n",
            "Epoch 442/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5674 - val_loss: 0.5721\n",
            "Epoch 443/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5675 - val_loss: 0.5722\n",
            "Epoch 444/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5677 - val_loss: 0.5725\n",
            "Epoch 445/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5679 - val_loss: 0.5725\n",
            "Epoch 446/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5678 - val_loss: 0.5724\n",
            "Epoch 447/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5677 - val_loss: 0.5723\n",
            "Epoch 448/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5676 - val_loss: 0.5720\n",
            "Epoch 449/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5674 - val_loss: 0.5721\n",
            "Epoch 450/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5674 - val_loss: 0.5720\n",
            "Epoch 451/1000\n",
            "2/2 [==============================] - 2s 926ms/step - loss: 0.5673 - val_loss: 0.5719\n",
            "Epoch 452/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5673 - val_loss: 0.5720\n",
            "Epoch 453/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5674 - val_loss: 0.5720\n",
            "Epoch 454/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5673 - val_loss: 0.5720\n",
            "Epoch 455/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5673 - val_loss: 0.5719\n",
            "Epoch 456/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5672 - val_loss: 0.5719\n",
            "Epoch 457/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5672 - val_loss: 0.5718\n",
            "Epoch 458/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5671 - val_loss: 0.5718\n",
            "Epoch 459/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5671 - val_loss: 0.5719\n",
            "Epoch 460/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5672 - val_loss: 0.5719\n",
            "Epoch 461/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5672 - val_loss: 0.5720\n",
            "Epoch 462/1000\n",
            "2/2 [==============================] - 2s 930ms/step - loss: 0.5673 - val_loss: 0.5721\n",
            "Epoch 463/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5675 - val_loss: 0.5726\n",
            "Epoch 464/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5678 - val_loss: 0.5723\n",
            "Epoch 465/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5675 - val_loss: 0.5726\n",
            "Epoch 466/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5679 - val_loss: 0.5720\n",
            "Epoch 467/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5673 - val_loss: 0.5723\n",
            "Epoch 468/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5675 - val_loss: 0.5719\n",
            "Epoch 469/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5672 - val_loss: 0.5719\n",
            "Epoch 470/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5671 - val_loss: 0.5720\n",
            "Epoch 471/1000\n",
            "2/2 [==============================] - 2s 974ms/step - loss: 0.5672 - val_loss: 0.5718\n",
            "Epoch 472/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5671 - val_loss: 0.5719\n",
            "Epoch 473/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5671 - val_loss: 0.5720\n",
            "Epoch 474/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5672 - val_loss: 0.5719\n",
            "Epoch 475/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5672 - val_loss: 0.5720\n",
            "Epoch 476/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5671 - val_loss: 0.5718\n",
            "Epoch 477/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5671 - val_loss: 0.5720\n",
            "Epoch 478/1000\n",
            "2/2 [==============================] - 2s 923ms/step - loss: 0.5672 - val_loss: 0.5719\n",
            "Epoch 479/1000\n",
            "2/2 [==============================] - 2s 928ms/step - loss: 0.5671 - val_loss: 0.5719\n",
            "Epoch 480/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5671 - val_loss: 0.5719\n",
            "Epoch 481/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5671 - val_loss: 0.5718\n",
            "Epoch 482/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5670 - val_loss: 0.5721\n",
            "Epoch 483/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5672 - val_loss: 0.5722\n",
            "Epoch 484/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5673 - val_loss: 0.5720\n",
            "Epoch 485/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5673 - val_loss: 0.5720\n",
            "Epoch 486/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5672 - val_loss: 0.5719\n",
            "Epoch 487/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5671 - val_loss: 0.5719\n",
            "Epoch 488/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5670 - val_loss: 0.5718\n",
            "Epoch 489/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5670 - val_loss: 0.5717\n",
            "Epoch 490/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5668 - val_loss: 0.5717\n",
            "Epoch 491/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5669 - val_loss: 0.5716\n",
            "Epoch 492/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5668 - val_loss: 0.5716\n",
            "Epoch 493/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5668 - val_loss: 0.5716\n",
            "Epoch 494/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5668 - val_loss: 0.5716\n",
            "Epoch 495/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5668 - val_loss: 0.5717\n",
            "Epoch 496/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5669 - val_loss: 0.5721\n",
            "Epoch 497/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5673 - val_loss: 0.5724\n",
            "Epoch 498/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5676 - val_loss: 0.5720\n",
            "Epoch 499/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5673 - val_loss: 0.5721\n",
            "Epoch 500/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5672 - val_loss: 0.5719\n",
            "Epoch 501/1000\n",
            "2/2 [==============================] - 2s 926ms/step - loss: 0.5671 - val_loss: 0.5718\n",
            "Epoch 502/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5669 - val_loss: 0.5719\n",
            "Epoch 503/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5670 - val_loss: 0.5716\n",
            "Epoch 504/1000\n",
            "2/2 [==============================] - 2s 932ms/step - loss: 0.5668 - val_loss: 0.5716\n",
            "Epoch 505/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5668 - val_loss: 0.5717\n",
            "Epoch 506/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5668 - val_loss: 0.5717\n",
            "Epoch 507/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5668 - val_loss: 0.5718\n",
            "Epoch 508/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5670 - val_loss: 0.5721\n",
            "Epoch 509/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5672 - val_loss: 0.5719\n",
            "Epoch 510/1000\n",
            "2/2 [==============================] - 2s 969ms/step - loss: 0.5670 - val_loss: 0.5718\n",
            "Epoch 511/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5669 - val_loss: 0.5719\n",
            "Epoch 512/1000\n",
            "2/2 [==============================] - 2s 935ms/step - loss: 0.5670 - val_loss: 0.5717\n",
            "Epoch 513/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5668 - val_loss: 0.5716\n",
            "Epoch 514/1000\n",
            "2/2 [==============================] - 2s 950ms/step - loss: 0.5668 - val_loss: 0.5717\n",
            "Epoch 515/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5668 - val_loss: 0.5716\n",
            "Epoch 516/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5667 - val_loss: 0.5717\n",
            "Epoch 517/1000\n",
            "2/2 [==============================] - 2s 950ms/step - loss: 0.5668 - val_loss: 0.5716\n",
            "Epoch 518/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5667 - val_loss: 0.5717\n",
            "Epoch 519/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5668 - val_loss: 0.5715\n",
            "Epoch 520/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5666 - val_loss: 0.5716\n",
            "Epoch 521/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5667 - val_loss: 0.5716\n",
            "Epoch 522/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5666 - val_loss: 0.5716\n",
            "Epoch 523/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5667 - val_loss: 0.5717\n",
            "Epoch 524/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5667 - val_loss: 0.5715\n",
            "Epoch 525/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5666 - val_loss: 0.5716\n",
            "Epoch 526/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5667 - val_loss: 0.5717\n",
            "Epoch 527/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5668 - val_loss: 0.5718\n",
            "Epoch 528/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5669 - val_loss: 0.5718\n",
            "Epoch 529/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5667 - val_loss: 0.5716\n",
            "Epoch 530/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5667 - val_loss: 0.5716\n",
            "Epoch 531/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5667 - val_loss: 0.5717\n",
            "Epoch 532/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5667 - val_loss: 0.5717\n",
            "Epoch 533/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5667 - val_loss: 0.5719\n",
            "Epoch 534/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5668 - val_loss: 0.5717\n",
            "Epoch 535/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5667 - val_loss: 0.5715\n",
            "Epoch 536/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5666 - val_loss: 0.5716\n",
            "Epoch 537/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5666 - val_loss: 0.5715\n",
            "Epoch 538/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5665 - val_loss: 0.5715\n",
            "Epoch 539/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5664 - val_loss: 0.5715\n",
            "Epoch 540/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5664 - val_loss: 0.5713\n",
            "Epoch 541/1000\n",
            "2/2 [==============================] - 2s 969ms/step - loss: 0.5663 - val_loss: 0.5715\n",
            "Epoch 542/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5665 - val_loss: 0.5715\n",
            "Epoch 543/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5665 - val_loss: 0.5718\n",
            "Epoch 544/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5667 - val_loss: 0.5716\n",
            "Epoch 545/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5665 - val_loss: 0.5716\n",
            "Epoch 546/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5666 - val_loss: 0.5718\n",
            "Epoch 547/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5668 - val_loss: 0.5717\n",
            "Epoch 548/1000\n",
            "2/2 [==============================] - 2s 930ms/step - loss: 0.5667 - val_loss: 0.5717\n",
            "Epoch 549/1000\n",
            "2/2 [==============================] - 2s 969ms/step - loss: 0.5667 - val_loss: 0.5717\n",
            "Epoch 550/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5666 - val_loss: 0.5716\n",
            "Epoch 551/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5666 - val_loss: 0.5714\n",
            "Epoch 552/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5663 - val_loss: 0.5715\n",
            "Epoch 553/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5665 - val_loss: 0.5714\n",
            "Epoch 554/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5664 - val_loss: 0.5715\n",
            "Epoch 555/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5665 - val_loss: 0.5714\n",
            "Epoch 556/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5664 - val_loss: 0.5715\n",
            "Epoch 557/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5664 - val_loss: 0.5714\n",
            "Epoch 558/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5663 - val_loss: 0.5717\n",
            "Epoch 559/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5666 - val_loss: 0.5718\n",
            "Epoch 560/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5666 - val_loss: 0.5716\n",
            "Epoch 561/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5665 - val_loss: 0.5717\n",
            "Epoch 562/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5667 - val_loss: 0.5715\n",
            "Epoch 563/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5664 - val_loss: 0.5714\n",
            "Epoch 564/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5663 - val_loss: 0.5714\n",
            "Epoch 565/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5663 - val_loss: 0.5714\n",
            "Epoch 566/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5663 - val_loss: 0.5714\n",
            "Epoch 567/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5663 - val_loss: 0.5713\n",
            "Epoch 568/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5662 - val_loss: 0.5715\n",
            "Epoch 569/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5663 - val_loss: 0.5714\n",
            "Epoch 570/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5663 - val_loss: 0.5713\n",
            "Epoch 571/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5662 - val_loss: 0.5714\n",
            "Epoch 572/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5663 - val_loss: 0.5713\n",
            "Epoch 573/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5661 - val_loss: 0.5712\n",
            "Epoch 574/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5661 - val_loss: 0.5713\n",
            "Epoch 575/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5661 - val_loss: 0.5713\n",
            "Epoch 576/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5662 - val_loss: 0.5714\n",
            "Epoch 577/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5663 - val_loss: 0.5715\n",
            "Epoch 578/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5663 - val_loss: 0.5716\n",
            "Epoch 579/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5664 - val_loss: 0.5718\n",
            "Epoch 580/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5667 - val_loss: 0.5717\n",
            "Epoch 581/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5666 - val_loss: 0.5715\n",
            "Epoch 582/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5664 - val_loss: 0.5716\n",
            "Epoch 583/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5665 - val_loss: 0.5713\n",
            "Epoch 584/1000\n",
            "2/2 [==============================] - 2s 923ms/step - loss: 0.5662 - val_loss: 0.5716\n",
            "Epoch 585/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5663 - val_loss: 0.5713\n",
            "Epoch 586/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5662 - val_loss: 0.5714\n",
            "Epoch 587/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5662 - val_loss: 0.5714\n",
            "Epoch 588/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5662 - val_loss: 0.5713\n",
            "Epoch 589/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5661 - val_loss: 0.5711\n",
            "Epoch 590/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5660 - val_loss: 0.5713\n",
            "Epoch 591/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5660 - val_loss: 0.5713\n",
            "Epoch 592/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5661 - val_loss: 0.5713\n",
            "Epoch 593/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5661 - val_loss: 0.5712\n",
            "Epoch 594/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5660 - val_loss: 0.5712\n",
            "Epoch 595/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5659 - val_loss: 0.5711\n",
            "Epoch 596/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5659 - val_loss: 0.5712\n",
            "Epoch 597/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5660 - val_loss: 0.5714\n",
            "Epoch 598/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5661 - val_loss: 0.5713\n",
            "Epoch 599/1000\n",
            "2/2 [==============================] - 2s 949ms/step - loss: 0.5661 - val_loss: 0.5714\n",
            "Epoch 600/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5663 - val_loss: 0.5716\n",
            "Epoch 601/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5663 - val_loss: 0.5712\n",
            "Epoch 602/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5660 - val_loss: 0.5715\n",
            "Epoch 603/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5663 - val_loss: 0.5714\n",
            "Epoch 604/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5662 - val_loss: 0.5716\n",
            "Epoch 605/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5663 - val_loss: 0.5714\n",
            "Epoch 606/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5661 - val_loss: 0.5714\n",
            "Epoch 607/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5660 - val_loss: 0.5712\n",
            "Epoch 608/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5660 - val_loss: 0.5712\n",
            "Epoch 609/1000\n",
            "2/2 [==============================] - 2s 926ms/step - loss: 0.5659 - val_loss: 0.5712\n",
            "Epoch 610/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5660 - val_loss: 0.5711\n",
            "Epoch 611/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5659 - val_loss: 0.5713\n",
            "Epoch 612/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5659 - val_loss: 0.5711\n",
            "Epoch 613/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5659 - val_loss: 0.5712\n",
            "Epoch 614/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5659 - val_loss: 0.5713\n",
            "Epoch 615/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5660 - val_loss: 0.5713\n",
            "Epoch 616/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5661 - val_loss: 0.5713\n",
            "Epoch 617/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5660 - val_loss: 0.5712\n",
            "Epoch 618/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5659 - val_loss: 0.5714\n",
            "Epoch 619/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5661 - val_loss: 0.5711\n",
            "Epoch 620/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5658 - val_loss: 0.5712\n",
            "Epoch 621/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5659 - val_loss: 0.5712\n",
            "Epoch 622/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5659 - val_loss: 0.5711\n",
            "Epoch 623/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5658 - val_loss: 0.5712\n",
            "Epoch 624/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5658 - val_loss: 0.5711\n",
            "Epoch 625/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5658 - val_loss: 0.5712\n",
            "Epoch 626/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5658 - val_loss: 0.5711\n",
            "Epoch 627/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5658 - val_loss: 0.5712\n",
            "Epoch 628/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5658 - val_loss: 0.5711\n",
            "Epoch 629/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5657 - val_loss: 0.5712\n",
            "Epoch 630/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5658 - val_loss: 0.5712\n",
            "Epoch 631/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5658 - val_loss: 0.5713\n",
            "Epoch 632/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5660 - val_loss: 0.5714\n",
            "Epoch 633/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5660 - val_loss: 0.5714\n",
            "Epoch 634/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5662 - val_loss: 0.5714\n",
            "Epoch 635/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5659 - val_loss: 0.5711\n",
            "Epoch 636/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5659 - val_loss: 0.5713\n",
            "Epoch 637/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5660 - val_loss: 0.5711\n",
            "Epoch 638/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5658 - val_loss: 0.5710\n",
            "Epoch 639/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5657 - val_loss: 0.5712\n",
            "Epoch 640/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5657 - val_loss: 0.5709\n",
            "Epoch 641/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5656 - val_loss: 0.5710\n",
            "Epoch 642/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5656 - val_loss: 0.5709\n",
            "Epoch 643/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5656 - val_loss: 0.5709\n",
            "Epoch 644/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5655 - val_loss: 0.5709\n",
            "Epoch 645/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5655 - val_loss: 0.5709\n",
            "Epoch 646/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5655 - val_loss: 0.5709\n",
            "Epoch 647/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5656 - val_loss: 0.5712\n",
            "Epoch 648/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5660 - val_loss: 0.5715\n",
            "Epoch 649/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5662 - val_loss: 0.5711\n",
            "Epoch 650/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5659 - val_loss: 0.5716\n",
            "Epoch 651/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5664 - val_loss: 0.5712\n",
            "Epoch 652/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5658 - val_loss: 0.5713\n",
            "Epoch 653/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5660 - val_loss: 0.5710\n",
            "Epoch 654/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5657 - val_loss: 0.5711\n",
            "Epoch 655/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5656 - val_loss: 0.5710\n",
            "Epoch 656/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5656 - val_loss: 0.5710\n",
            "Epoch 657/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5656 - val_loss: 0.5711\n",
            "Epoch 658/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5656 - val_loss: 0.5710\n",
            "Epoch 659/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5656 - val_loss: 0.5709\n",
            "Epoch 660/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5655 - val_loss: 0.5710\n",
            "Epoch 661/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5655 - val_loss: 0.5710\n",
            "Epoch 662/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5656 - val_loss: 0.5710\n",
            "Epoch 663/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5655 - val_loss: 0.5709\n",
            "Epoch 664/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5655 - val_loss: 0.5709\n",
            "Epoch 665/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5656 - val_loss: 0.5712\n",
            "Epoch 666/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5658 - val_loss: 0.5711\n",
            "Epoch 667/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5656 - val_loss: 0.5709\n",
            "Epoch 668/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5655 - val_loss: 0.5711\n",
            "Epoch 669/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5657 - val_loss: 0.5708\n",
            "Epoch 670/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5654 - val_loss: 0.5710\n",
            "Epoch 671/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5655 - val_loss: 0.5709\n",
            "Epoch 672/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5654 - val_loss: 0.5708\n",
            "Epoch 673/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5654 - val_loss: 0.5709\n",
            "Epoch 674/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5654 - val_loss: 0.5710\n",
            "Epoch 675/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5655 - val_loss: 0.5713\n",
            "Epoch 676/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5658 - val_loss: 0.5712\n",
            "Epoch 677/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5657 - val_loss: 0.5711\n",
            "Epoch 678/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5657 - val_loss: 0.5713\n",
            "Epoch 679/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5658 - val_loss: 0.5712\n",
            "Epoch 680/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5657 - val_loss: 0.5711\n",
            "Epoch 681/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5657 - val_loss: 0.5711\n",
            "Epoch 682/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5655 - val_loss: 0.5709\n",
            "Epoch 683/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5654 - val_loss: 0.5709\n",
            "Epoch 684/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5654 - val_loss: 0.5708\n",
            "Epoch 685/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5653 - val_loss: 0.5709\n",
            "Epoch 686/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5654 - val_loss: 0.5709\n",
            "Epoch 687/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5654 - val_loss: 0.5708\n",
            "Epoch 688/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 689/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5652 - val_loss: 0.5707\n",
            "Epoch 690/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 691/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5653 - val_loss: 0.5710\n",
            "Epoch 692/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5655 - val_loss: 0.5712\n",
            "Epoch 693/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5657 - val_loss: 0.5712\n",
            "Epoch 694/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5657 - val_loss: 0.5714\n",
            "Epoch 695/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5657 - val_loss: 0.5711\n",
            "Epoch 696/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5656 - val_loss: 0.5711\n",
            "Epoch 697/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5655 - val_loss: 0.5710\n",
            "Epoch 698/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5655 - val_loss: 0.5708\n",
            "Epoch 699/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5653 - val_loss: 0.5710\n",
            "Epoch 700/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5654 - val_loss: 0.5708\n",
            "Epoch 701/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5653 - val_loss: 0.5709\n",
            "Epoch 702/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5654 - val_loss: 0.5708\n",
            "Epoch 703/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 704/1000\n",
            "2/2 [==============================] - 2s 930ms/step - loss: 0.5652 - val_loss: 0.5709\n",
            "Epoch 705/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5654 - val_loss: 0.5709\n",
            "Epoch 706/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5653 - val_loss: 0.5708\n",
            "Epoch 707/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 708/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 709/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 710/1000\n",
            "2/2 [==============================] - 2s 927ms/step - loss: 0.5653 - val_loss: 0.5708\n",
            "Epoch 711/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5652 - val_loss: 0.5707\n",
            "Epoch 712/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5651 - val_loss: 0.5708\n",
            "Epoch 713/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5653 - val_loss: 0.5708\n",
            "Epoch 714/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5653 - val_loss: 0.5711\n",
            "Epoch 715/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5655 - val_loss: 0.5708\n",
            "Epoch 716/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 717/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5653 - val_loss: 0.5709\n",
            "Epoch 718/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5652 - val_loss: 0.5709\n",
            "Epoch 719/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 720/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5652 - val_loss: 0.5707\n",
            "Epoch 721/1000\n",
            "2/2 [==============================] - 2s 971ms/step - loss: 0.5652 - val_loss: 0.5709\n",
            "Epoch 722/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5652 - val_loss: 0.5706\n",
            "Epoch 723/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5650 - val_loss: 0.5707\n",
            "Epoch 724/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5651 - val_loss: 0.5706\n",
            "Epoch 725/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5650 - val_loss: 0.5708\n",
            "Epoch 726/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5651 - val_loss: 0.5708\n",
            "Epoch 727/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5652 - val_loss: 0.5709\n",
            "Epoch 728/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5653 - val_loss: 0.5709\n",
            "Epoch 729/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5653 - val_loss: 0.5711\n",
            "Epoch 730/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5655 - val_loss: 0.5711\n",
            "Epoch 731/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5653 - val_loss: 0.5708\n",
            "Epoch 732/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 733/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5652 - val_loss: 0.5709\n",
            "Epoch 734/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5653 - val_loss: 0.5707\n",
            "Epoch 735/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5651 - val_loss: 0.5708\n",
            "Epoch 736/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5651 - val_loss: 0.5705\n",
            "Epoch 737/1000\n",
            "2/2 [==============================] - 2s 968ms/step - loss: 0.5649 - val_loss: 0.5706\n",
            "Epoch 738/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5650 - val_loss: 0.5706\n",
            "Epoch 739/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5650 - val_loss: 0.5706\n",
            "Epoch 740/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5649 - val_loss: 0.5707\n",
            "Epoch 741/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5651 - val_loss: 0.5708\n",
            "Epoch 742/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 743/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 744/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5651 - val_loss: 0.5707\n",
            "Epoch 745/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5651 - val_loss: 0.5710\n",
            "Epoch 746/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5653 - val_loss: 0.5708\n",
            "Epoch 747/1000\n",
            "2/2 [==============================] - 2s 924ms/step - loss: 0.5651 - val_loss: 0.5706\n",
            "Epoch 748/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5650 - val_loss: 0.5707\n",
            "Epoch 749/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5649 - val_loss: 0.5705\n",
            "Epoch 750/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5648 - val_loss: 0.5706\n",
            "Epoch 751/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5649 - val_loss: 0.5705\n",
            "Epoch 752/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5648 - val_loss: 0.5707\n",
            "Epoch 753/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5650 - val_loss: 0.5706\n",
            "Epoch 754/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5649 - val_loss: 0.5708\n",
            "Epoch 755/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5651 - val_loss: 0.5707\n",
            "Epoch 756/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5650 - val_loss: 0.5708\n",
            "Epoch 757/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5652 - val_loss: 0.5708\n",
            "Epoch 758/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5651 - val_loss: 0.5708\n",
            "Epoch 759/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5650 - val_loss: 0.5705\n",
            "Epoch 760/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5648 - val_loss: 0.5707\n",
            "Epoch 761/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5649 - val_loss: 0.5706\n",
            "Epoch 762/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5648 - val_loss: 0.5705\n",
            "Epoch 763/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5647 - val_loss: 0.5705\n",
            "Epoch 764/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5648 - val_loss: 0.5706\n",
            "Epoch 765/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5649 - val_loss: 0.5710\n",
            "Epoch 766/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5653 - val_loss: 0.5708\n",
            "Epoch 767/1000\n",
            "2/2 [==============================] - 2s 926ms/step - loss: 0.5649 - val_loss: 0.5705\n",
            "Epoch 768/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5648 - val_loss: 0.5707\n",
            "Epoch 769/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5649 - val_loss: 0.5706\n",
            "Epoch 770/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5648 - val_loss: 0.5707\n",
            "Epoch 771/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5649 - val_loss: 0.5706\n",
            "Epoch 772/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5648 - val_loss: 0.5705\n",
            "Epoch 773/1000\n",
            "2/2 [==============================] - 2s 969ms/step - loss: 0.5648 - val_loss: 0.5705\n",
            "Epoch 774/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5647 - val_loss: 0.5705\n",
            "Epoch 775/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5647 - val_loss: 0.5704\n",
            "Epoch 776/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5646 - val_loss: 0.5704\n",
            "Epoch 777/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5646 - val_loss: 0.5704\n",
            "Epoch 778/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5646 - val_loss: 0.5704\n",
            "Epoch 779/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5646 - val_loss: 0.5704\n",
            "Epoch 780/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5646 - val_loss: 0.5705\n",
            "Epoch 781/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5647 - val_loss: 0.5707\n",
            "Epoch 782/1000\n",
            "2/2 [==============================] - 2s 931ms/step - loss: 0.5649 - val_loss: 0.5707\n",
            "Epoch 783/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5648 - val_loss: 0.5708\n",
            "Epoch 784/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5651 - val_loss: 0.5711\n",
            "Epoch 785/1000\n",
            "2/2 [==============================] - 2s 932ms/step - loss: 0.5652 - val_loss: 0.5706\n",
            "Epoch 786/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5648 - val_loss: 0.5711\n",
            "Epoch 787/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5652 - val_loss: 0.5704\n",
            "Epoch 788/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5647 - val_loss: 0.5707\n",
            "Epoch 789/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5648 - val_loss: 0.5706\n",
            "Epoch 790/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5647 - val_loss: 0.5704\n",
            "Epoch 791/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5646 - val_loss: 0.5705\n",
            "Epoch 792/1000\n",
            "2/2 [==============================] - 2s 925ms/step - loss: 0.5647 - val_loss: 0.5705\n",
            "Epoch 793/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5647 - val_loss: 0.5705\n",
            "Epoch 794/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5647 - val_loss: 0.5705\n",
            "Epoch 795/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5647 - val_loss: 0.5704\n",
            "Epoch 796/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5646 - val_loss: 0.5705\n",
            "Epoch 797/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5647 - val_loss: 0.5707\n",
            "Epoch 798/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5649 - val_loss: 0.5706\n",
            "Epoch 799/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5649 - val_loss: 0.5707\n",
            "Epoch 800/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5649 - val_loss: 0.5709\n",
            "Epoch 801/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5649 - val_loss: 0.5705\n",
            "Epoch 802/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5647 - val_loss: 0.5708\n",
            "Epoch 803/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5649 - val_loss: 0.5705\n",
            "Epoch 804/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5646 - val_loss: 0.5705\n",
            "Epoch 805/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5647 - val_loss: 0.5705\n",
            "Epoch 806/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5647 - val_loss: 0.5708\n",
            "Epoch 807/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5649 - val_loss: 0.5705\n",
            "Epoch 808/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5647 - val_loss: 0.5706\n",
            "Epoch 809/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5647 - val_loss: 0.5704\n",
            "Epoch 810/1000\n",
            "2/2 [==============================] - 2s 926ms/step - loss: 0.5645 - val_loss: 0.5705\n",
            "Epoch 811/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5646 - val_loss: 0.5704\n",
            "Epoch 812/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5646 - val_loss: 0.5703\n",
            "Epoch 813/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5644 - val_loss: 0.5704\n",
            "Epoch 814/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5645 - val_loss: 0.5703\n",
            "Epoch 815/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5644 - val_loss: 0.5704\n",
            "Epoch 816/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5645 - val_loss: 0.5704\n",
            "Epoch 817/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5645 - val_loss: 0.5703\n",
            "Epoch 818/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5644 - val_loss: 0.5704\n",
            "Epoch 819/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5645 - val_loss: 0.5705\n",
            "Epoch 820/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5646 - val_loss: 0.5705\n",
            "Epoch 821/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5646 - val_loss: 0.5704\n",
            "Epoch 822/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5646 - val_loss: 0.5706\n",
            "Epoch 823/1000\n",
            "2/2 [==============================] - 2s 969ms/step - loss: 0.5647 - val_loss: 0.5705\n",
            "Epoch 824/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5647 - val_loss: 0.5707\n",
            "Epoch 825/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5647 - val_loss: 0.5704\n",
            "Epoch 826/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5645 - val_loss: 0.5706\n",
            "Epoch 827/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5646 - val_loss: 0.5702\n",
            "Epoch 828/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5643 - val_loss: 0.5704\n",
            "Epoch 829/1000\n",
            "2/2 [==============================] - 2s 936ms/step - loss: 0.5645 - val_loss: 0.5704\n",
            "Epoch 830/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5645 - val_loss: 0.5705\n",
            "Epoch 831/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5645 - val_loss: 0.5704\n",
            "Epoch 832/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5645 - val_loss: 0.5704\n",
            "Epoch 833/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5644 - val_loss: 0.5703\n",
            "Epoch 834/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5644 - val_loss: 0.5704\n",
            "Epoch 835/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5644 - val_loss: 0.5703\n",
            "Epoch 836/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5643 - val_loss: 0.5703\n",
            "Epoch 837/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5644 - val_loss: 0.5703\n",
            "Epoch 838/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5644 - val_loss: 0.5704\n",
            "Epoch 839/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5644 - val_loss: 0.5701\n",
            "Epoch 840/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5642 - val_loss: 0.5703\n",
            "Epoch 841/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5644 - val_loss: 0.5704\n",
            "Epoch 842/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5644 - val_loss: 0.5704\n",
            "Epoch 843/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5644 - val_loss: 0.5704\n",
            "Epoch 844/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5644 - val_loss: 0.5703\n",
            "Epoch 845/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5643 - val_loss: 0.5703\n",
            "Epoch 846/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5644 - val_loss: 0.5707\n",
            "Epoch 847/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5646 - val_loss: 0.5704\n",
            "Epoch 848/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5644 - val_loss: 0.5703\n",
            "Epoch 849/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5643 - val_loss: 0.5704\n",
            "Epoch 850/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5644 - val_loss: 0.5703\n",
            "Epoch 851/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5644 - val_loss: 0.5706\n",
            "Epoch 852/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5646 - val_loss: 0.5707\n",
            "Epoch 853/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5648 - val_loss: 0.5707\n",
            "Epoch 854/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5648 - val_loss: 0.5704\n",
            "Epoch 855/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5644 - val_loss: 0.5704\n",
            "Epoch 856/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5645 - val_loss: 0.5702\n",
            "Epoch 857/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5642 - val_loss: 0.5702\n",
            "Epoch 858/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5643 - val_loss: 0.5702\n",
            "Epoch 859/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5641 - val_loss: 0.5702\n",
            "Epoch 860/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5642 - val_loss: 0.5702\n",
            "Epoch 861/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5641 - val_loss: 0.5701\n",
            "Epoch 862/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5642 - val_loss: 0.5704\n",
            "Epoch 863/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5644 - val_loss: 0.5705\n",
            "Epoch 864/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5645 - val_loss: 0.5701\n",
            "Epoch 865/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5642 - val_loss: 0.5704\n",
            "Epoch 866/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5643 - val_loss: 0.5702\n",
            "Epoch 867/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5642 - val_loss: 0.5702\n",
            "Epoch 868/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5642 - val_loss: 0.5701\n",
            "Epoch 869/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5641 - val_loss: 0.5701\n",
            "Epoch 870/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5641 - val_loss: 0.5701\n",
            "Epoch 871/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5641 - val_loss: 0.5701\n",
            "Epoch 872/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5640 - val_loss: 0.5701\n",
            "Epoch 873/1000\n",
            "2/2 [==============================] - 2s 919ms/step - loss: 0.5641 - val_loss: 0.5703\n",
            "Epoch 874/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5642 - val_loss: 0.5705\n",
            "Epoch 875/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5645 - val_loss: 0.5707\n",
            "Epoch 876/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5645 - val_loss: 0.5701\n",
            "Epoch 877/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5640 - val_loss: 0.5703\n",
            "Epoch 878/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5644 - val_loss: 0.5702\n",
            "Epoch 879/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5641 - val_loss: 0.5702\n",
            "Epoch 880/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5642 - val_loss: 0.5704\n",
            "Epoch 881/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5642 - val_loss: 0.5701\n",
            "Epoch 882/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5641 - val_loss: 0.5703\n",
            "Epoch 883/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5642 - val_loss: 0.5701\n",
            "Epoch 884/1000\n",
            "2/2 [==============================] - 2s 967ms/step - loss: 0.5640 - val_loss: 0.5702\n",
            "Epoch 885/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5641 - val_loss: 0.5702\n",
            "Epoch 886/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5641 - val_loss: 0.5703\n",
            "Epoch 887/1000\n",
            "2/2 [==============================] - 2s 971ms/step - loss: 0.5642 - val_loss: 0.5702\n",
            "Epoch 888/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5641 - val_loss: 0.5701\n",
            "Epoch 889/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5640 - val_loss: 0.5701\n",
            "Epoch 890/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5639 - val_loss: 0.5701\n",
            "Epoch 891/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5641 - val_loss: 0.5700\n",
            "Epoch 892/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5639 - val_loss: 0.5700\n",
            "Epoch 893/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5640 - val_loss: 0.5702\n",
            "Epoch 894/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5642 - val_loss: 0.5705\n",
            "Epoch 895/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5645 - val_loss: 0.5704\n",
            "Epoch 896/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5643 - val_loss: 0.5705\n",
            "Epoch 897/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5644 - val_loss: 0.5702\n",
            "Epoch 898/1000\n",
            "2/2 [==============================] - 2s 970ms/step - loss: 0.5640 - val_loss: 0.5703\n",
            "Epoch 899/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5642 - val_loss: 0.5701\n",
            "Epoch 900/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5640 - val_loss: 0.5702\n",
            "Epoch 901/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5640 - val_loss: 0.5702\n",
            "Epoch 902/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5641 - val_loss: 0.5701\n",
            "Epoch 903/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5640 - val_loss: 0.5703\n",
            "Epoch 904/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5641 - val_loss: 0.5702\n",
            "Epoch 905/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5640 - val_loss: 0.5700\n",
            "Epoch 906/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5640 - val_loss: 0.5702\n",
            "Epoch 907/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5641 - val_loss: 0.5702\n",
            "Epoch 908/1000\n",
            "2/2 [==============================] - 2s 947ms/step - loss: 0.5640 - val_loss: 0.5699\n",
            "Epoch 909/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5638 - val_loss: 0.5699\n",
            "Epoch 910/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5638 - val_loss: 0.5700\n",
            "Epoch 911/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5639 - val_loss: 0.5700\n",
            "Epoch 912/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5639 - val_loss: 0.5700\n",
            "Epoch 913/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5638 - val_loss: 0.5701\n",
            "Epoch 914/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5640 - val_loss: 0.5703\n",
            "Epoch 915/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5641 - val_loss: 0.5700\n",
            "Epoch 916/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5638 - val_loss: 0.5700\n",
            "Epoch 917/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5639 - val_loss: 0.5701\n",
            "Epoch 918/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5639 - val_loss: 0.5699\n",
            "Epoch 919/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5638 - val_loss: 0.5701\n",
            "Epoch 920/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5639 - val_loss: 0.5699\n",
            "Epoch 921/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5637 - val_loss: 0.5701\n",
            "Epoch 922/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5639 - val_loss: 0.5700\n",
            "Epoch 923/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5638 - val_loss: 0.5702\n",
            "Epoch 924/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5641 - val_loss: 0.5702\n",
            "Epoch 925/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5640 - val_loss: 0.5702\n",
            "Epoch 926/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5640 - val_loss: 0.5702\n",
            "Epoch 927/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5641 - val_loss: 0.5705\n",
            "Epoch 928/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5643 - val_loss: 0.5699\n",
            "Epoch 929/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5637 - val_loss: 0.5702\n",
            "Epoch 930/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5640 - val_loss: 0.5700\n",
            "Epoch 931/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5639 - val_loss: 0.5701\n",
            "Epoch 932/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5640 - val_loss: 0.5703\n",
            "Epoch 933/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5641 - val_loss: 0.5700\n",
            "Epoch 934/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5638 - val_loss: 0.5701\n",
            "Epoch 935/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5639 - val_loss: 0.5701\n",
            "Epoch 936/1000\n",
            "2/2 [==============================] - 2s 952ms/step - loss: 0.5639 - val_loss: 0.5700\n",
            "Epoch 937/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5638 - val_loss: 0.5700\n",
            "Epoch 938/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5638 - val_loss: 0.5700\n",
            "Epoch 939/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5637 - val_loss: 0.5699\n",
            "Epoch 940/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5637 - val_loss: 0.5700\n",
            "Epoch 941/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5637 - val_loss: 0.5698\n",
            "Epoch 942/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5636 - val_loss: 0.5698\n",
            "Epoch 943/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5636 - val_loss: 0.5700\n",
            "Epoch 944/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5637 - val_loss: 0.5700\n",
            "Epoch 945/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5638 - val_loss: 0.5702\n",
            "Epoch 946/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5640 - val_loss: 0.5701\n",
            "Epoch 947/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5638 - val_loss: 0.5700\n",
            "Epoch 948/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5638 - val_loss: 0.5701\n",
            "Epoch 949/1000\n",
            "2/2 [==============================] - 2s 929ms/step - loss: 0.5638 - val_loss: 0.5700\n",
            "Epoch 950/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5637 - val_loss: 0.5701\n",
            "Epoch 951/1000\n",
            "2/2 [==============================] - 2s 969ms/step - loss: 0.5639 - val_loss: 0.5700\n",
            "Epoch 952/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5638 - val_loss: 0.5702\n",
            "Epoch 953/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5639 - val_loss: 0.5700\n",
            "Epoch 954/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5638 - val_loss: 0.5700\n",
            "Epoch 955/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5638 - val_loss: 0.5701\n",
            "Epoch 956/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5640 - val_loss: 0.5703\n",
            "Epoch 957/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5641 - val_loss: 0.5699\n",
            "Epoch 958/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5636 - val_loss: 0.5700\n",
            "Epoch 959/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5637 - val_loss: 0.5698\n",
            "Epoch 960/1000\n",
            "2/2 [==============================] - 2s 932ms/step - loss: 0.5636 - val_loss: 0.5698\n",
            "Epoch 961/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5635 - val_loss: 0.5698\n",
            "Epoch 962/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5635 - val_loss: 0.5697\n",
            "Epoch 963/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5635 - val_loss: 0.5699\n",
            "Epoch 964/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5637 - val_loss: 0.5700\n",
            "Epoch 965/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5637 - val_loss: 0.5699\n",
            "Epoch 966/1000\n",
            "2/2 [==============================] - 2s 953ms/step - loss: 0.5637 - val_loss: 0.5699\n",
            "Epoch 967/1000\n",
            "2/2 [==============================] - 2s 956ms/step - loss: 0.5637 - val_loss: 0.5699\n",
            "Epoch 968/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5636 - val_loss: 0.5700\n",
            "Epoch 969/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5636 - val_loss: 0.5699\n",
            "Epoch 970/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5636 - val_loss: 0.5697\n",
            "Epoch 971/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5634 - val_loss: 0.5698\n",
            "Epoch 972/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5635 - val_loss: 0.5697\n",
            "Epoch 973/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5635 - val_loss: 0.5698\n",
            "Epoch 974/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5635 - val_loss: 0.5698\n",
            "Epoch 975/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5636 - val_loss: 0.5700\n",
            "Epoch 976/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5638 - val_loss: 0.5703\n",
            "Epoch 977/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5640 - val_loss: 0.5699\n",
            "Epoch 978/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5635 - val_loss: 0.5699\n",
            "Epoch 979/1000\n",
            "2/2 [==============================] - 2s 971ms/step - loss: 0.5637 - val_loss: 0.5699\n",
            "Epoch 980/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5636 - val_loss: 0.5699\n",
            "Epoch 981/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5635 - val_loss: 0.5698\n",
            "Epoch 982/1000\n",
            "2/2 [==============================] - 2s 951ms/step - loss: 0.5635 - val_loss: 0.5699\n",
            "Epoch 983/1000\n",
            "2/2 [==============================] - 2s 965ms/step - loss: 0.5635 - val_loss: 0.5698\n",
            "Epoch 984/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5634 - val_loss: 0.5698\n",
            "Epoch 985/1000\n",
            "2/2 [==============================] - 2s 958ms/step - loss: 0.5635 - val_loss: 0.5699\n",
            "Epoch 986/1000\n",
            "2/2 [==============================] - 2s 955ms/step - loss: 0.5635 - val_loss: 0.5698\n",
            "Epoch 987/1000\n",
            "2/2 [==============================] - 2s 957ms/step - loss: 0.5635 - val_loss: 0.5698\n",
            "Epoch 988/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5634 - val_loss: 0.5697\n",
            "Epoch 989/1000\n",
            "2/2 [==============================] - 2s 962ms/step - loss: 0.5634 - val_loss: 0.5698\n",
            "Epoch 990/1000\n",
            "2/2 [==============================] - 2s 954ms/step - loss: 0.5634 - val_loss: 0.5698\n",
            "Epoch 991/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5635 - val_loss: 0.5701\n",
            "Epoch 992/1000\n",
            "2/2 [==============================] - 2s 960ms/step - loss: 0.5638 - val_loss: 0.5700\n",
            "Epoch 993/1000\n",
            "2/2 [==============================] - 2s 964ms/step - loss: 0.5637 - val_loss: 0.5700\n",
            "Epoch 994/1000\n",
            "2/2 [==============================] - 2s 966ms/step - loss: 0.5637 - val_loss: 0.5701\n",
            "Epoch 995/1000\n",
            "2/2 [==============================] - 2s 961ms/step - loss: 0.5638 - val_loss: 0.5699\n",
            "Epoch 996/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5636 - val_loss: 0.5700\n",
            "Epoch 997/1000\n",
            "2/2 [==============================] - 2s 963ms/step - loss: 0.5637 - val_loss: 0.5700\n",
            "Epoch 998/1000\n",
            "2/2 [==============================] - 2s 959ms/step - loss: 0.5636 - val_loss: 0.5697\n",
            "Epoch 999/1000\n",
            "2/2 [==============================] - 2s 927ms/step - loss: 0.5634 - val_loss: 0.5699\n",
            "Epoch 1000/1000\n",
            "2/2 [==============================] - 2s 919ms/step - loss: 0.5635 - val_loss: 0.5697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f350d034518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjD0cWowA2hO"
      },
      "source": [
        "#encoder = Model(input_layer, encoded3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K56Bo6CXA2pj"
      },
      "source": [
        "#dat_resaXauto = encoder.predict(dat_resaX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "h4oyQZ1iWQiv",
        "outputId": "705ecd17-8ef7-473e-d88f-3d80c1a5bcf9"
      },
      "source": [
        "#pd.DataFrame(dat_resaXauto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.41</td>\n",
              "      <td>54.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.80</td>\n",
              "      <td>8.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.93</td>\n",
              "      <td>3.59</td>\n",
              "      <td>74.70</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.96</td>\n",
              "      <td>15.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.16</td>\n",
              "      <td>11.27</td>\n",
              "      <td>42.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.67</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.25</td>\n",
              "      <td>47.48</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.31</td>\n",
              "      <td>6.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.52</td>\n",
              "      <td>19.00</td>\n",
              "      <td>32.74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.97</td>\n",
              "      <td>48.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.20</td>\n",
              "      <td>11.17</td>\n",
              "      <td>26.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.31</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.33</td>\n",
              "      <td>48.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.76</td>\n",
              "      <td>14.76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.93</td>\n",
              "      <td>21.92</td>\n",
              "      <td>22.48</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.21</td>\n",
              "      <td>32.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.29</td>\n",
              "      <td>26.06</td>\n",
              "      <td>32.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.20</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.66</td>\n",
              "      <td>39.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49.01</td>\n",
              "      <td>22.92</td>\n",
              "      <td>28.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.91</td>\n",
              "      <td>59.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.62</td>\n",
              "      <td>18.49</td>\n",
              "      <td>18.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.84</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.15</td>\n",
              "      <td>50.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.46</td>\n",
              "      <td>4.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47.06</td>\n",
              "      <td>16.92</td>\n",
              "      <td>18.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.04</td>\n",
              "      <td>49.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.30</td>\n",
              "      <td>25.30</td>\n",
              "      <td>24.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.22</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5839</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.69</td>\n",
              "      <td>52.74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.55</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.09</td>\n",
              "      <td>9.41</td>\n",
              "      <td>29.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.42</td>\n",
              "      <td>5.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.60</td>\n",
              "      <td>28.13</td>\n",
              "      <td>25.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.24</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5840</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.28</td>\n",
              "      <td>37.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.90</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.95</td>\n",
              "      <td>21.56</td>\n",
              "      <td>47.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.05</td>\n",
              "      <td>7.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.10</td>\n",
              "      <td>25.16</td>\n",
              "      <td>32.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5841</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>47.47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.61</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.78</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.67</td>\n",
              "      <td>7.49</td>\n",
              "      <td>30.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.37</td>\n",
              "      <td>19.37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.70</td>\n",
              "      <td>34.64</td>\n",
              "      <td>30.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.57</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.13</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5842</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.36</td>\n",
              "      <td>32.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.77</td>\n",
              "      <td>13.71</td>\n",
              "      <td>53.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>82.16</td>\n",
              "      <td>3.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.50</td>\n",
              "      <td>13.56</td>\n",
              "      <td>12.53</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.54</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5843</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.67</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>28.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.77</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.66</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.97</td>\n",
              "      <td>0.68</td>\n",
              "      <td>77.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.91</td>\n",
              "      <td>30.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.97</td>\n",
              "      <td>2.24</td>\n",
              "      <td>57.53</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.39</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.80</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5844 rows × 60 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1      2    3    4      5   ...   54   55   56   57     58   59\n",
              "0     0.0  0.0   0.00  0.0  0.0  17.41  ...  0.0  0.0  0.0  0.0  10.67  0.0\n",
              "1     0.0  0.0  10.96  0.0  0.0  11.25  ...  0.0  0.0  0.0  0.0  16.31  0.0\n",
              "2     0.0  0.0   8.27  0.0  0.0  10.33  ...  0.0  0.0  0.0  0.0   7.20  0.0\n",
              "3     0.0  0.0  11.54  0.0  0.0   6.66  ...  0.0  0.0  0.0  0.0  11.84  0.0\n",
              "4     0.0  0.0   9.27  0.0  0.0   8.15  ...  0.0  0.0  0.0  0.0  12.22  0.0\n",
              "...   ...  ...    ...  ...  ...    ...  ...  ...  ...  ...  ...    ...  ...\n",
              "5839  0.0  0.0  19.81  0.0  0.0   8.69  ...  0.0  0.0  0.0  0.0  14.24  0.0\n",
              "5840  0.0  0.0  12.89  0.0  0.0  12.28  ...  0.0  0.0  0.0  0.0   3.28  0.0\n",
              "5841  0.0  0.0  21.99  0.0  0.0   0.00  ...  0.0  0.0  0.0  0.0  25.13  0.0\n",
              "5842  0.0  0.0  10.44  0.0  0.0  13.36  ...  0.0  0.0  0.0  0.0  15.54  0.0\n",
              "5843  0.0  0.0  38.67  0.0  0.0   0.00  ...  0.0  0.0  0.0  0.0  55.80  0.0\n",
              "\n",
              "[5844 rows x 60 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE-OZQxdZM9t",
        "outputId": "b0125263-7b42-4be0-cdcb-e01e28f96cf2"
      },
      "source": [
        "#dbscana3 = DBSCAN(eps=36, min_samples=130)\n",
        "#dbscana3.fit_predict(dat_resaXauto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  0, -1, ...,  4,  4, -1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tw2FdVzZ-Jf"
      },
      "source": [
        "#lab3=list(dbscana3.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teIUZ_OvZ-Ji"
      },
      "source": [
        "#lab3_=list(filter(lambda x:x>-1,lab3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpiP_8tCZ-Jn",
        "outputId": "d293a573-8ada-45f5-dc9b-43f092608724"
      },
      "source": [
        "#len(lab3_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4653"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKvJT_xyZ-Jr",
        "outputId": "915c37c5-6692-4519-aa61-71498ec1fb06"
      },
      "source": [
        "#len(set(lab3_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r29Bw4_uZ-Ju",
        "outputId": "d1a47336-02fc-4984-89e4-01f5b2b5130d"
      },
      "source": [
        "#lab3.count(-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1191"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ui_rkJzZ-Jw"
      },
      "source": [
        "#dat_resa.drop(['dbscan'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "td9MOtuFZ-Jy",
        "outputId": "03206eff-bbb9-499a-8a7c-dba601176a9a"
      },
      "source": [
        "#dat_resaXauto_=pd.DataFrame(dat_resaXauto)\n",
        "#dat_resaXauto_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.41</td>\n",
              "      <td>54.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.80</td>\n",
              "      <td>8.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.93</td>\n",
              "      <td>3.59</td>\n",
              "      <td>74.70</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.96</td>\n",
              "      <td>15.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.16</td>\n",
              "      <td>11.27</td>\n",
              "      <td>42.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.67</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.25</td>\n",
              "      <td>47.48</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.31</td>\n",
              "      <td>6.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.52</td>\n",
              "      <td>19.00</td>\n",
              "      <td>32.74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.97</td>\n",
              "      <td>48.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.20</td>\n",
              "      <td>11.17</td>\n",
              "      <td>26.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.31</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.33</td>\n",
              "      <td>48.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.76</td>\n",
              "      <td>14.76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.93</td>\n",
              "      <td>21.92</td>\n",
              "      <td>22.48</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.21</td>\n",
              "      <td>32.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.29</td>\n",
              "      <td>26.06</td>\n",
              "      <td>32.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.20</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.66</td>\n",
              "      <td>39.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49.01</td>\n",
              "      <td>22.92</td>\n",
              "      <td>28.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.91</td>\n",
              "      <td>59.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.62</td>\n",
              "      <td>18.49</td>\n",
              "      <td>18.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.84</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.15</td>\n",
              "      <td>50.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.46</td>\n",
              "      <td>4.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47.06</td>\n",
              "      <td>16.92</td>\n",
              "      <td>18.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.04</td>\n",
              "      <td>49.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.30</td>\n",
              "      <td>25.30</td>\n",
              "      <td>24.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.22</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5839</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.69</td>\n",
              "      <td>52.74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.55</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.09</td>\n",
              "      <td>9.41</td>\n",
              "      <td>29.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.42</td>\n",
              "      <td>5.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.60</td>\n",
              "      <td>28.13</td>\n",
              "      <td>25.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.24</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5840</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.28</td>\n",
              "      <td>37.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.90</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.95</td>\n",
              "      <td>21.56</td>\n",
              "      <td>47.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.05</td>\n",
              "      <td>7.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.10</td>\n",
              "      <td>25.16</td>\n",
              "      <td>32.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.28</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5841</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>47.47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.61</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.78</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.67</td>\n",
              "      <td>7.49</td>\n",
              "      <td>30.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.37</td>\n",
              "      <td>19.37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.70</td>\n",
              "      <td>34.64</td>\n",
              "      <td>30.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.57</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.13</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5842</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.36</td>\n",
              "      <td>32.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.77</td>\n",
              "      <td>13.71</td>\n",
              "      <td>53.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>82.16</td>\n",
              "      <td>3.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.50</td>\n",
              "      <td>13.56</td>\n",
              "      <td>12.53</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.54</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5843</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.67</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>28.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.77</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.66</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.97</td>\n",
              "      <td>0.68</td>\n",
              "      <td>77.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.91</td>\n",
              "      <td>30.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.97</td>\n",
              "      <td>2.24</td>\n",
              "      <td>57.53</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.39</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.80</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5844 rows × 60 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1      2    3    4      5   ...   54   55   56   57     58   59\n",
              "0     0.0  0.0   0.00  0.0  0.0  17.41  ...  0.0  0.0  0.0  0.0  10.67  0.0\n",
              "1     0.0  0.0  10.96  0.0  0.0  11.25  ...  0.0  0.0  0.0  0.0  16.31  0.0\n",
              "2     0.0  0.0   8.27  0.0  0.0  10.33  ...  0.0  0.0  0.0  0.0   7.20  0.0\n",
              "3     0.0  0.0  11.54  0.0  0.0   6.66  ...  0.0  0.0  0.0  0.0  11.84  0.0\n",
              "4     0.0  0.0   9.27  0.0  0.0   8.15  ...  0.0  0.0  0.0  0.0  12.22  0.0\n",
              "...   ...  ...    ...  ...  ...    ...  ...  ...  ...  ...  ...    ...  ...\n",
              "5839  0.0  0.0  19.81  0.0  0.0   8.69  ...  0.0  0.0  0.0  0.0  14.24  0.0\n",
              "5840  0.0  0.0  12.89  0.0  0.0  12.28  ...  0.0  0.0  0.0  0.0   3.28  0.0\n",
              "5841  0.0  0.0  21.99  0.0  0.0   0.00  ...  0.0  0.0  0.0  0.0  25.13  0.0\n",
              "5842  0.0  0.0  10.44  0.0  0.0  13.36  ...  0.0  0.0  0.0  0.0  15.54  0.0\n",
              "5843  0.0  0.0  38.67  0.0  0.0   0.00  ...  0.0  0.0  0.0  0.0  55.80  0.0\n",
              "\n",
              "[5844 rows x 60 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJvRInTLZ-J1"
      },
      "source": [
        "#dat_resaXauto_['Disease']=pd.DataFrame(dat_resaY)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3JEggyOZ-J5"
      },
      "source": [
        "#dat_resaXauto_['dbscan']=pd.DataFrame(lab3)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UpcXo4PZ-J7",
        "outputId": "b8280e03-7bea-40a1-a0be-14fa270c1da3"
      },
      "source": [
        "#dat_resaXauto_[dat_resaXauto_['dbscan']==0]['Disease'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "diffuse large B-cell lymphoma    963\n",
              "acute myeloid leukaemia          503\n",
              "breast cancer                      4\n",
              "Name: Disease, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyty5Hd0HF1T"
      },
      "source": [
        "#### train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DDlEl0ZW4dQ"
      },
      "source": [
        "if cnt==1: \n",
        " dat_resaXtr, dat_resaXts, dat_resaYtr, dat_resaYts = train_test_split(dat_resa.drop(['Disease'],axis=1),dat_resa['Disease'], \n",
        "                                                    train_size=0.75, \n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=111,\n",
        "                                                    shuffle=True, \n",
        "                                                    stratify=dat_resa['Disease'])\n",
        " scaler=MinMaxScaler()\n",
        " dat_resaXtr=scaler.fit(dat_resaXtr).transform(dat_resaXtr)\n",
        " dat_resaXts=scaler.fit(dat_resaXts).transform(dat_resaXts)\n",
        " over_sampler = SMOTE(k_neighbors=10, sampling_strategy=\"auto\")\n",
        " dat_resaXtr, dat_resaYtr = over_sampler.fit_resample(dat_resaXtr,dat_resaYtr)\n",
        " dat_resaXts, dat_resaYts = over_sampler.fit_resample(dat_resaXts,dat_resaYts)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hibQiD9SzX4h"
      },
      "source": [
        "##### selectkbest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YJGd5y5fIQY",
        "outputId": "4f688d22-f8e9-4286-ac8b-18d87d26be5b"
      },
      "source": [
        "if cnt==1: \n",
        " selector = SelectKBest(score_func=f_classif, k=60)\n",
        " dat_resaXredtr=selector.fit(dat_resaXtr,dat_resaYtr).transform(dat_resaXtr)\n",
        " dat_resaXredts=selector.fit(dat_resaXtr,dat_resaYtr).transform(dat_resaXts)\n",
        " dat_resaXredts\n",
        " selector.pvalues_\n",
        " selector.scores_"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.05865022, 0.16592724, 0.35230838, ..., 0.48667567, 0.06476949,\n",
              "        0.1249239 ],\n",
              "       [0.06380628, 0.11889973, 0.17940385, ..., 0.55225347, 0.06545248,\n",
              "        0.09777183],\n",
              "       [0.37280177, 0.13339249, 0.16539668, ..., 0.78988088, 0.10893569,\n",
              "        0.0765859 ],\n",
              "       ...,\n",
              "       [0.32428983, 0.19559854, 0.29415287, ..., 0.49540665, 0.05864796,\n",
              "        0.10863451],\n",
              "       [0.04020712, 0.0957912 , 0.22944503, ..., 0.5175359 , 0.07205104,\n",
              "        0.08947111],\n",
              "       [0.02121163, 0.1655427 , 0.35861708, ..., 0.4111663 , 0.12194177,\n",
              "        0.08851615]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6LgsZ7Kzlci"
      },
      "source": [
        "##### autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwT2F7TBzKJ6"
      },
      "source": [
        "if cnt==1: \n",
        " input_dim = 53999\n",
        " latent_dim1 =5000\n",
        " latent_dim2=500\n",
        " latent_dim3=60\n",
        " input_layer = Input(shape=(input_dim,), name='input')\n",
        "\n",
        " encoded1 = Dense(latent_dim1, \n",
        "                activation='relu', name='features1')(input_layer)\n",
        " encoded2 = Dense(latent_dim2, \n",
        "                activation='relu', name='features2')(encoded1)\n",
        " encoded3 = Dense(latent_dim3, \n",
        "                activation='relu', name='features3')(encoded2)\n",
        "\n",
        " decoded1 = Dense(latent_dim2, activation='relu', name='reconstructed1')(encoded3)\n",
        " decoded2 = Dense(latent_dim1, activation='relu', name='reconstructed2')(decoded1)\n",
        " decoded3 = Dense(input_dim, activation='sigmoid', name='reconstructed3')(decoded2)\n",
        "\n",
        " autoencoder = Model(inputs=[input_layer], outputs=[decoded3])\n",
        "\n",
        " autoencoder.compile(optimizer='adam', \n",
        "                    loss='binary_crossentropy')\n",
        " autoencoder.fit(dat_resaXtr,dat_resaXtr ,\n",
        "                epochs=1000,\n",
        "                batch_size=3000,\n",
        "                shuffle=True,\n",
        "                validation_split=0.2)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKXLYiRpzKKA",
        "outputId": "32567fe8-aff4-46d3-c571-0aba36f6cda3"
      },
      "source": [
        "#autoencoder.fit(dat_resaXtr,dat_resaXtr ,\n",
        "#                epochs=1000,\n",
        "#                batch_size=3000,\n",
        "#                shuffle=True,\n",
        "#                validation_split=0.2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "2/2 [==============================] - 1s 504ms/step - loss: 0.6929 - val_loss: 0.6526\n",
            "Epoch 2/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.6547 - val_loss: 0.7230\n",
            "Epoch 3/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.7113 - val_loss: 0.6273\n",
            "Epoch 4/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.6280 - val_loss: 0.6084\n",
            "Epoch 5/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.6090 - val_loss: 0.6103\n",
            "Epoch 6/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.6101 - val_loss: 0.6104\n",
            "Epoch 7/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.6099 - val_loss: 0.6079\n",
            "Epoch 8/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.6077 - val_loss: 0.6064\n",
            "Epoch 9/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.6066 - val_loss: 0.6064\n",
            "Epoch 10/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.6062 - val_loss: 0.6066\n",
            "Epoch 11/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.6064 - val_loss: 0.6059\n",
            "Epoch 12/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.6056 - val_loss: 0.6061\n",
            "Epoch 13/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.6057 - val_loss: 0.6060\n",
            "Epoch 14/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.6056 - val_loss: 0.6060\n",
            "Epoch 15/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.6055 - val_loss: 0.6059\n",
            "Epoch 16/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.6054 - val_loss: 0.6059\n",
            "Epoch 17/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.6054 - val_loss: 0.6059\n",
            "Epoch 18/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.6054 - val_loss: 0.6059\n",
            "Epoch 19/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.6054 - val_loss: 0.6058\n",
            "Epoch 20/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.6053 - val_loss: 0.6056\n",
            "Epoch 21/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.6052 - val_loss: 0.6056\n",
            "Epoch 22/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.6052 - val_loss: 0.6056\n",
            "Epoch 23/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.6052 - val_loss: 0.6055\n",
            "Epoch 24/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.6051 - val_loss: 0.6054\n",
            "Epoch 25/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.6049 - val_loss: 0.6054\n",
            "Epoch 26/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.6049 - val_loss: 0.6053\n",
            "Epoch 27/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.6047 - val_loss: 0.6048\n",
            "Epoch 28/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.6040 - val_loss: 0.6043\n",
            "Epoch 29/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.6031 - val_loss: 0.6039\n",
            "Epoch 30/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.6018 - val_loss: 0.6026\n",
            "Epoch 31/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5994 - val_loss: 0.6017\n",
            "Epoch 32/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5978 - val_loss: 0.6025\n",
            "Epoch 33/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5983 - val_loss: 0.6027\n",
            "Epoch 34/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5981 - val_loss: 0.6020\n",
            "Epoch 35/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5971 - val_loss: 0.6016\n",
            "Epoch 36/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5973 - val_loss: 0.6011\n",
            "Epoch 37/1000\n",
            "2/2 [==============================] - 1s 652ms/step - loss: 0.5971 - val_loss: 0.6010\n",
            "Epoch 38/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5969 - val_loss: 0.6007\n",
            "Epoch 39/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5966 - val_loss: 0.6002\n",
            "Epoch 40/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5962 - val_loss: 0.6001\n",
            "Epoch 41/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5960 - val_loss: 0.5995\n",
            "Epoch 42/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5956 - val_loss: 0.5987\n",
            "Epoch 43/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5947 - val_loss: 0.5991\n",
            "Epoch 44/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5949 - val_loss: 0.5994\n",
            "Epoch 45/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5950 - val_loss: 0.5975\n",
            "Epoch 46/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5926 - val_loss: 0.5970\n",
            "Epoch 47/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5918 - val_loss: 0.5972\n",
            "Epoch 48/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5913 - val_loss: 0.5969\n",
            "Epoch 49/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5909 - val_loss: 0.5967\n",
            "Epoch 50/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5910 - val_loss: 0.5969\n",
            "Epoch 51/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5909 - val_loss: 0.5960\n",
            "Epoch 52/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5898 - val_loss: 0.5953\n",
            "Epoch 53/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5895 - val_loss: 0.5951\n",
            "Epoch 54/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5892 - val_loss: 0.5946\n",
            "Epoch 55/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5887 - val_loss: 0.5937\n",
            "Epoch 56/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5880 - val_loss: 0.5933\n",
            "Epoch 57/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5877 - val_loss: 0.5926\n",
            "Epoch 58/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5870 - val_loss: 0.5921\n",
            "Epoch 59/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5868 - val_loss: 0.5917\n",
            "Epoch 60/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5868 - val_loss: 0.5908\n",
            "Epoch 61/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5860 - val_loss: 0.5906\n",
            "Epoch 62/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5856 - val_loss: 0.5901\n",
            "Epoch 63/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5853 - val_loss: 0.5897\n",
            "Epoch 64/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5851 - val_loss: 0.5893\n",
            "Epoch 65/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5846 - val_loss: 0.5892\n",
            "Epoch 66/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5845 - val_loss: 0.5887\n",
            "Epoch 67/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5842 - val_loss: 0.5885\n",
            "Epoch 68/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5839 - val_loss: 0.5884\n",
            "Epoch 69/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5837 - val_loss: 0.5879\n",
            "Epoch 70/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5834 - val_loss: 0.5874\n",
            "Epoch 71/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5832 - val_loss: 0.5874\n",
            "Epoch 72/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5833 - val_loss: 0.5874\n",
            "Epoch 73/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5835 - val_loss: 0.5877\n",
            "Epoch 74/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5838 - val_loss: 0.5876\n",
            "Epoch 75/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5840 - val_loss: 0.5862\n",
            "Epoch 76/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5826 - val_loss: 0.5857\n",
            "Epoch 77/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5824 - val_loss: 0.5853\n",
            "Epoch 78/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5820 - val_loss: 0.5851\n",
            "Epoch 79/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5817 - val_loss: 0.5848\n",
            "Epoch 80/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5817 - val_loss: 0.5847\n",
            "Epoch 81/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5813 - val_loss: 0.5840\n",
            "Epoch 82/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5809 - val_loss: 0.5837\n",
            "Epoch 83/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5806 - val_loss: 0.5837\n",
            "Epoch 84/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5806 - val_loss: 0.5837\n",
            "Epoch 85/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5805 - val_loss: 0.5833\n",
            "Epoch 86/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5803 - val_loss: 0.5832\n",
            "Epoch 87/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5802 - val_loss: 0.5829\n",
            "Epoch 88/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5798 - val_loss: 0.5827\n",
            "Epoch 89/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5796 - val_loss: 0.5825\n",
            "Epoch 90/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5794 - val_loss: 0.5823\n",
            "Epoch 91/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5793 - val_loss: 0.5822\n",
            "Epoch 92/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5791 - val_loss: 0.5824\n",
            "Epoch 93/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5793 - val_loss: 0.5827\n",
            "Epoch 94/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5800 - val_loss: 0.5831\n",
            "Epoch 95/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5799 - val_loss: 0.5820\n",
            "Epoch 96/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5791 - val_loss: 0.5823\n",
            "Epoch 97/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5792 - val_loss: 0.5818\n",
            "Epoch 98/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5788 - val_loss: 0.5816\n",
            "Epoch 99/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5786 - val_loss: 0.5814\n",
            "Epoch 100/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5783 - val_loss: 0.5812\n",
            "Epoch 101/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5781 - val_loss: 0.5811\n",
            "Epoch 102/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5780 - val_loss: 0.5810\n",
            "Epoch 103/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5779 - val_loss: 0.5809\n",
            "Epoch 104/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5778 - val_loss: 0.5807\n",
            "Epoch 105/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5776 - val_loss: 0.5806\n",
            "Epoch 106/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5776 - val_loss: 0.5806\n",
            "Epoch 107/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5776 - val_loss: 0.5806\n",
            "Epoch 108/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5775 - val_loss: 0.5804\n",
            "Epoch 109/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5773 - val_loss: 0.5803\n",
            "Epoch 110/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5771 - val_loss: 0.5802\n",
            "Epoch 111/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5771 - val_loss: 0.5800\n",
            "Epoch 112/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5770 - val_loss: 0.5803\n",
            "Epoch 113/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5773 - val_loss: 0.5805\n",
            "Epoch 114/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5774 - val_loss: 0.5803\n",
            "Epoch 115/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5774 - val_loss: 0.5809\n",
            "Epoch 116/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5778 - val_loss: 0.5803\n",
            "Epoch 117/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5772 - val_loss: 0.5799\n",
            "Epoch 118/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5768 - val_loss: 0.5800\n",
            "Epoch 119/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5767 - val_loss: 0.5796\n",
            "Epoch 120/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5764 - val_loss: 0.5797\n",
            "Epoch 121/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5764 - val_loss: 0.5794\n",
            "Epoch 122/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5761 - val_loss: 0.5794\n",
            "Epoch 123/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5760 - val_loss: 0.5792\n",
            "Epoch 124/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5759 - val_loss: 0.5790\n",
            "Epoch 125/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5757 - val_loss: 0.5790\n",
            "Epoch 126/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5757 - val_loss: 0.5790\n",
            "Epoch 127/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5756 - val_loss: 0.5789\n",
            "Epoch 128/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5756 - val_loss: 0.5793\n",
            "Epoch 129/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5761 - val_loss: 0.5794\n",
            "Epoch 130/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5761 - val_loss: 0.5793\n",
            "Epoch 131/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5759 - val_loss: 0.5791\n",
            "Epoch 132/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5757 - val_loss: 0.5792\n",
            "Epoch 133/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5758 - val_loss: 0.5787\n",
            "Epoch 134/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5753 - val_loss: 0.5787\n",
            "Epoch 135/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5754 - val_loss: 0.5785\n",
            "Epoch 136/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5751 - val_loss: 0.5785\n",
            "Epoch 137/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5751 - val_loss: 0.5784\n",
            "Epoch 138/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5750 - val_loss: 0.5785\n",
            "Epoch 139/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5751 - val_loss: 0.5783\n",
            "Epoch 140/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5748 - val_loss: 0.5782\n",
            "Epoch 141/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5747 - val_loss: 0.5782\n",
            "Epoch 142/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5746 - val_loss: 0.5781\n",
            "Epoch 143/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5746 - val_loss: 0.5781\n",
            "Epoch 144/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5746 - val_loss: 0.5783\n",
            "Epoch 145/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5748 - val_loss: 0.5784\n",
            "Epoch 146/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5751 - val_loss: 0.5785\n",
            "Epoch 147/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5752 - val_loss: 0.5788\n",
            "Epoch 148/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5753 - val_loss: 0.5787\n",
            "Epoch 149/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5751 - val_loss: 0.5783\n",
            "Epoch 150/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5748 - val_loss: 0.5778\n",
            "Epoch 151/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5743 - val_loss: 0.5781\n",
            "Epoch 152/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5745 - val_loss: 0.5781\n",
            "Epoch 153/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5745 - val_loss: 0.5779\n",
            "Epoch 154/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5743 - val_loss: 0.5780\n",
            "Epoch 155/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5743 - val_loss: 0.5777\n",
            "Epoch 156/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5741 - val_loss: 0.5776\n",
            "Epoch 157/1000\n",
            "2/2 [==============================] - 1s 653ms/step - loss: 0.5739 - val_loss: 0.5776\n",
            "Epoch 158/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5740 - val_loss: 0.5777\n",
            "Epoch 159/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5740 - val_loss: 0.5779\n",
            "Epoch 160/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5742 - val_loss: 0.5785\n",
            "Epoch 161/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5748 - val_loss: 0.5776\n",
            "Epoch 162/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5740 - val_loss: 0.5781\n",
            "Epoch 163/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5742 - val_loss: 0.5777\n",
            "Epoch 164/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5741 - val_loss: 0.5776\n",
            "Epoch 165/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5739 - val_loss: 0.5774\n",
            "Epoch 166/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5737 - val_loss: 0.5774\n",
            "Epoch 167/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5736 - val_loss: 0.5773\n",
            "Epoch 168/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5735 - val_loss: 0.5773\n",
            "Epoch 169/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5735 - val_loss: 0.5772\n",
            "Epoch 170/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5734 - val_loss: 0.5773\n",
            "Epoch 171/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5735 - val_loss: 0.5775\n",
            "Epoch 172/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5738 - val_loss: 0.5778\n",
            "Epoch 173/1000\n",
            "2/2 [==============================] - 1s 653ms/step - loss: 0.5741 - val_loss: 0.5773\n",
            "Epoch 174/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5734 - val_loss: 0.5776\n",
            "Epoch 175/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5736 - val_loss: 0.5772\n",
            "Epoch 176/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5734 - val_loss: 0.5771\n",
            "Epoch 177/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5733 - val_loss: 0.5773\n",
            "Epoch 178/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5734 - val_loss: 0.5771\n",
            "Epoch 179/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5732 - val_loss: 0.5771\n",
            "Epoch 180/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5732 - val_loss: 0.5770\n",
            "Epoch 181/1000\n",
            "2/2 [==============================] - 1s 671ms/step - loss: 0.5731 - val_loss: 0.5769\n",
            "Epoch 182/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5730 - val_loss: 0.5771\n",
            "Epoch 183/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5730 - val_loss: 0.5769\n",
            "Epoch 184/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5730 - val_loss: 0.5770\n",
            "Epoch 185/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5731 - val_loss: 0.5771\n",
            "Epoch 186/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5732 - val_loss: 0.5772\n",
            "Epoch 187/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5734 - val_loss: 0.5770\n",
            "Epoch 188/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5731 - val_loss: 0.5770\n",
            "Epoch 189/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5730 - val_loss: 0.5771\n",
            "Epoch 190/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5731 - val_loss: 0.5771\n",
            "Epoch 191/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5731 - val_loss: 0.5770\n",
            "Epoch 192/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5729 - val_loss: 0.5768\n",
            "Epoch 193/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5728 - val_loss: 0.5767\n",
            "Epoch 194/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5727 - val_loss: 0.5767\n",
            "Epoch 195/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5727 - val_loss: 0.5766\n",
            "Epoch 196/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5725 - val_loss: 0.5766\n",
            "Epoch 197/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5726 - val_loss: 0.5766\n",
            "Epoch 198/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5725 - val_loss: 0.5765\n",
            "Epoch 199/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5725 - val_loss: 0.5765\n",
            "Epoch 200/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5724 - val_loss: 0.5765\n",
            "Epoch 201/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5724 - val_loss: 0.5765\n",
            "Epoch 202/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5724 - val_loss: 0.5766\n",
            "Epoch 203/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5726 - val_loss: 0.5769\n",
            "Epoch 204/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5728 - val_loss: 0.5769\n",
            "Epoch 205/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5729 - val_loss: 0.5767\n",
            "Epoch 206/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5726 - val_loss: 0.5770\n",
            "Epoch 207/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5728 - val_loss: 0.5768\n",
            "Epoch 208/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5727 - val_loss: 0.5767\n",
            "Epoch 209/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5726 - val_loss: 0.5766\n",
            "Epoch 210/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5725 - val_loss: 0.5766\n",
            "Epoch 211/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5723 - val_loss: 0.5765\n",
            "Epoch 212/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5723 - val_loss: 0.5764\n",
            "Epoch 213/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5722 - val_loss: 0.5762\n",
            "Epoch 214/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5721 - val_loss: 0.5763\n",
            "Epoch 215/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5721 - val_loss: 0.5763\n",
            "Epoch 216/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5721 - val_loss: 0.5762\n",
            "Epoch 217/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5720 - val_loss: 0.5763\n",
            "Epoch 218/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5721 - val_loss: 0.5762\n",
            "Epoch 219/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5719 - val_loss: 0.5761\n",
            "Epoch 220/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5718 - val_loss: 0.5762\n",
            "Epoch 221/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5719 - val_loss: 0.5761\n",
            "Epoch 222/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5719 - val_loss: 0.5763\n",
            "Epoch 223/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5721 - val_loss: 0.5765\n",
            "Epoch 224/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5723 - val_loss: 0.5761\n",
            "Epoch 225/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5719 - val_loss: 0.5763\n",
            "Epoch 226/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5720 - val_loss: 0.5760\n",
            "Epoch 227/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5717 - val_loss: 0.5762\n",
            "Epoch 228/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5719 - val_loss: 0.5761\n",
            "Epoch 229/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5718 - val_loss: 0.5760\n",
            "Epoch 230/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5717 - val_loss: 0.5760\n",
            "Epoch 231/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5716 - val_loss: 0.5759\n",
            "Epoch 232/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5716 - val_loss: 0.5759\n",
            "Epoch 233/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5716 - val_loss: 0.5762\n",
            "Epoch 234/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5719 - val_loss: 0.5769\n",
            "Epoch 235/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5724 - val_loss: 0.5764\n",
            "Epoch 236/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5720 - val_loss: 0.5763\n",
            "Epoch 237/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5720 - val_loss: 0.5760\n",
            "Epoch 238/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5717 - val_loss: 0.5760\n",
            "Epoch 239/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5717 - val_loss: 0.5760\n",
            "Epoch 240/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5716 - val_loss: 0.5758\n",
            "Epoch 241/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5714 - val_loss: 0.5759\n",
            "Epoch 242/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5715 - val_loss: 0.5759\n",
            "Epoch 243/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5715 - val_loss: 0.5759\n",
            "Epoch 244/1000\n",
            "2/2 [==============================] - 1s 648ms/step - loss: 0.5714 - val_loss: 0.5759\n",
            "Epoch 245/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5715 - val_loss: 0.5759\n",
            "Epoch 246/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5715 - val_loss: 0.5759\n",
            "Epoch 247/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5713 - val_loss: 0.5758\n",
            "Epoch 248/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5713 - val_loss: 0.5756\n",
            "Epoch 249/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5712 - val_loss: 0.5759\n",
            "Epoch 250/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5715 - val_loss: 0.5759\n",
            "Epoch 251/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5715 - val_loss: 0.5760\n",
            "Epoch 252/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5716 - val_loss: 0.5760\n",
            "Epoch 253/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5715 - val_loss: 0.5759\n",
            "Epoch 254/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5715 - val_loss: 0.5759\n",
            "Epoch 255/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5713 - val_loss: 0.5759\n",
            "Epoch 256/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5713 - val_loss: 0.5755\n",
            "Epoch 257/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5710 - val_loss: 0.5756\n",
            "Epoch 258/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5710 - val_loss: 0.5756\n",
            "Epoch 259/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5710 - val_loss: 0.5756\n",
            "Epoch 260/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5710 - val_loss: 0.5755\n",
            "Epoch 261/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5709 - val_loss: 0.5755\n",
            "Epoch 262/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5709 - val_loss: 0.5757\n",
            "Epoch 263/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5712 - val_loss: 0.5758\n",
            "Epoch 264/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5713 - val_loss: 0.5758\n",
            "Epoch 265/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5712 - val_loss: 0.5758\n",
            "Epoch 266/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5712 - val_loss: 0.5759\n",
            "Epoch 267/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5712 - val_loss: 0.5757\n",
            "Epoch 268/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5710 - val_loss: 0.5755\n",
            "Epoch 269/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5709 - val_loss: 0.5754\n",
            "Epoch 270/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5708 - val_loss: 0.5755\n",
            "Epoch 271/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5708 - val_loss: 0.5754\n",
            "Epoch 272/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5708 - val_loss: 0.5754\n",
            "Epoch 273/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5707 - val_loss: 0.5753\n",
            "Epoch 274/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5707 - val_loss: 0.5753\n",
            "Epoch 275/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5706 - val_loss: 0.5753\n",
            "Epoch 276/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5706 - val_loss: 0.5754\n",
            "Epoch 277/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5706 - val_loss: 0.5755\n",
            "Epoch 278/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5707 - val_loss: 0.5754\n",
            "Epoch 279/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5706 - val_loss: 0.5755\n",
            "Epoch 280/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5708 - val_loss: 0.5756\n",
            "Epoch 281/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5708 - val_loss: 0.5756\n",
            "Epoch 282/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5709 - val_loss: 0.5757\n",
            "Epoch 283/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5709 - val_loss: 0.5757\n",
            "Epoch 284/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5710 - val_loss: 0.5754\n",
            "Epoch 285/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5707 - val_loss: 0.5757\n",
            "Epoch 286/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5710 - val_loss: 0.5756\n",
            "Epoch 287/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5709 - val_loss: 0.5752\n",
            "Epoch 288/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5705 - val_loss: 0.5753\n",
            "Epoch 289/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5705 - val_loss: 0.5752\n",
            "Epoch 290/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5704 - val_loss: 0.5751\n",
            "Epoch 291/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5703 - val_loss: 0.5751\n",
            "Epoch 292/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5702 - val_loss: 0.5750\n",
            "Epoch 293/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5702 - val_loss: 0.5750\n",
            "Epoch 294/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5702 - val_loss: 0.5752\n",
            "Epoch 295/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5703 - val_loss: 0.5754\n",
            "Epoch 296/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5705 - val_loss: 0.5753\n",
            "Epoch 297/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5705 - val_loss: 0.5751\n",
            "Epoch 298/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5703 - val_loss: 0.5753\n",
            "Epoch 299/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5704 - val_loss: 0.5752\n",
            "Epoch 300/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5703 - val_loss: 0.5751\n",
            "Epoch 301/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5702 - val_loss: 0.5753\n",
            "Epoch 302/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5703 - val_loss: 0.5752\n",
            "Epoch 303/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5703 - val_loss: 0.5753\n",
            "Epoch 304/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5704 - val_loss: 0.5750\n",
            "Epoch 305/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5701 - val_loss: 0.5752\n",
            "Epoch 306/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5702 - val_loss: 0.5749\n",
            "Epoch 307/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5700 - val_loss: 0.5749\n",
            "Epoch 308/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5700 - val_loss: 0.5749\n",
            "Epoch 309/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5699 - val_loss: 0.5749\n",
            "Epoch 310/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5700 - val_loss: 0.5751\n",
            "Epoch 311/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5701 - val_loss: 0.5750\n",
            "Epoch 312/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5699 - val_loss: 0.5748\n",
            "Epoch 313/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5698 - val_loss: 0.5747\n",
            "Epoch 314/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5698 - val_loss: 0.5748\n",
            "Epoch 315/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5698 - val_loss: 0.5748\n",
            "Epoch 316/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5698 - val_loss: 0.5749\n",
            "Epoch 317/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5699 - val_loss: 0.5748\n",
            "Epoch 318/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5698 - val_loss: 0.5751\n",
            "Epoch 319/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5701 - val_loss: 0.5755\n",
            "Epoch 320/1000\n",
            "2/2 [==============================] - 1s 653ms/step - loss: 0.5704 - val_loss: 0.5749\n",
            "Epoch 321/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5698 - val_loss: 0.5750\n",
            "Epoch 322/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5699 - val_loss: 0.5747\n",
            "Epoch 323/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5698 - val_loss: 0.5748\n",
            "Epoch 324/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5697 - val_loss: 0.5747\n",
            "Epoch 325/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5697 - val_loss: 0.5747\n",
            "Epoch 326/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5696 - val_loss: 0.5746\n",
            "Epoch 327/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5696 - val_loss: 0.5747\n",
            "Epoch 328/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5696 - val_loss: 0.5746\n",
            "Epoch 329/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5695 - val_loss: 0.5746\n",
            "Epoch 330/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5695 - val_loss: 0.5746\n",
            "Epoch 331/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5695 - val_loss: 0.5746\n",
            "Epoch 332/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5695 - val_loss: 0.5745\n",
            "Epoch 333/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5695 - val_loss: 0.5749\n",
            "Epoch 334/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5698 - val_loss: 0.5758\n",
            "Epoch 335/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5708 - val_loss: 0.5754\n",
            "Epoch 336/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5704 - val_loss: 0.5759\n",
            "Epoch 337/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5707 - val_loss: 0.5762\n",
            "Epoch 338/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5711 - val_loss: 0.5760\n",
            "Epoch 339/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5706 - val_loss: 0.5749\n",
            "Epoch 340/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5700 - val_loss: 0.5746\n",
            "Epoch 341/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5696 - val_loss: 0.5748\n",
            "Epoch 342/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5697 - val_loss: 0.5747\n",
            "Epoch 343/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5696 - val_loss: 0.5745\n",
            "Epoch 344/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5695 - val_loss: 0.5745\n",
            "Epoch 345/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5694 - val_loss: 0.5746\n",
            "Epoch 346/1000\n",
            "2/2 [==============================] - 1s 672ms/step - loss: 0.5694 - val_loss: 0.5745\n",
            "Epoch 347/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5693 - val_loss: 0.5744\n",
            "Epoch 348/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5693 - val_loss: 0.5744\n",
            "Epoch 349/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5692 - val_loss: 0.5744\n",
            "Epoch 350/1000\n",
            "2/2 [==============================] - 1s 653ms/step - loss: 0.5692 - val_loss: 0.5744\n",
            "Epoch 351/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5692 - val_loss: 0.5745\n",
            "Epoch 352/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5693 - val_loss: 0.5745\n",
            "Epoch 353/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5692 - val_loss: 0.5745\n",
            "Epoch 354/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5693 - val_loss: 0.5744\n",
            "Epoch 355/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5691 - val_loss: 0.5743\n",
            "Epoch 356/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5690 - val_loss: 0.5744\n",
            "Epoch 357/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5691 - val_loss: 0.5744\n",
            "Epoch 358/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5692 - val_loss: 0.5743\n",
            "Epoch 359/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5690 - val_loss: 0.5743\n",
            "Epoch 360/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5690 - val_loss: 0.5742\n",
            "Epoch 361/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5689 - val_loss: 0.5742\n",
            "Epoch 362/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5689 - val_loss: 0.5743\n",
            "Epoch 363/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5689 - val_loss: 0.5743\n",
            "Epoch 364/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5689 - val_loss: 0.5747\n",
            "Epoch 365/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5694 - val_loss: 0.5752\n",
            "Epoch 366/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5698 - val_loss: 0.5749\n",
            "Epoch 367/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5696 - val_loss: 0.5747\n",
            "Epoch 368/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5693 - val_loss: 0.5748\n",
            "Epoch 369/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5695 - val_loss: 0.5745\n",
            "Epoch 370/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5692 - val_loss: 0.5744\n",
            "Epoch 371/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5690 - val_loss: 0.5744\n",
            "Epoch 372/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5691 - val_loss: 0.5742\n",
            "Epoch 373/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5689 - val_loss: 0.5742\n",
            "Epoch 374/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5689 - val_loss: 0.5742\n",
            "Epoch 375/1000\n",
            "2/2 [==============================] - 1s 652ms/step - loss: 0.5689 - val_loss: 0.5741\n",
            "Epoch 376/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5688 - val_loss: 0.5741\n",
            "Epoch 377/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5687 - val_loss: 0.5742\n",
            "Epoch 378/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5688 - val_loss: 0.5741\n",
            "Epoch 379/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5688 - val_loss: 0.5742\n",
            "Epoch 380/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5687 - val_loss: 0.5741\n",
            "Epoch 381/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5687 - val_loss: 0.5741\n",
            "Epoch 382/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5687 - val_loss: 0.5741\n",
            "Epoch 383/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5687 - val_loss: 0.5740\n",
            "Epoch 384/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5685 - val_loss: 0.5741\n",
            "Epoch 385/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5687 - val_loss: 0.5744\n",
            "Epoch 386/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5691 - val_loss: 0.5751\n",
            "Epoch 387/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5695 - val_loss: 0.5742\n",
            "Epoch 388/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5688 - val_loss: 0.5743\n",
            "Epoch 389/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5690 - val_loss: 0.5741\n",
            "Epoch 390/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5687 - val_loss: 0.5741\n",
            "Epoch 391/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5686 - val_loss: 0.5741\n",
            "Epoch 392/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5687 - val_loss: 0.5739\n",
            "Epoch 393/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5685 - val_loss: 0.5741\n",
            "Epoch 394/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5686 - val_loss: 0.5740\n",
            "Epoch 395/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5685 - val_loss: 0.5740\n",
            "Epoch 396/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5686 - val_loss: 0.5742\n",
            "Epoch 397/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5687 - val_loss: 0.5742\n",
            "Epoch 398/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5687 - val_loss: 0.5744\n",
            "Epoch 399/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5688 - val_loss: 0.5744\n",
            "Epoch 400/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5689 - val_loss: 0.5740\n",
            "Epoch 401/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5685 - val_loss: 0.5741\n",
            "Epoch 402/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5686 - val_loss: 0.5739\n",
            "Epoch 403/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5684 - val_loss: 0.5739\n",
            "Epoch 404/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5684 - val_loss: 0.5740\n",
            "Epoch 405/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5684 - val_loss: 0.5739\n",
            "Epoch 406/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5683 - val_loss: 0.5739\n",
            "Epoch 407/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5684 - val_loss: 0.5739\n",
            "Epoch 408/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5684 - val_loss: 0.5739\n",
            "Epoch 409/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5684 - val_loss: 0.5739\n",
            "Epoch 410/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5683 - val_loss: 0.5738\n",
            "Epoch 411/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5683 - val_loss: 0.5741\n",
            "Epoch 412/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5686 - val_loss: 0.5740\n",
            "Epoch 413/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5684 - val_loss: 0.5740\n",
            "Epoch 414/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5684 - val_loss: 0.5739\n",
            "Epoch 415/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5684 - val_loss: 0.5739\n",
            "Epoch 416/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5683 - val_loss: 0.5739\n",
            "Epoch 417/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5683 - val_loss: 0.5743\n",
            "Epoch 418/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5686 - val_loss: 0.5741\n",
            "Epoch 419/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5685 - val_loss: 0.5740\n",
            "Epoch 420/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5684 - val_loss: 0.5739\n",
            "Epoch 421/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5683 - val_loss: 0.5743\n",
            "Epoch 422/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5687 - val_loss: 0.5741\n",
            "Epoch 423/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5685 - val_loss: 0.5741\n",
            "Epoch 424/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5685 - val_loss: 0.5738\n",
            "Epoch 425/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5682 - val_loss: 0.5738\n",
            "Epoch 426/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5681 - val_loss: 0.5738\n",
            "Epoch 427/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5682 - val_loss: 0.5737\n",
            "Epoch 428/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5681 - val_loss: 0.5736\n",
            "Epoch 429/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5680 - val_loss: 0.5737\n",
            "Epoch 430/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5680 - val_loss: 0.5736\n",
            "Epoch 431/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5679 - val_loss: 0.5735\n",
            "Epoch 432/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5679 - val_loss: 0.5736\n",
            "Epoch 433/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5679 - val_loss: 0.5737\n",
            "Epoch 434/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5680 - val_loss: 0.5737\n",
            "Epoch 435/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5680 - val_loss: 0.5739\n",
            "Epoch 436/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5681 - val_loss: 0.5737\n",
            "Epoch 437/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5679 - val_loss: 0.5736\n",
            "Epoch 438/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5679 - val_loss: 0.5737\n",
            "Epoch 439/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5680 - val_loss: 0.5736\n",
            "Epoch 440/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5679 - val_loss: 0.5735\n",
            "Epoch 441/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5679 - val_loss: 0.5736\n",
            "Epoch 442/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5679 - val_loss: 0.5736\n",
            "Epoch 443/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5679 - val_loss: 0.5736\n",
            "Epoch 444/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5679 - val_loss: 0.5737\n",
            "Epoch 445/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5679 - val_loss: 0.5737\n",
            "Epoch 446/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5680 - val_loss: 0.5737\n",
            "Epoch 447/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5680 - val_loss: 0.5736\n",
            "Epoch 448/1000\n",
            "2/2 [==============================] - 1s 636ms/step - loss: 0.5679 - val_loss: 0.5736\n",
            "Epoch 449/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5679 - val_loss: 0.5741\n",
            "Epoch 450/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5684 - val_loss: 0.5738\n",
            "Epoch 451/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5681 - val_loss: 0.5742\n",
            "Epoch 452/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5685 - val_loss: 0.5742\n",
            "Epoch 453/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5686 - val_loss: 0.5745\n",
            "Epoch 454/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5687 - val_loss: 0.5741\n",
            "Epoch 455/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5684 - val_loss: 0.5737\n",
            "Epoch 456/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5680 - val_loss: 0.5737\n",
            "Epoch 457/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5679 - val_loss: 0.5738\n",
            "Epoch 458/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5682 - val_loss: 0.5737\n",
            "Epoch 459/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5680 - val_loss: 0.5735\n",
            "Epoch 460/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5678 - val_loss: 0.5735\n",
            "Epoch 461/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5677 - val_loss: 0.5736\n",
            "Epoch 462/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5677 - val_loss: 0.5735\n",
            "Epoch 463/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5677 - val_loss: 0.5735\n",
            "Epoch 464/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5677 - val_loss: 0.5734\n",
            "Epoch 465/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5676 - val_loss: 0.5733\n",
            "Epoch 466/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5674 - val_loss: 0.5733\n",
            "Epoch 467/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5675 - val_loss: 0.5735\n",
            "Epoch 468/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5676 - val_loss: 0.5736\n",
            "Epoch 469/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5677 - val_loss: 0.5734\n",
            "Epoch 470/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5676 - val_loss: 0.5733\n",
            "Epoch 471/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5675 - val_loss: 0.5734\n",
            "Epoch 472/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5675 - val_loss: 0.5733\n",
            "Epoch 473/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5674 - val_loss: 0.5735\n",
            "Epoch 474/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5676 - val_loss: 0.5734\n",
            "Epoch 475/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5676 - val_loss: 0.5733\n",
            "Epoch 476/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5674 - val_loss: 0.5733\n",
            "Epoch 477/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5674 - val_loss: 0.5732\n",
            "Epoch 478/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5674 - val_loss: 0.5736\n",
            "Epoch 479/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5676 - val_loss: 0.5735\n",
            "Epoch 480/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5675 - val_loss: 0.5736\n",
            "Epoch 481/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5676 - val_loss: 0.5734\n",
            "Epoch 482/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5675 - val_loss: 0.5736\n",
            "Epoch 483/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5677 - val_loss: 0.5735\n",
            "Epoch 484/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5676 - val_loss: 0.5735\n",
            "Epoch 485/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5676 - val_loss: 0.5733\n",
            "Epoch 486/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5674 - val_loss: 0.5734\n",
            "Epoch 487/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5674 - val_loss: 0.5733\n",
            "Epoch 488/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5673 - val_loss: 0.5731\n",
            "Epoch 489/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5672 - val_loss: 0.5732\n",
            "Epoch 490/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5673 - val_loss: 0.5732\n",
            "Epoch 491/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5673 - val_loss: 0.5731\n",
            "Epoch 492/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5671 - val_loss: 0.5732\n",
            "Epoch 493/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5672 - val_loss: 0.5733\n",
            "Epoch 494/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5673 - val_loss: 0.5732\n",
            "Epoch 495/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5673 - val_loss: 0.5731\n",
            "Epoch 496/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5672 - val_loss: 0.5733\n",
            "Epoch 497/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5673 - val_loss: 0.5736\n",
            "Epoch 498/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5675 - val_loss: 0.5732\n",
            "Epoch 499/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5673 - val_loss: 0.5731\n",
            "Epoch 500/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5672 - val_loss: 0.5734\n",
            "Epoch 501/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5674 - val_loss: 0.5729\n",
            "Epoch 502/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5670 - val_loss: 0.5732\n",
            "Epoch 503/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5672 - val_loss: 0.5730\n",
            "Epoch 504/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5670 - val_loss: 0.5732\n",
            "Epoch 505/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5672 - val_loss: 0.5733\n",
            "Epoch 506/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5673 - val_loss: 0.5733\n",
            "Epoch 507/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5673 - val_loss: 0.5730\n",
            "Epoch 508/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5670 - val_loss: 0.5731\n",
            "Epoch 509/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5672 - val_loss: 0.5731\n",
            "Epoch 510/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5671 - val_loss: 0.5732\n",
            "Epoch 511/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5673 - val_loss: 0.5732\n",
            "Epoch 512/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5673 - val_loss: 0.5732\n",
            "Epoch 513/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5673 - val_loss: 0.5731\n",
            "Epoch 514/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5671 - val_loss: 0.5730\n",
            "Epoch 515/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5670 - val_loss: 0.5729\n",
            "Epoch 516/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5669 - val_loss: 0.5728\n",
            "Epoch 517/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5668 - val_loss: 0.5729\n",
            "Epoch 518/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5668 - val_loss: 0.5729\n",
            "Epoch 519/1000\n",
            "2/2 [==============================] - 1s 670ms/step - loss: 0.5669 - val_loss: 0.5730\n",
            "Epoch 520/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5670 - val_loss: 0.5733\n",
            "Epoch 521/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5672 - val_loss: 0.5736\n",
            "Epoch 522/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5675 - val_loss: 0.5737\n",
            "Epoch 523/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5676 - val_loss: 0.5735\n",
            "Epoch 524/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5674 - val_loss: 0.5738\n",
            "Epoch 525/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5676 - val_loss: 0.5733\n",
            "Epoch 526/1000\n",
            "2/2 [==============================] - 1s 670ms/step - loss: 0.5672 - val_loss: 0.5732\n",
            "Epoch 527/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5671 - val_loss: 0.5732\n",
            "Epoch 528/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5671 - val_loss: 0.5730\n",
            "Epoch 529/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5670 - val_loss: 0.5730\n",
            "Epoch 530/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5669 - val_loss: 0.5730\n",
            "Epoch 531/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5669 - val_loss: 0.5730\n",
            "Epoch 532/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5669 - val_loss: 0.5727\n",
            "Epoch 533/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5666 - val_loss: 0.5727\n",
            "Epoch 534/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5666 - val_loss: 0.5729\n",
            "Epoch 535/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5667 - val_loss: 0.5729\n",
            "Epoch 536/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5669 - val_loss: 0.5730\n",
            "Epoch 537/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5670 - val_loss: 0.5729\n",
            "Epoch 538/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5668 - val_loss: 0.5729\n",
            "Epoch 539/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5668 - val_loss: 0.5729\n",
            "Epoch 540/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5668 - val_loss: 0.5729\n",
            "Epoch 541/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5667 - val_loss: 0.5729\n",
            "Epoch 542/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5668 - val_loss: 0.5730\n",
            "Epoch 543/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5669 - val_loss: 0.5729\n",
            "Epoch 544/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5667 - val_loss: 0.5729\n",
            "Epoch 545/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5668 - val_loss: 0.5726\n",
            "Epoch 546/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5665 - val_loss: 0.5727\n",
            "Epoch 547/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5666 - val_loss: 0.5727\n",
            "Epoch 548/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5665 - val_loss: 0.5727\n",
            "Epoch 549/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5666 - val_loss: 0.5727\n",
            "Epoch 550/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5665 - val_loss: 0.5727\n",
            "Epoch 551/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5665 - val_loss: 0.5727\n",
            "Epoch 552/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5665 - val_loss: 0.5727\n",
            "Epoch 553/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5665 - val_loss: 0.5727\n",
            "Epoch 554/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5665 - val_loss: 0.5730\n",
            "Epoch 555/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5667 - val_loss: 0.5729\n",
            "Epoch 556/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5667 - val_loss: 0.5731\n",
            "Epoch 557/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5669 - val_loss: 0.5733\n",
            "Epoch 558/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5670 - val_loss: 0.5733\n",
            "Epoch 559/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5670 - val_loss: 0.5728\n",
            "Epoch 560/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5667 - val_loss: 0.5727\n",
            "Epoch 561/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5665 - val_loss: 0.5728\n",
            "Epoch 562/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5666 - val_loss: 0.5726\n",
            "Epoch 563/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5665 - val_loss: 0.5726\n",
            "Epoch 564/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5663 - val_loss: 0.5727\n",
            "Epoch 565/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5664 - val_loss: 0.5728\n",
            "Epoch 566/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5665 - val_loss: 0.5728\n",
            "Epoch 567/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5666 - val_loss: 0.5729\n",
            "Epoch 568/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5666 - val_loss: 0.5726\n",
            "Epoch 569/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5663 - val_loss: 0.5731\n",
            "Epoch 570/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5668 - val_loss: 0.5730\n",
            "Epoch 571/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5667 - val_loss: 0.5728\n",
            "Epoch 572/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5665 - val_loss: 0.5729\n",
            "Epoch 573/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5666 - val_loss: 0.5725\n",
            "Epoch 574/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5663 - val_loss: 0.5728\n",
            "Epoch 575/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5665 - val_loss: 0.5725\n",
            "Epoch 576/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5662 - val_loss: 0.5726\n",
            "Epoch 577/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5663 - val_loss: 0.5723\n",
            "Epoch 578/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5661 - val_loss: 0.5724\n",
            "Epoch 579/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5662 - val_loss: 0.5724\n",
            "Epoch 580/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5660 - val_loss: 0.5724\n",
            "Epoch 581/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5661 - val_loss: 0.5725\n",
            "Epoch 582/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5662 - val_loss: 0.5729\n",
            "Epoch 583/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5667 - val_loss: 0.5730\n",
            "Epoch 584/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5667 - val_loss: 0.5727\n",
            "Epoch 585/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5664 - val_loss: 0.5728\n",
            "Epoch 586/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5666 - val_loss: 0.5726\n",
            "Epoch 587/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5663 - val_loss: 0.5728\n",
            "Epoch 588/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5665 - val_loss: 0.5726\n",
            "Epoch 589/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5663 - val_loss: 0.5726\n",
            "Epoch 590/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5663 - val_loss: 0.5729\n",
            "Epoch 591/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5664 - val_loss: 0.5727\n",
            "Epoch 592/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5664 - val_loss: 0.5725\n",
            "Epoch 593/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5662 - val_loss: 0.5728\n",
            "Epoch 594/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5664 - val_loss: 0.5725\n",
            "Epoch 595/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5661 - val_loss: 0.5724\n",
            "Epoch 596/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5660 - val_loss: 0.5724\n",
            "Epoch 597/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5660 - val_loss: 0.5724\n",
            "Epoch 598/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5660 - val_loss: 0.5723\n",
            "Epoch 599/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5659 - val_loss: 0.5726\n",
            "Epoch 600/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5662 - val_loss: 0.5726\n",
            "Epoch 601/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5662 - val_loss: 0.5724\n",
            "Epoch 602/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5659 - val_loss: 0.5726\n",
            "Epoch 603/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5662 - val_loss: 0.5727\n",
            "Epoch 604/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5664 - val_loss: 0.5726\n",
            "Epoch 605/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5663 - val_loss: 0.5730\n",
            "Epoch 606/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5665 - val_loss: 0.5727\n",
            "Epoch 607/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5663 - val_loss: 0.5726\n",
            "Epoch 608/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5662 - val_loss: 0.5725\n",
            "Epoch 609/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5661 - val_loss: 0.5725\n",
            "Epoch 610/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5661 - val_loss: 0.5726\n",
            "Epoch 611/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5662 - val_loss: 0.5724\n",
            "Epoch 612/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5660 - val_loss: 0.5723\n",
            "Epoch 613/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5659 - val_loss: 0.5722\n",
            "Epoch 614/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5658 - val_loss: 0.5723\n",
            "Epoch 615/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5658 - val_loss: 0.5722\n",
            "Epoch 616/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5657 - val_loss: 0.5722\n",
            "Epoch 617/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5657 - val_loss: 0.5723\n",
            "Epoch 618/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5658 - val_loss: 0.5724\n",
            "Epoch 619/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5661 - val_loss: 0.5726\n",
            "Epoch 620/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5661 - val_loss: 0.5725\n",
            "Epoch 621/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5661 - val_loss: 0.5729\n",
            "Epoch 622/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5663 - val_loss: 0.5721\n",
            "Epoch 623/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5657 - val_loss: 0.5723\n",
            "Epoch 624/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5658 - val_loss: 0.5722\n",
            "Epoch 625/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5658 - val_loss: 0.5721\n",
            "Epoch 626/1000\n",
            "2/2 [==============================] - 1s 672ms/step - loss: 0.5656 - val_loss: 0.5724\n",
            "Epoch 627/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5659 - val_loss: 0.5723\n",
            "Epoch 628/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5659 - val_loss: 0.5723\n",
            "Epoch 629/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5658 - val_loss: 0.5723\n",
            "Epoch 630/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5658 - val_loss: 0.5724\n",
            "Epoch 631/1000\n",
            "2/2 [==============================] - 1s 652ms/step - loss: 0.5659 - val_loss: 0.5723\n",
            "Epoch 632/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5658 - val_loss: 0.5722\n",
            "Epoch 633/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5658 - val_loss: 0.5721\n",
            "Epoch 634/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5656 - val_loss: 0.5720\n",
            "Epoch 635/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5655 - val_loss: 0.5720\n",
            "Epoch 636/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5655 - val_loss: 0.5720\n",
            "Epoch 637/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5654 - val_loss: 0.5721\n",
            "Epoch 638/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5656 - val_loss: 0.5724\n",
            "Epoch 639/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5658 - val_loss: 0.5722\n",
            "Epoch 640/1000\n",
            "2/2 [==============================] - 1s 653ms/step - loss: 0.5657 - val_loss: 0.5721\n",
            "Epoch 641/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5656 - val_loss: 0.5721\n",
            "Epoch 642/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5655 - val_loss: 0.5720\n",
            "Epoch 643/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5654 - val_loss: 0.5721\n",
            "Epoch 644/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5655 - val_loss: 0.5720\n",
            "Epoch 645/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5655 - val_loss: 0.5721\n",
            "Epoch 646/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5657 - val_loss: 0.5722\n",
            "Epoch 647/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5657 - val_loss: 0.5725\n",
            "Epoch 648/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5659 - val_loss: 0.5724\n",
            "Epoch 649/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5657 - val_loss: 0.5724\n",
            "Epoch 650/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5659 - val_loss: 0.5723\n",
            "Epoch 651/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5657 - val_loss: 0.5724\n",
            "Epoch 652/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5657 - val_loss: 0.5722\n",
            "Epoch 653/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5656 - val_loss: 0.5724\n",
            "Epoch 654/1000\n",
            "2/2 [==============================] - 1s 642ms/step - loss: 0.5658 - val_loss: 0.5724\n",
            "Epoch 655/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5658 - val_loss: 0.5719\n",
            "Epoch 656/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5654 - val_loss: 0.5720\n",
            "Epoch 657/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5654 - val_loss: 0.5720\n",
            "Epoch 658/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5654 - val_loss: 0.5719\n",
            "Epoch 659/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5653 - val_loss: 0.5720\n",
            "Epoch 660/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5653 - val_loss: 0.5720\n",
            "Epoch 661/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5654 - val_loss: 0.5719\n",
            "Epoch 662/1000\n",
            "2/2 [==============================] - 1s 670ms/step - loss: 0.5653 - val_loss: 0.5720\n",
            "Epoch 663/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5654 - val_loss: 0.5722\n",
            "Epoch 664/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5656 - val_loss: 0.5720\n",
            "Epoch 665/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5653 - val_loss: 0.5721\n",
            "Epoch 666/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5655 - val_loss: 0.5724\n",
            "Epoch 667/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5657 - val_loss: 0.5721\n",
            "Epoch 668/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5655 - val_loss: 0.5724\n",
            "Epoch 669/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5657 - val_loss: 0.5722\n",
            "Epoch 670/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5657 - val_loss: 0.5721\n",
            "Epoch 671/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5654 - val_loss: 0.5719\n",
            "Epoch 672/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5653 - val_loss: 0.5720\n",
            "Epoch 673/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5653 - val_loss: 0.5719\n",
            "Epoch 674/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5652 - val_loss: 0.5719\n",
            "Epoch 675/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5652 - val_loss: 0.5718\n",
            "Epoch 676/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5651 - val_loss: 0.5718\n",
            "Epoch 677/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5651 - val_loss: 0.5719\n",
            "Epoch 678/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5652 - val_loss: 0.5719\n",
            "Epoch 679/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5652 - val_loss: 0.5717\n",
            "Epoch 680/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5650 - val_loss: 0.5717\n",
            "Epoch 681/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5650 - val_loss: 0.5720\n",
            "Epoch 682/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5653 - val_loss: 0.5720\n",
            "Epoch 683/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5654 - val_loss: 0.5717\n",
            "Epoch 684/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5650 - val_loss: 0.5720\n",
            "Epoch 685/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5654 - val_loss: 0.5717\n",
            "Epoch 686/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5650 - val_loss: 0.5718\n",
            "Epoch 687/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5651 - val_loss: 0.5717\n",
            "Epoch 688/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5650 - val_loss: 0.5719\n",
            "Epoch 689/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5652 - val_loss: 0.5719\n",
            "Epoch 690/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5652 - val_loss: 0.5724\n",
            "Epoch 691/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5656 - val_loss: 0.5719\n",
            "Epoch 692/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5651 - val_loss: 0.5719\n",
            "Epoch 693/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5652 - val_loss: 0.5722\n",
            "Epoch 694/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5654 - val_loss: 0.5719\n",
            "Epoch 695/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5652 - val_loss: 0.5718\n",
            "Epoch 696/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5650 - val_loss: 0.5719\n",
            "Epoch 697/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5651 - val_loss: 0.5717\n",
            "Epoch 698/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5650 - val_loss: 0.5721\n",
            "Epoch 699/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5653 - val_loss: 0.5718\n",
            "Epoch 700/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5650 - val_loss: 0.5717\n",
            "Epoch 701/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5650 - val_loss: 0.5721\n",
            "Epoch 702/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5652 - val_loss: 0.5720\n",
            "Epoch 703/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5652 - val_loss: 0.5718\n",
            "Epoch 704/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5650 - val_loss: 0.5717\n",
            "Epoch 705/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5650 - val_loss: 0.5716\n",
            "Epoch 706/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5649 - val_loss: 0.5718\n",
            "Epoch 707/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5650 - val_loss: 0.5715\n",
            "Epoch 708/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5648 - val_loss: 0.5716\n",
            "Epoch 709/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5649 - val_loss: 0.5716\n",
            "Epoch 710/1000\n",
            "2/2 [==============================] - 1s 672ms/step - loss: 0.5649 - val_loss: 0.5718\n",
            "Epoch 711/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5651 - val_loss: 0.5719\n",
            "Epoch 712/1000\n",
            "2/2 [==============================] - 1s 643ms/step - loss: 0.5652 - val_loss: 0.5717\n",
            "Epoch 713/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5650 - val_loss: 0.5716\n",
            "Epoch 714/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5649 - val_loss: 0.5719\n",
            "Epoch 715/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5651 - val_loss: 0.5716\n",
            "Epoch 716/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5649 - val_loss: 0.5718\n",
            "Epoch 717/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5649 - val_loss: 0.5715\n",
            "Epoch 718/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5648 - val_loss: 0.5715\n",
            "Epoch 719/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5648 - val_loss: 0.5715\n",
            "Epoch 720/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5647 - val_loss: 0.5715\n",
            "Epoch 721/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5647 - val_loss: 0.5718\n",
            "Epoch 722/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5650 - val_loss: 0.5718\n",
            "Epoch 723/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5651 - val_loss: 0.5717\n",
            "Epoch 724/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5649 - val_loss: 0.5719\n",
            "Epoch 725/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5650 - val_loss: 0.5716\n",
            "Epoch 726/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5648 - val_loss: 0.5716\n",
            "Epoch 727/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5648 - val_loss: 0.5714\n",
            "Epoch 728/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5646 - val_loss: 0.5717\n",
            "Epoch 729/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5647 - val_loss: 0.5715\n",
            "Epoch 730/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5646 - val_loss: 0.5715\n",
            "Epoch 731/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5646 - val_loss: 0.5717\n",
            "Epoch 732/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5648 - val_loss: 0.5717\n",
            "Epoch 733/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5648 - val_loss: 0.5718\n",
            "Epoch 734/1000\n",
            "2/2 [==============================] - 1s 671ms/step - loss: 0.5649 - val_loss: 0.5720\n",
            "Epoch 735/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5651 - val_loss: 0.5718\n",
            "Epoch 736/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5648 - val_loss: 0.5719\n",
            "Epoch 737/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5651 - val_loss: 0.5717\n",
            "Epoch 738/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5648 - val_loss: 0.5717\n",
            "Epoch 739/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5649 - val_loss: 0.5715\n",
            "Epoch 740/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5646 - val_loss: 0.5716\n",
            "Epoch 741/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5646 - val_loss: 0.5714\n",
            "Epoch 742/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5645 - val_loss: 0.5712\n",
            "Epoch 743/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5644 - val_loss: 0.5713\n",
            "Epoch 744/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5644 - val_loss: 0.5713\n",
            "Epoch 745/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5644 - val_loss: 0.5714\n",
            "Epoch 746/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5645 - val_loss: 0.5715\n",
            "Epoch 747/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5646 - val_loss: 0.5715\n",
            "Epoch 748/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5646 - val_loss: 0.5713\n",
            "Epoch 749/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5644 - val_loss: 0.5715\n",
            "Epoch 750/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5645 - val_loss: 0.5714\n",
            "Epoch 751/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5645 - val_loss: 0.5716\n",
            "Epoch 752/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5647 - val_loss: 0.5714\n",
            "Epoch 753/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5645 - val_loss: 0.5717\n",
            "Epoch 754/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5647 - val_loss: 0.5714\n",
            "Epoch 755/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5645 - val_loss: 0.5719\n",
            "Epoch 756/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5649 - val_loss: 0.5719\n",
            "Epoch 757/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5650 - val_loss: 0.5721\n",
            "Epoch 758/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5650 - val_loss: 0.5717\n",
            "Epoch 759/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5647 - val_loss: 0.5714\n",
            "Epoch 760/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5645 - val_loss: 0.5715\n",
            "Epoch 761/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5646 - val_loss: 0.5715\n",
            "Epoch 762/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5645 - val_loss: 0.5713\n",
            "Epoch 763/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5643 - val_loss: 0.5711\n",
            "Epoch 764/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5642 - val_loss: 0.5715\n",
            "Epoch 765/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5645 - val_loss: 0.5712\n",
            "Epoch 766/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5643 - val_loss: 0.5712\n",
            "Epoch 767/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5643 - val_loss: 0.5712\n",
            "Epoch 768/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5642 - val_loss: 0.5713\n",
            "Epoch 769/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5643 - val_loss: 0.5713\n",
            "Epoch 770/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5643 - val_loss: 0.5712\n",
            "Epoch 771/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5643 - val_loss: 0.5714\n",
            "Epoch 772/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5644 - val_loss: 0.5717\n",
            "Epoch 773/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5646 - val_loss: 0.5715\n",
            "Epoch 774/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5645 - val_loss: 0.5714\n",
            "Epoch 775/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5644 - val_loss: 0.5715\n",
            "Epoch 776/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5645 - val_loss: 0.5712\n",
            "Epoch 777/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5643 - val_loss: 0.5714\n",
            "Epoch 778/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5645 - val_loss: 0.5715\n",
            "Epoch 779/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5645 - val_loss: 0.5714\n",
            "Epoch 780/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5645 - val_loss: 0.5717\n",
            "Epoch 781/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5646 - val_loss: 0.5712\n",
            "Epoch 782/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5642 - val_loss: 0.5712\n",
            "Epoch 783/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5642 - val_loss: 0.5713\n",
            "Epoch 784/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5643 - val_loss: 0.5710\n",
            "Epoch 785/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5640 - val_loss: 0.5713\n",
            "Epoch 786/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5643 - val_loss: 0.5713\n",
            "Epoch 787/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5642 - val_loss: 0.5712\n",
            "Epoch 788/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5641 - val_loss: 0.5712\n",
            "Epoch 789/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5642 - val_loss: 0.5713\n",
            "Epoch 790/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5642 - val_loss: 0.5713\n",
            "Epoch 791/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5642 - val_loss: 0.5711\n",
            "Epoch 792/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5640 - val_loss: 0.5710\n",
            "Epoch 793/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5640 - val_loss: 0.5713\n",
            "Epoch 794/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5642 - val_loss: 0.5712\n",
            "Epoch 795/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5641 - val_loss: 0.5713\n",
            "Epoch 796/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5642 - val_loss: 0.5718\n",
            "Epoch 797/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5648 - val_loss: 0.5714\n",
            "Epoch 798/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5643 - val_loss: 0.5719\n",
            "Epoch 799/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5647 - val_loss: 0.5711\n",
            "Epoch 800/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5641 - val_loss: 0.5714\n",
            "Epoch 801/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5643 - val_loss: 0.5711\n",
            "Epoch 802/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5640 - val_loss: 0.5712\n",
            "Epoch 803/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5641 - val_loss: 0.5712\n",
            "Epoch 804/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5641 - val_loss: 0.5710\n",
            "Epoch 805/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5639 - val_loss: 0.5709\n",
            "Epoch 806/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5638 - val_loss: 0.5710\n",
            "Epoch 807/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5639 - val_loss: 0.5711\n",
            "Epoch 808/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5640 - val_loss: 0.5712\n",
            "Epoch 809/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5640 - val_loss: 0.5712\n",
            "Epoch 810/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5640 - val_loss: 0.5711\n",
            "Epoch 811/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5640 - val_loss: 0.5712\n",
            "Epoch 812/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5641 - val_loss: 0.5713\n",
            "Epoch 813/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5641 - val_loss: 0.5712\n",
            "Epoch 814/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5641 - val_loss: 0.5710\n",
            "Epoch 815/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5639 - val_loss: 0.5711\n",
            "Epoch 816/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5639 - val_loss: 0.5711\n",
            "Epoch 817/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5639 - val_loss: 0.5709\n",
            "Epoch 818/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5638 - val_loss: 0.5712\n",
            "Epoch 819/1000\n",
            "2/2 [==============================] - 1s 670ms/step - loss: 0.5640 - val_loss: 0.5709\n",
            "Epoch 820/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5637 - val_loss: 0.5710\n",
            "Epoch 821/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5638 - val_loss: 0.5710\n",
            "Epoch 822/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5638 - val_loss: 0.5713\n",
            "Epoch 823/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5640 - val_loss: 0.5712\n",
            "Epoch 824/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5640 - val_loss: 0.5712\n",
            "Epoch 825/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5639 - val_loss: 0.5711\n",
            "Epoch 826/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5640 - val_loss: 0.5710\n",
            "Epoch 827/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5638 - val_loss: 0.5712\n",
            "Epoch 828/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5639 - val_loss: 0.5710\n",
            "Epoch 829/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5639 - val_loss: 0.5712\n",
            "Epoch 830/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5640 - val_loss: 0.5711\n",
            "Epoch 831/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5640 - val_loss: 0.5713\n",
            "Epoch 832/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5641 - val_loss: 0.5710\n",
            "Epoch 833/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5638 - val_loss: 0.5712\n",
            "Epoch 834/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5640 - val_loss: 0.5710\n",
            "Epoch 835/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5639 - val_loss: 0.5709\n",
            "Epoch 836/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5637 - val_loss: 0.5711\n",
            "Epoch 837/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5639 - val_loss: 0.5709\n",
            "Epoch 838/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5637 - val_loss: 0.5709\n",
            "Epoch 839/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5637 - val_loss: 0.5710\n",
            "Epoch 840/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5637 - val_loss: 0.5708\n",
            "Epoch 841/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5637 - val_loss: 0.5709\n",
            "Epoch 842/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5637 - val_loss: 0.5713\n",
            "Epoch 843/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5640 - val_loss: 0.5709\n",
            "Epoch 844/1000\n",
            "2/2 [==============================] - 1s 671ms/step - loss: 0.5636 - val_loss: 0.5708\n",
            "Epoch 845/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5636 - val_loss: 0.5707\n",
            "Epoch 846/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5635 - val_loss: 0.5708\n",
            "Epoch 847/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5635 - val_loss: 0.5712\n",
            "Epoch 848/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5639 - val_loss: 0.5712\n",
            "Epoch 849/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5639 - val_loss: 0.5711\n",
            "Epoch 850/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5638 - val_loss: 0.5710\n",
            "Epoch 851/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5638 - val_loss: 0.5710\n",
            "Epoch 852/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5637 - val_loss: 0.5711\n",
            "Epoch 853/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5637 - val_loss: 0.5707\n",
            "Epoch 854/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5635 - val_loss: 0.5712\n",
            "Epoch 855/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5639 - val_loss: 0.5709\n",
            "Epoch 856/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5637 - val_loss: 0.5712\n",
            "Epoch 857/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5640 - val_loss: 0.5711\n",
            "Epoch 858/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5637 - val_loss: 0.5711\n",
            "Epoch 859/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5637 - val_loss: 0.5708\n",
            "Epoch 860/1000\n",
            "2/2 [==============================] - 1s 650ms/step - loss: 0.5635 - val_loss: 0.5709\n",
            "Epoch 861/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5636 - val_loss: 0.5706\n",
            "Epoch 862/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5634 - val_loss: 0.5708\n",
            "Epoch 863/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5635 - val_loss: 0.5706\n",
            "Epoch 864/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5633 - val_loss: 0.5707\n",
            "Epoch 865/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5634 - val_loss: 0.5705\n",
            "Epoch 866/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5632 - val_loss: 0.5706\n",
            "Epoch 867/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5633 - val_loss: 0.5707\n",
            "Epoch 868/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5634 - val_loss: 0.5707\n",
            "Epoch 869/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5634 - val_loss: 0.5706\n",
            "Epoch 870/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5633 - val_loss: 0.5707\n",
            "Epoch 871/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5634 - val_loss: 0.5710\n",
            "Epoch 872/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5639 - val_loss: 0.5715\n",
            "Epoch 873/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5642 - val_loss: 0.5711\n",
            "Epoch 874/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5639 - val_loss: 0.5713\n",
            "Epoch 875/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5639 - val_loss: 0.5709\n",
            "Epoch 876/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5635 - val_loss: 0.5706\n",
            "Epoch 877/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5633 - val_loss: 0.5709\n",
            "Epoch 878/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5635 - val_loss: 0.5707\n",
            "Epoch 879/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5633 - val_loss: 0.5708\n",
            "Epoch 880/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5635 - val_loss: 0.5707\n",
            "Epoch 881/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5633 - val_loss: 0.5708\n",
            "Epoch 882/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5634 - val_loss: 0.5705\n",
            "Epoch 883/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5632 - val_loss: 0.5706\n",
            "Epoch 884/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5632 - val_loss: 0.5705\n",
            "Epoch 885/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5631 - val_loss: 0.5706\n",
            "Epoch 886/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5632 - val_loss: 0.5709\n",
            "Epoch 887/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5635 - val_loss: 0.5709\n",
            "Epoch 888/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5635 - val_loss: 0.5707\n",
            "Epoch 889/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5633 - val_loss: 0.5707\n",
            "Epoch 890/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5633 - val_loss: 0.5707\n",
            "Epoch 891/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5633 - val_loss: 0.5704\n",
            "Epoch 892/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5630 - val_loss: 0.5706\n",
            "Epoch 893/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5632 - val_loss: 0.5705\n",
            "Epoch 894/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5631 - val_loss: 0.5709\n",
            "Epoch 895/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5635 - val_loss: 0.5709\n",
            "Epoch 896/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5634 - val_loss: 0.5710\n",
            "Epoch 897/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5634 - val_loss: 0.5706\n",
            "Epoch 898/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5632 - val_loss: 0.5711\n",
            "Epoch 899/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5636 - val_loss: 0.5706\n",
            "Epoch 900/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5632 - val_loss: 0.5705\n",
            "Epoch 901/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5631 - val_loss: 0.5708\n",
            "Epoch 902/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5633 - val_loss: 0.5705\n",
            "Epoch 903/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5631 - val_loss: 0.5707\n",
            "Epoch 904/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5631 - val_loss: 0.5705\n",
            "Epoch 905/1000\n",
            "2/2 [==============================] - 1s 671ms/step - loss: 0.5631 - val_loss: 0.5704\n",
            "Epoch 906/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5629 - val_loss: 0.5706\n",
            "Epoch 907/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5631 - val_loss: 0.5707\n",
            "Epoch 908/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5632 - val_loss: 0.5705\n",
            "Epoch 909/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5630 - val_loss: 0.5709\n",
            "Epoch 910/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5634 - val_loss: 0.5709\n",
            "Epoch 911/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5634 - val_loss: 0.5710\n",
            "Epoch 912/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5635 - val_loss: 0.5713\n",
            "Epoch 913/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5639 - val_loss: 0.5710\n",
            "Epoch 914/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5635 - val_loss: 0.5709\n",
            "Epoch 915/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5634 - val_loss: 0.5708\n",
            "Epoch 916/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5633 - val_loss: 0.5705\n",
            "Epoch 917/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5630 - val_loss: 0.5708\n",
            "Epoch 918/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5634 - val_loss: 0.5707\n",
            "Epoch 919/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5632 - val_loss: 0.5706\n",
            "Epoch 920/1000\n",
            "2/2 [==============================] - 1s 670ms/step - loss: 0.5631 - val_loss: 0.5705\n",
            "Epoch 921/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5630 - val_loss: 0.5705\n",
            "Epoch 922/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5630 - val_loss: 0.5704\n",
            "Epoch 923/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5628 - val_loss: 0.5704\n",
            "Epoch 924/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5629 - val_loss: 0.5704\n",
            "Epoch 925/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5629 - val_loss: 0.5706\n",
            "Epoch 926/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5631 - val_loss: 0.5708\n",
            "Epoch 927/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5633 - val_loss: 0.5706\n",
            "Epoch 928/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5630 - val_loss: 0.5705\n",
            "Epoch 929/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5629 - val_loss: 0.5703\n",
            "Epoch 930/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5628 - val_loss: 0.5706\n",
            "Epoch 931/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5630 - val_loss: 0.5704\n",
            "Epoch 932/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5628 - val_loss: 0.5704\n",
            "Epoch 933/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5628 - val_loss: 0.5706\n",
            "Epoch 934/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5630 - val_loss: 0.5705\n",
            "Epoch 935/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5630 - val_loss: 0.5706\n",
            "Epoch 936/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5630 - val_loss: 0.5703\n",
            "Epoch 937/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5628 - val_loss: 0.5707\n",
            "Epoch 938/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5631 - val_loss: 0.5709\n",
            "Epoch 939/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5634 - val_loss: 0.5707\n",
            "Epoch 940/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5632 - val_loss: 0.5707\n",
            "Epoch 941/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5632 - val_loss: 0.5709\n",
            "Epoch 942/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5631 - val_loss: 0.5707\n",
            "Epoch 943/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5632 - val_loss: 0.5710\n",
            "Epoch 944/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5633 - val_loss: 0.5705\n",
            "Epoch 945/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5629 - val_loss: 0.5703\n",
            "Epoch 946/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5628 - val_loss: 0.5704\n",
            "Epoch 947/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5628 - val_loss: 0.5703\n",
            "Epoch 948/1000\n",
            "2/2 [==============================] - 1s 671ms/step - loss: 0.5627 - val_loss: 0.5702\n",
            "Epoch 949/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5626 - val_loss: 0.5703\n",
            "Epoch 950/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5627 - val_loss: 0.5703\n",
            "Epoch 951/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5627 - val_loss: 0.5703\n",
            "Epoch 952/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5627 - val_loss: 0.5703\n",
            "Epoch 953/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5627 - val_loss: 0.5703\n",
            "Epoch 954/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5627 - val_loss: 0.5705\n",
            "Epoch 955/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5628 - val_loss: 0.5707\n",
            "Epoch 956/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5630 - val_loss: 0.5705\n",
            "Epoch 957/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5628 - val_loss: 0.5707\n",
            "Epoch 958/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5631 - val_loss: 0.5707\n",
            "Epoch 959/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5630 - val_loss: 0.5705\n",
            "Epoch 960/1000\n",
            "2/2 [==============================] - 1s 668ms/step - loss: 0.5629 - val_loss: 0.5705\n",
            "Epoch 961/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5628 - val_loss: 0.5703\n",
            "Epoch 962/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5627 - val_loss: 0.5703\n",
            "Epoch 963/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5627 - val_loss: 0.5706\n",
            "Epoch 964/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5629 - val_loss: 0.5704\n",
            "Epoch 965/1000\n",
            "2/2 [==============================] - 1s 658ms/step - loss: 0.5627 - val_loss: 0.5706\n",
            "Epoch 966/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5630 - val_loss: 0.5709\n",
            "Epoch 967/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5631 - val_loss: 0.5704\n",
            "Epoch 968/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5627 - val_loss: 0.5704\n",
            "Epoch 969/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5627 - val_loss: 0.5701\n",
            "Epoch 970/1000\n",
            "2/2 [==============================] - 1s 670ms/step - loss: 0.5625 - val_loss: 0.5703\n",
            "Epoch 971/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5626 - val_loss: 0.5701\n",
            "Epoch 972/1000\n",
            "2/2 [==============================] - 1s 666ms/step - loss: 0.5624 - val_loss: 0.5702\n",
            "Epoch 973/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5625 - val_loss: 0.5701\n",
            "Epoch 974/1000\n",
            "2/2 [==============================] - 1s 655ms/step - loss: 0.5624 - val_loss: 0.5701\n",
            "Epoch 975/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5624 - val_loss: 0.5701\n",
            "Epoch 976/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5624 - val_loss: 0.5702\n",
            "Epoch 977/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5626 - val_loss: 0.5703\n",
            "Epoch 978/1000\n",
            "2/2 [==============================] - 1s 669ms/step - loss: 0.5626 - val_loss: 0.5702\n",
            "Epoch 979/1000\n",
            "2/2 [==============================] - 1s 667ms/step - loss: 0.5626 - val_loss: 0.5704\n",
            "Epoch 980/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5627 - val_loss: 0.5707\n",
            "Epoch 981/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5629 - val_loss: 0.5707\n",
            "Epoch 982/1000\n",
            "2/2 [==============================] - 1s 657ms/step - loss: 0.5630 - val_loss: 0.5704\n",
            "Epoch 983/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5628 - val_loss: 0.5708\n",
            "Epoch 984/1000\n",
            "2/2 [==============================] - 1s 661ms/step - loss: 0.5630 - val_loss: 0.5701\n",
            "Epoch 985/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5625 - val_loss: 0.5702\n",
            "Epoch 986/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5626 - val_loss: 0.5704\n",
            "Epoch 987/1000\n",
            "2/2 [==============================] - 1s 659ms/step - loss: 0.5627 - val_loss: 0.5702\n",
            "Epoch 988/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5626 - val_loss: 0.5707\n",
            "Epoch 989/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5630 - val_loss: 0.5706\n",
            "Epoch 990/1000\n",
            "2/2 [==============================] - 1s 660ms/step - loss: 0.5628 - val_loss: 0.5706\n",
            "Epoch 991/1000\n",
            "2/2 [==============================] - 1s 663ms/step - loss: 0.5628 - val_loss: 0.5701\n",
            "Epoch 992/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5625 - val_loss: 0.5704\n",
            "Epoch 993/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5627 - val_loss: 0.5703\n",
            "Epoch 994/1000\n",
            "2/2 [==============================] - 1s 665ms/step - loss: 0.5625 - val_loss: 0.5700\n",
            "Epoch 995/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5623 - val_loss: 0.5703\n",
            "Epoch 996/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5625 - val_loss: 0.5702\n",
            "Epoch 997/1000\n",
            "2/2 [==============================] - 1s 664ms/step - loss: 0.5625 - val_loss: 0.5702\n",
            "Epoch 998/1000\n",
            "2/2 [==============================] - 1s 662ms/step - loss: 0.5624 - val_loss: 0.5703\n",
            "Epoch 999/1000\n",
            "2/2 [==============================] - 1s 654ms/step - loss: 0.5624 - val_loss: 0.5702\n",
            "Epoch 1000/1000\n",
            "2/2 [==============================] - 1s 656ms/step - loss: 0.5624 - val_loss: 0.5701\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe1ecc2bfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLtYr4iTzKKG"
      },
      "source": [
        "if cnt==1: \n",
        " encoder = Model(input_layer, encoded3)\n",
        " dat_resaXautotr = encoder.predict(dat_resaXtr)\n",
        " dat_resaXautots = encoder.predict(dat_resaXts)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUugRh4Xv4_D"
      },
      "source": [
        "### dataset1+normal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uypysCj3wELK"
      },
      "source": [
        "#### train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYHkmDQewELL"
      },
      "source": [
        "if cnt==2: \n",
        " dat_resaXtr, dat_resaXts, dat_resaYtr, dat_resaYts = train_test_split(dat_resa.drop(['Disease'],axis=1),dat_resa['Disease'], \n",
        "                                                    train_size=0.75, \n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=111,\n",
        "                                                    shuffle=True, \n",
        "                                                    stratify=dat_resa['Disease'])\n",
        " scaler=MinMaxScaler()\n",
        " dat_resaXtr=scaler.fit(dat_resaXtr).transform(dat_resaXtr)\n",
        " dat_resaXts=scaler.fit(dat_resaXts).transform(dat_resaXts)\n",
        " over_sampler = SMOTE(k_neighbors=10, sampling_strategy=\"auto\")\n",
        " dat_resaXtr, dat_resaYtr = over_sampler.fit_resample(dat_resaXtr,dat_resaYtr)\n",
        " dat_resaXts, dat_resaYts = over_sampler.fit_resample(dat_resaXts,dat_resaYts)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY4egth3wELM"
      },
      "source": [
        "##### selectkbest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es6THTT2wELM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68a4aa6-ba2e-49e6-e88f-59f6ba2940d6"
      },
      "source": [
        "if cnt==2: \n",
        " selector = SelectKBest(score_func=f_classif, k=60)\n",
        " dat_resaXredtr=selector.fit(dat_resaXtr,dat_resaYtr).transform(dat_resaXtr)\n",
        " dat_resaXredts=selector.fit(dat_resaXtr,dat_resaYtr).transform(dat_resaXts)\n",
        " dat_resaXredts\n",
        " selector.pvalues_\n",
        " selector.scores_"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.07267598, 0.77228916, 0.70724401, ..., 0.0460482 , 0.29207582,\n",
              "        0.16078714],\n",
              "       [0.01665771, 0.49240964, 0.37908497, ..., 0.07917059, 0.72870518,\n",
              "        0.44756419],\n",
              "       [0.03815153, 0.27879518, 0.34300109, ..., 0.11835196, 0.92920758,\n",
              "        0.700024  ],\n",
              "       ...,\n",
              "       [0.04652385, 0.15557028, 0.18741078, ..., 0.04895648, 0.3015848 ,\n",
              "        0.10384757],\n",
              "       [0.09994088, 0.11667107, 0.12217627, ..., 0.04638883, 0.35752095,\n",
              "        0.10314747],\n",
              "       [0.0489423 , 0.04475157, 0.11348838, ..., 0.04712783, 0.089795  ,\n",
              "        0.10130859]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n55V7LeqwELN"
      },
      "source": [
        "##### autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytAqUSW-wELN"
      },
      "source": [
        "if cnt==2: \n",
        " input_dim = 53999\n",
        " latent_dim1 =5000\n",
        " latent_dim2=500\n",
        " latent_dim3=60\n",
        " input_layer = Input(shape=(input_dim,), name='input')\n",
        "\n",
        " encoded1 = Dense(latent_dim1, \n",
        "                activation='relu', name='features1')(input_layer)\n",
        " encoded2 = Dense(latent_dim2, \n",
        "                activation='relu', name='features2')(encoded1)\n",
        " encoded3 = Dense(latent_dim3, \n",
        "                activation='relu', name='features3')(encoded2)\n",
        "\n",
        " decoded1 = Dense(latent_dim2, activation='relu', name='reconstructed1')(encoded3)\n",
        " decoded2 = Dense(latent_dim1, activation='relu', name='reconstructed2')(decoded1)\n",
        " decoded3 = Dense(input_dim, activation='sigmoid', name='reconstructed3')(decoded2)\n",
        "\n",
        " autoencoder = Model(inputs=[input_layer], outputs=[decoded3])\n",
        "\n",
        " autoencoder.compile(optimizer='adam', \n",
        "                    loss='binary_crossentropy')\n",
        " autoencoder.fit(dat_resaXtr,dat_resaXtr ,\n",
        "                epochs=1000,\n",
        "                batch_size=3000,\n",
        "                shuffle=True,\n",
        "                validation_split=0.2)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnV_8Sf-wELN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e023bf1-adcb-4838-bc88-1448e17c49b0"
      },
      "source": [
        "#autoencoder.fit(dat_resaXtr,dat_resaXtr ,\n",
        "#                epochs=1000,\n",
        "#                batch_size=3000,\n",
        "#                shuffle=True,\n",
        "#                validation_split=0.2)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "2/2 [==============================] - 1s 723ms/step - loss: 0.6925 - val_loss: 0.6369\n",
            "Epoch 2/1000\n",
            "2/2 [==============================] - 2s 854ms/step - loss: 0.6411 - val_loss: 0.7423\n",
            "Epoch 3/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.7158 - val_loss: 0.6088\n",
            "Epoch 4/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.6215 - val_loss: 0.5881\n",
            "Epoch 5/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5934 - val_loss: 0.6005\n",
            "Epoch 6/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5994 - val_loss: 0.5919\n",
            "Epoch 7/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5941 - val_loss: 0.5875\n",
            "Epoch 8/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5892 - val_loss: 0.5889\n",
            "Epoch 9/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5908 - val_loss: 0.5875\n",
            "Epoch 10/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5890 - val_loss: 0.5870\n",
            "Epoch 11/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5890 - val_loss: 0.5866\n",
            "Epoch 12/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5882 - val_loss: 0.5863\n",
            "Epoch 13/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5883 - val_loss: 0.5860\n",
            "Epoch 14/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5878 - val_loss: 0.5861\n",
            "Epoch 15/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5879 - val_loss: 0.5858\n",
            "Epoch 16/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5877 - val_loss: 0.5859\n",
            "Epoch 17/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5877 - val_loss: 0.5857\n",
            "Epoch 18/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5875 - val_loss: 0.5856\n",
            "Epoch 19/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5876 - val_loss: 0.5856\n",
            "Epoch 20/1000\n",
            "2/2 [==============================] - 2s 826ms/step - loss: 0.5875 - val_loss: 0.5856\n",
            "Epoch 21/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5875 - val_loss: 0.5858\n",
            "Epoch 22/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5875 - val_loss: 0.5855\n",
            "Epoch 23/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5874 - val_loss: 0.5856\n",
            "Epoch 24/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5875 - val_loss: 0.5855\n",
            "Epoch 25/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5873 - val_loss: 0.5855\n",
            "Epoch 26/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5874 - val_loss: 0.5855\n",
            "Epoch 27/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5873 - val_loss: 0.5854\n",
            "Epoch 28/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5872 - val_loss: 0.5853\n",
            "Epoch 29/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5872 - val_loss: 0.5853\n",
            "Epoch 30/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5872 - val_loss: 0.5853\n",
            "Epoch 31/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5872 - val_loss: 0.5853\n",
            "Epoch 32/1000\n",
            "2/2 [==============================] - 2s 828ms/step - loss: 0.5871 - val_loss: 0.5850\n",
            "Epoch 33/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5866 - val_loss: 0.5845\n",
            "Epoch 34/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5861 - val_loss: 0.5841\n",
            "Epoch 35/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5855 - val_loss: 0.5831\n",
            "Epoch 36/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5842 - val_loss: 0.5820\n",
            "Epoch 37/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5824 - val_loss: 0.5813\n",
            "Epoch 38/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5813 - val_loss: 0.5816\n",
            "Epoch 39/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5812 - val_loss: 0.5831\n",
            "Epoch 40/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5818 - val_loss: 0.5833\n",
            "Epoch 41/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5818 - val_loss: 0.5812\n",
            "Epoch 42/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5807 - val_loss: 0.5819\n",
            "Epoch 43/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5810 - val_loss: 0.5816\n",
            "Epoch 44/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5811 - val_loss: 0.5811\n",
            "Epoch 45/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5806 - val_loss: 0.5809\n",
            "Epoch 46/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5804 - val_loss: 0.5808\n",
            "Epoch 47/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5803 - val_loss: 0.5808\n",
            "Epoch 48/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5802 - val_loss: 0.5804\n",
            "Epoch 49/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5801 - val_loss: 0.5802\n",
            "Epoch 50/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5798 - val_loss: 0.5797\n",
            "Epoch 51/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5796 - val_loss: 0.5795\n",
            "Epoch 52/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5794 - val_loss: 0.5792\n",
            "Epoch 53/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.5792 - val_loss: 0.5786\n",
            "Epoch 54/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5789 - val_loss: 0.5785\n",
            "Epoch 55/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5788 - val_loss: 0.5782\n",
            "Epoch 56/1000\n",
            "2/2 [==============================] - 2s 853ms/step - loss: 0.5786 - val_loss: 0.5784\n",
            "Epoch 57/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5786 - val_loss: 0.5782\n",
            "Epoch 58/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5784 - val_loss: 0.5782\n",
            "Epoch 59/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5784 - val_loss: 0.5782\n",
            "Epoch 60/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5782 - val_loss: 0.5782\n",
            "Epoch 61/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5780 - val_loss: 0.5783\n",
            "Epoch 62/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5779 - val_loss: 0.5781\n",
            "Epoch 63/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5777 - val_loss: 0.5780\n",
            "Epoch 64/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5776 - val_loss: 0.5781\n",
            "Epoch 65/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5777 - val_loss: 0.5780\n",
            "Epoch 66/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5773 - val_loss: 0.5774\n",
            "Epoch 67/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5763 - val_loss: 0.5771\n",
            "Epoch 68/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5756 - val_loss: 0.5766\n",
            "Epoch 69/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5749 - val_loss: 0.5781\n",
            "Epoch 70/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5780 - val_loss: 0.5784\n",
            "Epoch 71/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5769 - val_loss: 0.5779\n",
            "Epoch 72/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5762 - val_loss: 0.5765\n",
            "Epoch 73/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5751 - val_loss: 0.5768\n",
            "Epoch 74/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5749 - val_loss: 0.5763\n",
            "Epoch 75/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5747 - val_loss: 0.5754\n",
            "Epoch 76/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5738 - val_loss: 0.5755\n",
            "Epoch 77/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5735 - val_loss: 0.5754\n",
            "Epoch 78/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5734 - val_loss: 0.5749\n",
            "Epoch 79/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5730 - val_loss: 0.5743\n",
            "Epoch 80/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5727 - val_loss: 0.5743\n",
            "Epoch 81/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5725 - val_loss: 0.5740\n",
            "Epoch 82/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5723 - val_loss: 0.5735\n",
            "Epoch 83/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5721 - val_loss: 0.5735\n",
            "Epoch 84/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5718 - val_loss: 0.5731\n",
            "Epoch 85/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5716 - val_loss: 0.5724\n",
            "Epoch 86/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5711 - val_loss: 0.5719\n",
            "Epoch 87/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5707 - val_loss: 0.5716\n",
            "Epoch 88/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5704 - val_loss: 0.5710\n",
            "Epoch 89/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5698 - val_loss: 0.5708\n",
            "Epoch 90/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5701 - val_loss: 0.5752\n",
            "Epoch 91/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5733 - val_loss: 0.5760\n",
            "Epoch 92/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5737 - val_loss: 0.5737\n",
            "Epoch 93/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5730 - val_loss: 0.5714\n",
            "Epoch 94/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5706 - val_loss: 0.5719\n",
            "Epoch 95/1000\n",
            "2/2 [==============================] - 2s 817ms/step - loss: 0.5709 - val_loss: 0.5701\n",
            "Epoch 96/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5699 - val_loss: 0.5701\n",
            "Epoch 97/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5695 - val_loss: 0.5700\n",
            "Epoch 98/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5693 - val_loss: 0.5695\n",
            "Epoch 99/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5688 - val_loss: 0.5688\n",
            "Epoch 100/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5684 - val_loss: 0.5684\n",
            "Epoch 101/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5682 - val_loss: 0.5682\n",
            "Epoch 102/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5677 - val_loss: 0.5681\n",
            "Epoch 103/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5677 - val_loss: 0.5678\n",
            "Epoch 104/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5674 - val_loss: 0.5675\n",
            "Epoch 105/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5673 - val_loss: 0.5672\n",
            "Epoch 106/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5671 - val_loss: 0.5671\n",
            "Epoch 107/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5669 - val_loss: 0.5668\n",
            "Epoch 108/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5667 - val_loss: 0.5666\n",
            "Epoch 109/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5666 - val_loss: 0.5665\n",
            "Epoch 110/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5665 - val_loss: 0.5664\n",
            "Epoch 111/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5664 - val_loss: 0.5663\n",
            "Epoch 112/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5664 - val_loss: 0.5662\n",
            "Epoch 113/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5662 - val_loss: 0.5659\n",
            "Epoch 114/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5659 - val_loss: 0.5658\n",
            "Epoch 115/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5659 - val_loss: 0.5657\n",
            "Epoch 116/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5658 - val_loss: 0.5654\n",
            "Epoch 117/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5656 - val_loss: 0.5653\n",
            "Epoch 118/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5655 - val_loss: 0.5652\n",
            "Epoch 119/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5654 - val_loss: 0.5650\n",
            "Epoch 120/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5652 - val_loss: 0.5650\n",
            "Epoch 121/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5653 - val_loss: 0.5650\n",
            "Epoch 122/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5653 - val_loss: 0.5647\n",
            "Epoch 123/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5650 - val_loss: 0.5646\n",
            "Epoch 124/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5650 - val_loss: 0.5644\n",
            "Epoch 125/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.5648 - val_loss: 0.5644\n",
            "Epoch 126/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5648 - val_loss: 0.5646\n",
            "Epoch 127/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5651 - val_loss: 0.5645\n",
            "Epoch 128/1000\n",
            "2/2 [==============================] - 2s 853ms/step - loss: 0.5649 - val_loss: 0.5641\n",
            "Epoch 129/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5646 - val_loss: 0.5641\n",
            "Epoch 130/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5645 - val_loss: 0.5639\n",
            "Epoch 131/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5644 - val_loss: 0.5638\n",
            "Epoch 132/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5642 - val_loss: 0.5638\n",
            "Epoch 133/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5642 - val_loss: 0.5637\n",
            "Epoch 134/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5641 - val_loss: 0.5635\n",
            "Epoch 135/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5640 - val_loss: 0.5635\n",
            "Epoch 136/1000\n",
            "2/2 [==============================] - 2s 855ms/step - loss: 0.5639 - val_loss: 0.5633\n",
            "Epoch 137/1000\n",
            "2/2 [==============================] - 2s 853ms/step - loss: 0.5637 - val_loss: 0.5632\n",
            "Epoch 138/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5636 - val_loss: 0.5631\n",
            "Epoch 139/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5635 - val_loss: 0.5630\n",
            "Epoch 140/1000\n",
            "2/2 [==============================] - 2s 816ms/step - loss: 0.5634 - val_loss: 0.5630\n",
            "Epoch 141/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5634 - val_loss: 0.5630\n",
            "Epoch 142/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5636 - val_loss: 0.5644\n",
            "Epoch 143/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5654 - val_loss: 0.5636\n",
            "Epoch 144/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5641 - val_loss: 0.5637\n",
            "Epoch 145/1000\n",
            "2/2 [==============================] - 2s 854ms/step - loss: 0.5643 - val_loss: 0.5634\n",
            "Epoch 146/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5638 - val_loss: 0.5630\n",
            "Epoch 147/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5635 - val_loss: 0.5630\n",
            "Epoch 148/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5634 - val_loss: 0.5632\n",
            "Epoch 149/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5634 - val_loss: 0.5629\n",
            "Epoch 150/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5633 - val_loss: 0.5627\n",
            "Epoch 151/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5631 - val_loss: 0.5626\n",
            "Epoch 152/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5630 - val_loss: 0.5626\n",
            "Epoch 153/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5629 - val_loss: 0.5625\n",
            "Epoch 154/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5628 - val_loss: 0.5624\n",
            "Epoch 155/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5626 - val_loss: 0.5623\n",
            "Epoch 156/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5625 - val_loss: 0.5622\n",
            "Epoch 157/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5624 - val_loss: 0.5622\n",
            "Epoch 158/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5623 - val_loss: 0.5621\n",
            "Epoch 159/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5622 - val_loss: 0.5623\n",
            "Epoch 160/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5626 - val_loss: 0.5633\n",
            "Epoch 161/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5635 - val_loss: 0.5622\n",
            "Epoch 162/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5624 - val_loss: 0.5625\n",
            "Epoch 163/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5623 - val_loss: 0.5622\n",
            "Epoch 164/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5622 - val_loss: 0.5619\n",
            "Epoch 165/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5619 - val_loss: 0.5617\n",
            "Epoch 166/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5616 - val_loss: 0.5619\n",
            "Epoch 167/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5616 - val_loss: 0.5617\n",
            "Epoch 168/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5616 - val_loss: 0.5616\n",
            "Epoch 169/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5614 - val_loss: 0.5617\n",
            "Epoch 170/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5615 - val_loss: 0.5619\n",
            "Epoch 171/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5617 - val_loss: 0.5618\n",
            "Epoch 172/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5616 - val_loss: 0.5615\n",
            "Epoch 173/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5611 - val_loss: 0.5615\n",
            "Epoch 174/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5612 - val_loss: 0.5613\n",
            "Epoch 175/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5609 - val_loss: 0.5613\n",
            "Epoch 176/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5609 - val_loss: 0.5614\n",
            "Epoch 177/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5609 - val_loss: 0.5612\n",
            "Epoch 178/1000\n",
            "2/2 [==============================] - 2s 854ms/step - loss: 0.5607 - val_loss: 0.5613\n",
            "Epoch 179/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5608 - val_loss: 0.5614\n",
            "Epoch 180/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5608 - val_loss: 0.5612\n",
            "Epoch 181/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5605 - val_loss: 0.5610\n",
            "Epoch 182/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5604 - val_loss: 0.5610\n",
            "Epoch 183/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5604 - val_loss: 0.5610\n",
            "Epoch 184/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5603 - val_loss: 0.5609\n",
            "Epoch 185/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5602 - val_loss: 0.5610\n",
            "Epoch 186/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5603 - val_loss: 0.5610\n",
            "Epoch 187/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5602 - val_loss: 0.5609\n",
            "Epoch 188/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5603 - val_loss: 0.5614\n",
            "Epoch 189/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5611 - val_loss: 0.5616\n",
            "Epoch 190/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5607 - val_loss: 0.5612\n",
            "Epoch 191/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5605 - val_loss: 0.5613\n",
            "Epoch 192/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5606 - val_loss: 0.5611\n",
            "Epoch 193/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5604 - val_loss: 0.5608\n",
            "Epoch 194/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5600 - val_loss: 0.5611\n",
            "Epoch 195/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5601 - val_loss: 0.5609\n",
            "Epoch 196/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5599 - val_loss: 0.5606\n",
            "Epoch 197/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5597 - val_loss: 0.5605\n",
            "Epoch 198/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5596 - val_loss: 0.5606\n",
            "Epoch 199/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5596 - val_loss: 0.5606\n",
            "Epoch 200/1000\n",
            "2/2 [==============================] - 2s 816ms/step - loss: 0.5595 - val_loss: 0.5605\n",
            "Epoch 201/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5594 - val_loss: 0.5604\n",
            "Epoch 202/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5594 - val_loss: 0.5604\n",
            "Epoch 203/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5593 - val_loss: 0.5604\n",
            "Epoch 204/1000\n",
            "2/2 [==============================] - 2s 834ms/step - loss: 0.5593 - val_loss: 0.5605\n",
            "Epoch 205/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5594 - val_loss: 0.5605\n",
            "Epoch 206/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5594 - val_loss: 0.5605\n",
            "Epoch 207/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5594 - val_loss: 0.5607\n",
            "Epoch 208/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5595 - val_loss: 0.5604\n",
            "Epoch 209/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5592 - val_loss: 0.5607\n",
            "Epoch 210/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5596 - val_loss: 0.5607\n",
            "Epoch 211/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5595 - val_loss: 0.5608\n",
            "Epoch 212/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5596 - val_loss: 0.5605\n",
            "Epoch 213/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5593 - val_loss: 0.5602\n",
            "Epoch 214/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5590 - val_loss: 0.5604\n",
            "Epoch 215/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5591 - val_loss: 0.5601\n",
            "Epoch 216/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5589 - val_loss: 0.5601\n",
            "Epoch 217/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5589 - val_loss: 0.5600\n",
            "Epoch 218/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5587 - val_loss: 0.5601\n",
            "Epoch 219/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5587 - val_loss: 0.5600\n",
            "Epoch 220/1000\n",
            "2/2 [==============================] - 2s 822ms/step - loss: 0.5587 - val_loss: 0.5599\n",
            "Epoch 221/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5585 - val_loss: 0.5599\n",
            "Epoch 222/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5585 - val_loss: 0.5599\n",
            "Epoch 223/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5585 - val_loss: 0.5599\n",
            "Epoch 224/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5585 - val_loss: 0.5600\n",
            "Epoch 225/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5586 - val_loss: 0.5603\n",
            "Epoch 226/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5589 - val_loss: 0.5607\n",
            "Epoch 227/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5593 - val_loss: 0.5603\n",
            "Epoch 228/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5590 - val_loss: 0.5606\n",
            "Epoch 229/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5592 - val_loss: 0.5604\n",
            "Epoch 230/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5591 - val_loss: 0.5603\n",
            "Epoch 231/1000\n",
            "2/2 [==============================] - 2s 835ms/step - loss: 0.5587 - val_loss: 0.5600\n",
            "Epoch 232/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5585 - val_loss: 0.5600\n",
            "Epoch 233/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5585 - val_loss: 0.5600\n",
            "Epoch 234/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5584 - val_loss: 0.5598\n",
            "Epoch 235/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5583 - val_loss: 0.5597\n",
            "Epoch 236/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5582 - val_loss: 0.5596\n",
            "Epoch 237/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5580 - val_loss: 0.5596\n",
            "Epoch 238/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5581 - val_loss: 0.5597\n",
            "Epoch 239/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5581 - val_loss: 0.5596\n",
            "Epoch 240/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5580 - val_loss: 0.5595\n",
            "Epoch 241/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5579 - val_loss: 0.5595\n",
            "Epoch 242/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5578 - val_loss: 0.5595\n",
            "Epoch 243/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5578 - val_loss: 0.5595\n",
            "Epoch 244/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5579 - val_loss: 0.5599\n",
            "Epoch 245/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5583 - val_loss: 0.5601\n",
            "Epoch 246/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5583 - val_loss: 0.5594\n",
            "Epoch 247/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5578 - val_loss: 0.5595\n",
            "Epoch 248/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5578 - val_loss: 0.5596\n",
            "Epoch 249/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5578 - val_loss: 0.5594\n",
            "Epoch 250/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5577 - val_loss: 0.5594\n",
            "Epoch 251/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5577 - val_loss: 0.5593\n",
            "Epoch 252/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5576 - val_loss: 0.5593\n",
            "Epoch 253/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5575 - val_loss: 0.5593\n",
            "Epoch 254/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5575 - val_loss: 0.5593\n",
            "Epoch 255/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5574 - val_loss: 0.5593\n",
            "Epoch 256/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5574 - val_loss: 0.5593\n",
            "Epoch 257/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5575 - val_loss: 0.5596\n",
            "Epoch 258/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5580 - val_loss: 0.5600\n",
            "Epoch 259/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5583 - val_loss: 0.5595\n",
            "Epoch 260/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5578 - val_loss: 0.5600\n",
            "Epoch 261/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5581 - val_loss: 0.5595\n",
            "Epoch 262/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.5577 - val_loss: 0.5593\n",
            "Epoch 263/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5575 - val_loss: 0.5594\n",
            "Epoch 264/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5575 - val_loss: 0.5592\n",
            "Epoch 265/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5573 - val_loss: 0.5591\n",
            "Epoch 266/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5572 - val_loss: 0.5591\n",
            "Epoch 267/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5572 - val_loss: 0.5592\n",
            "Epoch 268/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5572 - val_loss: 0.5591\n",
            "Epoch 269/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5572 - val_loss: 0.5590\n",
            "Epoch 270/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5570 - val_loss: 0.5590\n",
            "Epoch 271/1000\n",
            "2/2 [==============================] - 2s 824ms/step - loss: 0.5570 - val_loss: 0.5591\n",
            "Epoch 272/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5571 - val_loss: 0.5591\n",
            "Epoch 273/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5572 - val_loss: 0.5593\n",
            "Epoch 274/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5573 - val_loss: 0.5596\n",
            "Epoch 275/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5575 - val_loss: 0.5592\n",
            "Epoch 276/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5572 - val_loss: 0.5593\n",
            "Epoch 277/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5572 - val_loss: 0.5590\n",
            "Epoch 278/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5570 - val_loss: 0.5591\n",
            "Epoch 279/1000\n",
            "2/2 [==============================] - 2s 817ms/step - loss: 0.5571 - val_loss: 0.5589\n",
            "Epoch 280/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5569 - val_loss: 0.5590\n",
            "Epoch 281/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5569 - val_loss: 0.5589\n",
            "Epoch 282/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5568 - val_loss: 0.5589\n",
            "Epoch 283/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5568 - val_loss: 0.5590\n",
            "Epoch 284/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5569 - val_loss: 0.5589\n",
            "Epoch 285/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5568 - val_loss: 0.5591\n",
            "Epoch 286/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5570 - val_loss: 0.5591\n",
            "Epoch 287/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5570 - val_loss: 0.5589\n",
            "Epoch 288/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5567 - val_loss: 0.5589\n",
            "Epoch 289/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5567 - val_loss: 0.5588\n",
            "Epoch 290/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5566 - val_loss: 0.5589\n",
            "Epoch 291/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5568 - val_loss: 0.5589\n",
            "Epoch 292/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5568 - val_loss: 0.5591\n",
            "Epoch 293/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5569 - val_loss: 0.5593\n",
            "Epoch 294/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5571 - val_loss: 0.5590\n",
            "Epoch 295/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5568 - val_loss: 0.5591\n",
            "Epoch 296/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5569 - val_loss: 0.5588\n",
            "Epoch 297/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5566 - val_loss: 0.5589\n",
            "Epoch 298/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5567 - val_loss: 0.5588\n",
            "Epoch 299/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5566 - val_loss: 0.5587\n",
            "Epoch 300/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5564 - val_loss: 0.5587\n",
            "Epoch 301/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5564 - val_loss: 0.5586\n",
            "Epoch 302/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5564 - val_loss: 0.5586\n",
            "Epoch 303/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5564 - val_loss: 0.5588\n",
            "Epoch 304/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5565 - val_loss: 0.5591\n",
            "Epoch 305/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5569 - val_loss: 0.5594\n",
            "Epoch 306/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5569 - val_loss: 0.5588\n",
            "Epoch 307/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5566 - val_loss: 0.5590\n",
            "Epoch 308/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5567 - val_loss: 0.5588\n",
            "Epoch 309/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5565 - val_loss: 0.5586\n",
            "Epoch 310/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5563 - val_loss: 0.5586\n",
            "Epoch 311/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5563 - val_loss: 0.5586\n",
            "Epoch 312/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5563 - val_loss: 0.5585\n",
            "Epoch 313/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5562 - val_loss: 0.5588\n",
            "Epoch 314/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5564 - val_loss: 0.5590\n",
            "Epoch 315/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5565 - val_loss: 0.5588\n",
            "Epoch 316/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5564 - val_loss: 0.5589\n",
            "Epoch 317/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5564 - val_loss: 0.5585\n",
            "Epoch 318/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5561 - val_loss: 0.5588\n",
            "Epoch 319/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5563 - val_loss: 0.5587\n",
            "Epoch 320/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5564 - val_loss: 0.5593\n",
            "Epoch 321/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5569 - val_loss: 0.5589\n",
            "Epoch 322/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5563 - val_loss: 0.5587\n",
            "Epoch 323/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5563 - val_loss: 0.5585\n",
            "Epoch 324/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5561 - val_loss: 0.5585\n",
            "Epoch 325/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5560 - val_loss: 0.5585\n",
            "Epoch 326/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5561 - val_loss: 0.5584\n",
            "Epoch 327/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5559 - val_loss: 0.5585\n",
            "Epoch 328/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5559 - val_loss: 0.5584\n",
            "Epoch 329/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5559 - val_loss: 0.5584\n",
            "Epoch 330/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5559 - val_loss: 0.5584\n",
            "Epoch 331/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5559 - val_loss: 0.5584\n",
            "Epoch 332/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5559 - val_loss: 0.5585\n",
            "Epoch 333/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5560 - val_loss: 0.5588\n",
            "Epoch 334/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5563 - val_loss: 0.5586\n",
            "Epoch 335/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5561 - val_loss: 0.5584\n",
            "Epoch 336/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5558 - val_loss: 0.5587\n",
            "Epoch 337/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5561 - val_loss: 0.5585\n",
            "Epoch 338/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5560 - val_loss: 0.5588\n",
            "Epoch 339/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5562 - val_loss: 0.5587\n",
            "Epoch 340/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5561 - val_loss: 0.5586\n",
            "Epoch 341/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5560 - val_loss: 0.5583\n",
            "Epoch 342/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5557 - val_loss: 0.5585\n",
            "Epoch 343/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5558 - val_loss: 0.5582\n",
            "Epoch 344/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5556 - val_loss: 0.5583\n",
            "Epoch 345/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5557 - val_loss: 0.5584\n",
            "Epoch 346/1000\n",
            "2/2 [==============================] - 2s 828ms/step - loss: 0.5557 - val_loss: 0.5582\n",
            "Epoch 347/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5556 - val_loss: 0.5582\n",
            "Epoch 348/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5555 - val_loss: 0.5583\n",
            "Epoch 349/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5556 - val_loss: 0.5582\n",
            "Epoch 350/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5555 - val_loss: 0.5582\n",
            "Epoch 351/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5555 - val_loss: 0.5582\n",
            "Epoch 352/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5555 - val_loss: 0.5583\n",
            "Epoch 353/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5557 - val_loss: 0.5587\n",
            "Epoch 354/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5561 - val_loss: 0.5590\n",
            "Epoch 355/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5564 - val_loss: 0.5590\n",
            "Epoch 356/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5564 - val_loss: 0.5588\n",
            "Epoch 357/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5560 - val_loss: 0.5588\n",
            "Epoch 358/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5560 - val_loss: 0.5583\n",
            "Epoch 359/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5557 - val_loss: 0.5583\n",
            "Epoch 360/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5556 - val_loss: 0.5584\n",
            "Epoch 361/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5556 - val_loss: 0.5582\n",
            "Epoch 362/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5554 - val_loss: 0.5581\n",
            "Epoch 363/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5554 - val_loss: 0.5583\n",
            "Epoch 364/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5554 - val_loss: 0.5582\n",
            "Epoch 365/1000\n",
            "2/2 [==============================] - 2s 820ms/step - loss: 0.5554 - val_loss: 0.5582\n",
            "Epoch 366/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5554 - val_loss: 0.5583\n",
            "Epoch 367/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5554 - val_loss: 0.5581\n",
            "Epoch 368/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5553 - val_loss: 0.5581\n",
            "Epoch 369/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5553 - val_loss: 0.5583\n",
            "Epoch 370/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5555 - val_loss: 0.5586\n",
            "Epoch 371/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5557 - val_loss: 0.5583\n",
            "Epoch 372/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5555 - val_loss: 0.5585\n",
            "Epoch 373/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5557 - val_loss: 0.5585\n",
            "Epoch 374/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5557 - val_loss: 0.5586\n",
            "Epoch 375/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5557 - val_loss: 0.5580\n",
            "Epoch 376/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5552 - val_loss: 0.5582\n",
            "Epoch 377/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5553 - val_loss: 0.5583\n",
            "Epoch 378/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5553 - val_loss: 0.5579\n",
            "Epoch 379/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5551 - val_loss: 0.5581\n",
            "Epoch 380/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5552 - val_loss: 0.5580\n",
            "Epoch 381/1000\n",
            "2/2 [==============================] - 2s 819ms/step - loss: 0.5551 - val_loss: 0.5579\n",
            "Epoch 382/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5550 - val_loss: 0.5579\n",
            "Epoch 383/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5550 - val_loss: 0.5579\n",
            "Epoch 384/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5549 - val_loss: 0.5580\n",
            "Epoch 385/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5550 - val_loss: 0.5579\n",
            "Epoch 386/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5550 - val_loss: 0.5582\n",
            "Epoch 387/1000\n",
            "2/2 [==============================] - 2s 854ms/step - loss: 0.5552 - val_loss: 0.5584\n",
            "Epoch 388/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5554 - val_loss: 0.5580\n",
            "Epoch 389/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5550 - val_loss: 0.5581\n",
            "Epoch 390/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5552 - val_loss: 0.5581\n",
            "Epoch 391/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5552 - val_loss: 0.5581\n",
            "Epoch 392/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5551 - val_loss: 0.5580\n",
            "Epoch 393/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5551 - val_loss: 0.5584\n",
            "Epoch 394/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5553 - val_loss: 0.5581\n",
            "Epoch 395/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5551 - val_loss: 0.5580\n",
            "Epoch 396/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5549 - val_loss: 0.5579\n",
            "Epoch 397/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5549 - val_loss: 0.5579\n",
            "Epoch 398/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5548 - val_loss: 0.5578\n",
            "Epoch 399/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5547 - val_loss: 0.5579\n",
            "Epoch 400/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5548 - val_loss: 0.5578\n",
            "Epoch 401/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5547 - val_loss: 0.5577\n",
            "Epoch 402/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5546 - val_loss: 0.5577\n",
            "Epoch 403/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5546 - val_loss: 0.5577\n",
            "Epoch 404/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5546 - val_loss: 0.5577\n",
            "Epoch 405/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5546 - val_loss: 0.5578\n",
            "Epoch 406/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5547 - val_loss: 0.5581\n",
            "Epoch 407/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5552 - val_loss: 0.5588\n",
            "Epoch 408/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5558 - val_loss: 0.5586\n",
            "Epoch 409/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5556 - val_loss: 0.5590\n",
            "Epoch 410/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5560 - val_loss: 0.5586\n",
            "Epoch 411/1000\n",
            "2/2 [==============================] - 2s 819ms/step - loss: 0.5555 - val_loss: 0.5580\n",
            "Epoch 412/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5550 - val_loss: 0.5583\n",
            "Epoch 413/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5551 - val_loss: 0.5582\n",
            "Epoch 414/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5550 - val_loss: 0.5580\n",
            "Epoch 415/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5549 - val_loss: 0.5579\n",
            "Epoch 416/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5547 - val_loss: 0.5577\n",
            "Epoch 417/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5546 - val_loss: 0.5577\n",
            "Epoch 418/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5546 - val_loss: 0.5578\n",
            "Epoch 419/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5546 - val_loss: 0.5578\n",
            "Epoch 420/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5546 - val_loss: 0.5578\n",
            "Epoch 421/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5546 - val_loss: 0.5577\n",
            "Epoch 422/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5545 - val_loss: 0.5576\n",
            "Epoch 423/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5544 - val_loss: 0.5577\n",
            "Epoch 424/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5544 - val_loss: 0.5578\n",
            "Epoch 425/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5546 - val_loss: 0.5579\n",
            "Epoch 426/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5546 - val_loss: 0.5576\n",
            "Epoch 427/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5544 - val_loss: 0.5578\n",
            "Epoch 428/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5546 - val_loss: 0.5579\n",
            "Epoch 429/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5546 - val_loss: 0.5577\n",
            "Epoch 430/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5545 - val_loss: 0.5580\n",
            "Epoch 431/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5547 - val_loss: 0.5578\n",
            "Epoch 432/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5545 - val_loss: 0.5578\n",
            "Epoch 433/1000\n",
            "2/2 [==============================] - 2s 826ms/step - loss: 0.5545 - val_loss: 0.5576\n",
            "Epoch 434/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5543 - val_loss: 0.5576\n",
            "Epoch 435/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5543 - val_loss: 0.5576\n",
            "Epoch 436/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5543 - val_loss: 0.5575\n",
            "Epoch 437/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5542 - val_loss: 0.5575\n",
            "Epoch 438/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5542 - val_loss: 0.5576\n",
            "Epoch 439/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5543 - val_loss: 0.5575\n",
            "Epoch 440/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5542 - val_loss: 0.5575\n",
            "Epoch 441/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5541 - val_loss: 0.5575\n",
            "Epoch 442/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5542 - val_loss: 0.5576\n",
            "Epoch 443/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5543 - val_loss: 0.5578\n",
            "Epoch 444/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5544 - val_loss: 0.5576\n",
            "Epoch 445/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5542 - val_loss: 0.5579\n",
            "Epoch 446/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5547 - val_loss: 0.5589\n",
            "Epoch 447/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5554 - val_loss: 0.5579\n",
            "Epoch 448/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5546 - val_loss: 0.5582\n",
            "Epoch 449/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5549 - val_loss: 0.5581\n",
            "Epoch 450/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5548 - val_loss: 0.5578\n",
            "Epoch 451/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5545 - val_loss: 0.5576\n",
            "Epoch 452/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5543 - val_loss: 0.5578\n",
            "Epoch 453/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5543 - val_loss: 0.5576\n",
            "Epoch 454/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5543 - val_loss: 0.5575\n",
            "Epoch 455/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5542 - val_loss: 0.5575\n",
            "Epoch 456/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5541 - val_loss: 0.5575\n",
            "Epoch 457/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5541 - val_loss: 0.5576\n",
            "Epoch 458/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5541 - val_loss: 0.5574\n",
            "Epoch 459/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5540 - val_loss: 0.5574\n",
            "Epoch 460/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5540 - val_loss: 0.5574\n",
            "Epoch 461/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5540 - val_loss: 0.5574\n",
            "Epoch 462/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5540 - val_loss: 0.5575\n",
            "Epoch 463/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5541 - val_loss: 0.5577\n",
            "Epoch 464/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5543 - val_loss: 0.5576\n",
            "Epoch 465/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5541 - val_loss: 0.5574\n",
            "Epoch 466/1000\n",
            "2/2 [==============================] - 2s 858ms/step - loss: 0.5540 - val_loss: 0.5576\n",
            "Epoch 467/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5542 - val_loss: 0.5574\n",
            "Epoch 468/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5539 - val_loss: 0.5575\n",
            "Epoch 469/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5541 - val_loss: 0.5574\n",
            "Epoch 470/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5539 - val_loss: 0.5574\n",
            "Epoch 471/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5539 - val_loss: 0.5574\n",
            "Epoch 472/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5539 - val_loss: 0.5574\n",
            "Epoch 473/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5540 - val_loss: 0.5577\n",
            "Epoch 474/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5541 - val_loss: 0.5578\n",
            "Epoch 475/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5543 - val_loss: 0.5576\n",
            "Epoch 476/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5541 - val_loss: 0.5575\n",
            "Epoch 477/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5540 - val_loss: 0.5575\n",
            "Epoch 478/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5539 - val_loss: 0.5575\n",
            "Epoch 479/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5540 - val_loss: 0.5574\n",
            "Epoch 480/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5539 - val_loss: 0.5574\n",
            "Epoch 481/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5539 - val_loss: 0.5575\n",
            "Epoch 482/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5539 - val_loss: 0.5573\n",
            "Epoch 483/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5538 - val_loss: 0.5572\n",
            "Epoch 484/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.5537 - val_loss: 0.5574\n",
            "Epoch 485/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5538 - val_loss: 0.5574\n",
            "Epoch 486/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5538 - val_loss: 0.5574\n",
            "Epoch 487/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5538 - val_loss: 0.5575\n",
            "Epoch 488/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5538 - val_loss: 0.5573\n",
            "Epoch 489/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5537 - val_loss: 0.5572\n",
            "Epoch 490/1000\n",
            "2/2 [==============================] - 2s 835ms/step - loss: 0.5537 - val_loss: 0.5574\n",
            "Epoch 491/1000\n",
            "2/2 [==============================] - 2s 824ms/step - loss: 0.5538 - val_loss: 0.5572\n",
            "Epoch 492/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5536 - val_loss: 0.5572\n",
            "Epoch 493/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5536 - val_loss: 0.5573\n",
            "Epoch 494/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5537 - val_loss: 0.5572\n",
            "Epoch 495/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5535 - val_loss: 0.5572\n",
            "Epoch 496/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5535 - val_loss: 0.5571\n",
            "Epoch 497/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5535 - val_loss: 0.5573\n",
            "Epoch 498/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5537 - val_loss: 0.5574\n",
            "Epoch 499/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5539 - val_loss: 0.5576\n",
            "Epoch 500/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5540 - val_loss: 0.5577\n",
            "Epoch 501/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5540 - val_loss: 0.5576\n",
            "Epoch 502/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5539 - val_loss: 0.5574\n",
            "Epoch 503/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5537 - val_loss: 0.5571\n",
            "Epoch 504/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5535 - val_loss: 0.5573\n",
            "Epoch 505/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5536 - val_loss: 0.5572\n",
            "Epoch 506/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5535 - val_loss: 0.5571\n",
            "Epoch 507/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5534 - val_loss: 0.5572\n",
            "Epoch 508/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5535 - val_loss: 0.5571\n",
            "Epoch 509/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.5534 - val_loss: 0.5572\n",
            "Epoch 510/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5535 - val_loss: 0.5573\n",
            "Epoch 511/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5535 - val_loss: 0.5572\n",
            "Epoch 512/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5535 - val_loss: 0.5575\n",
            "Epoch 513/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5538 - val_loss: 0.5580\n",
            "Epoch 514/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5544 - val_loss: 0.5578\n",
            "Epoch 515/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5541 - val_loss: 0.5575\n",
            "Epoch 516/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5539 - val_loss: 0.5573\n",
            "Epoch 517/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5537 - val_loss: 0.5573\n",
            "Epoch 518/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5536 - val_loss: 0.5573\n",
            "Epoch 519/1000\n",
            "2/2 [==============================] - 2s 834ms/step - loss: 0.5535 - val_loss: 0.5572\n",
            "Epoch 520/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5534 - val_loss: 0.5571\n",
            "Epoch 521/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5533 - val_loss: 0.5570\n",
            "Epoch 522/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5533 - val_loss: 0.5571\n",
            "Epoch 523/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5533 - val_loss: 0.5573\n",
            "Epoch 524/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5537 - val_loss: 0.5578\n",
            "Epoch 525/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5540 - val_loss: 0.5572\n",
            "Epoch 526/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5535 - val_loss: 0.5574\n",
            "Epoch 527/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5536 - val_loss: 0.5572\n",
            "Epoch 528/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5534 - val_loss: 0.5570\n",
            "Epoch 529/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5533 - val_loss: 0.5570\n",
            "Epoch 530/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5532 - val_loss: 0.5571\n",
            "Epoch 531/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5533 - val_loss: 0.5570\n",
            "Epoch 532/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5532 - val_loss: 0.5570\n",
            "Epoch 533/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5532 - val_loss: 0.5570\n",
            "Epoch 534/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5532 - val_loss: 0.5571\n",
            "Epoch 535/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5533 - val_loss: 0.5572\n",
            "Epoch 536/1000\n",
            "2/2 [==============================] - 2s 834ms/step - loss: 0.5533 - val_loss: 0.5572\n",
            "Epoch 537/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5534 - val_loss: 0.5577\n",
            "Epoch 538/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5539 - val_loss: 0.5575\n",
            "Epoch 539/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5535 - val_loss: 0.5572\n",
            "Epoch 540/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5533 - val_loss: 0.5570\n",
            "Epoch 541/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5532 - val_loss: 0.5570\n",
            "Epoch 542/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5531 - val_loss: 0.5570\n",
            "Epoch 543/1000\n",
            "2/2 [==============================] - 2s 835ms/step - loss: 0.5531 - val_loss: 0.5569\n",
            "Epoch 544/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5530 - val_loss: 0.5569\n",
            "Epoch 545/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5530 - val_loss: 0.5570\n",
            "Epoch 546/1000\n",
            "2/2 [==============================] - 2s 853ms/step - loss: 0.5531 - val_loss: 0.5570\n",
            "Epoch 547/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5532 - val_loss: 0.5572\n",
            "Epoch 548/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5534 - val_loss: 0.5570\n",
            "Epoch 549/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5531 - val_loss: 0.5571\n",
            "Epoch 550/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5533 - val_loss: 0.5573\n",
            "Epoch 551/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5535 - val_loss: 0.5573\n",
            "Epoch 552/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5534 - val_loss: 0.5571\n",
            "Epoch 553/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5533 - val_loss: 0.5571\n",
            "Epoch 554/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.5532 - val_loss: 0.5569\n",
            "Epoch 555/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5531 - val_loss: 0.5570\n",
            "Epoch 556/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5530 - val_loss: 0.5568\n",
            "Epoch 557/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5529 - val_loss: 0.5568\n",
            "Epoch 558/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5529 - val_loss: 0.5568\n",
            "Epoch 559/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5529 - val_loss: 0.5568\n",
            "Epoch 560/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5529 - val_loss: 0.5567\n",
            "Epoch 561/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5528 - val_loss: 0.5569\n",
            "Epoch 562/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5529 - val_loss: 0.5569\n",
            "Epoch 563/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5529 - val_loss: 0.5570\n",
            "Epoch 564/1000\n",
            "2/2 [==============================] - 2s 853ms/step - loss: 0.5529 - val_loss: 0.5569\n",
            "Epoch 565/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5529 - val_loss: 0.5568\n",
            "Epoch 566/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5528 - val_loss: 0.5570\n",
            "Epoch 567/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5531 - val_loss: 0.5574\n",
            "Epoch 568/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5535 - val_loss: 0.5570\n",
            "Epoch 569/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5531 - val_loss: 0.5571\n",
            "Epoch 570/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5533 - val_loss: 0.5569\n",
            "Epoch 571/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5530 - val_loss: 0.5570\n",
            "Epoch 572/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5530 - val_loss: 0.5568\n",
            "Epoch 573/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5529 - val_loss: 0.5569\n",
            "Epoch 574/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5529 - val_loss: 0.5568\n",
            "Epoch 575/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5528 - val_loss: 0.5568\n",
            "Epoch 576/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5528 - val_loss: 0.5568\n",
            "Epoch 577/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5528 - val_loss: 0.5569\n",
            "Epoch 578/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5529 - val_loss: 0.5569\n",
            "Epoch 579/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5529 - val_loss: 0.5570\n",
            "Epoch 580/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5530 - val_loss: 0.5569\n",
            "Epoch 581/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5529 - val_loss: 0.5568\n",
            "Epoch 582/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5528 - val_loss: 0.5569\n",
            "Epoch 583/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5529 - val_loss: 0.5568\n",
            "Epoch 584/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5527 - val_loss: 0.5568\n",
            "Epoch 585/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5527 - val_loss: 0.5567\n",
            "Epoch 586/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5526 - val_loss: 0.5568\n",
            "Epoch 587/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5527 - val_loss: 0.5568\n",
            "Epoch 588/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5528 - val_loss: 0.5568\n",
            "Epoch 589/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5528 - val_loss: 0.5568\n",
            "Epoch 590/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5528 - val_loss: 0.5569\n",
            "Epoch 591/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5528 - val_loss: 0.5571\n",
            "Epoch 592/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5531 - val_loss: 0.5569\n",
            "Epoch 593/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5529 - val_loss: 0.5568\n",
            "Epoch 594/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5528 - val_loss: 0.5569\n",
            "Epoch 595/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5529 - val_loss: 0.5568\n",
            "Epoch 596/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5527 - val_loss: 0.5567\n",
            "Epoch 597/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5526 - val_loss: 0.5567\n",
            "Epoch 598/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5526 - val_loss: 0.5566\n",
            "Epoch 599/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5525 - val_loss: 0.5568\n",
            "Epoch 600/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5526 - val_loss: 0.5566\n",
            "Epoch 601/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5525 - val_loss: 0.5566\n",
            "Epoch 602/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5525 - val_loss: 0.5566\n",
            "Epoch 603/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5525 - val_loss: 0.5567\n",
            "Epoch 604/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5525 - val_loss: 0.5567\n",
            "Epoch 605/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5525 - val_loss: 0.5566\n",
            "Epoch 606/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5525 - val_loss: 0.5569\n",
            "Epoch 607/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5528 - val_loss: 0.5571\n",
            "Epoch 608/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5530 - val_loss: 0.5572\n",
            "Epoch 609/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5530 - val_loss: 0.5572\n",
            "Epoch 610/1000\n",
            "2/2 [==============================] - 2s 854ms/step - loss: 0.5531 - val_loss: 0.5569\n",
            "Epoch 611/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5529 - val_loss: 0.5572\n",
            "Epoch 612/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5530 - val_loss: 0.5567\n",
            "Epoch 613/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5526 - val_loss: 0.5567\n",
            "Epoch 614/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5526 - val_loss: 0.5566\n",
            "Epoch 615/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5525 - val_loss: 0.5567\n",
            "Epoch 616/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5525 - val_loss: 0.5565\n",
            "Epoch 617/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5524 - val_loss: 0.5566\n",
            "Epoch 618/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5524 - val_loss: 0.5566\n",
            "Epoch 619/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5524 - val_loss: 0.5566\n",
            "Epoch 620/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5524 - val_loss: 0.5568\n",
            "Epoch 621/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5526 - val_loss: 0.5570\n",
            "Epoch 622/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5528 - val_loss: 0.5567\n",
            "Epoch 623/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5525 - val_loss: 0.5568\n",
            "Epoch 624/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5527 - val_loss: 0.5568\n",
            "Epoch 625/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5526 - val_loss: 0.5566\n",
            "Epoch 626/1000\n",
            "2/2 [==============================] - 2s 826ms/step - loss: 0.5524 - val_loss: 0.5565\n",
            "Epoch 627/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5523 - val_loss: 0.5565\n",
            "Epoch 628/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5523 - val_loss: 0.5566\n",
            "Epoch 629/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5524 - val_loss: 0.5565\n",
            "Epoch 630/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5523 - val_loss: 0.5566\n",
            "Epoch 631/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5524 - val_loss: 0.5565\n",
            "Epoch 632/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5523 - val_loss: 0.5568\n",
            "Epoch 633/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5526 - val_loss: 0.5566\n",
            "Epoch 634/1000\n",
            "2/2 [==============================] - 2s 834ms/step - loss: 0.5523 - val_loss: 0.5566\n",
            "Epoch 635/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5523 - val_loss: 0.5565\n",
            "Epoch 636/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5523 - val_loss: 0.5567\n",
            "Epoch 637/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5524 - val_loss: 0.5565\n",
            "Epoch 638/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5522 - val_loss: 0.5566\n",
            "Epoch 639/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.5524 - val_loss: 0.5567\n",
            "Epoch 640/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5525 - val_loss: 0.5567\n",
            "Epoch 641/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5525 - val_loss: 0.5566\n",
            "Epoch 642/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5523 - val_loss: 0.5566\n",
            "Epoch 643/1000\n",
            "2/2 [==============================] - 2s 853ms/step - loss: 0.5524 - val_loss: 0.5566\n",
            "Epoch 644/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5523 - val_loss: 0.5566\n",
            "Epoch 645/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5523 - val_loss: 0.5565\n",
            "Epoch 646/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5523 - val_loss: 0.5567\n",
            "Epoch 647/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5524 - val_loss: 0.5566\n",
            "Epoch 648/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5523 - val_loss: 0.5566\n",
            "Epoch 649/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5523 - val_loss: 0.5565\n",
            "Epoch 650/1000\n",
            "2/2 [==============================] - 2s 835ms/step - loss: 0.5522 - val_loss: 0.5565\n",
            "Epoch 651/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5521 - val_loss: 0.5564\n",
            "Epoch 652/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5521 - val_loss: 0.5564\n",
            "Epoch 653/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5521 - val_loss: 0.5564\n",
            "Epoch 654/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5521 - val_loss: 0.5564\n",
            "Epoch 655/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5521 - val_loss: 0.5566\n",
            "Epoch 656/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5523 - val_loss: 0.5566\n",
            "Epoch 657/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5524 - val_loss: 0.5566\n",
            "Epoch 658/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5522 - val_loss: 0.5564\n",
            "Epoch 659/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5522 - val_loss: 0.5567\n",
            "Epoch 660/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5523 - val_loss: 0.5564\n",
            "Epoch 661/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5521 - val_loss: 0.5567\n",
            "Epoch 662/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5524 - val_loss: 0.5566\n",
            "Epoch 663/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5523 - val_loss: 0.5564\n",
            "Epoch 664/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5521 - val_loss: 0.5566\n",
            "Epoch 665/1000\n",
            "2/2 [==============================] - 2s 834ms/step - loss: 0.5522 - val_loss: 0.5564\n",
            "Epoch 666/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5521 - val_loss: 0.5566\n",
            "Epoch 667/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5522 - val_loss: 0.5565\n",
            "Epoch 668/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5522 - val_loss: 0.5565\n",
            "Epoch 669/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5521 - val_loss: 0.5563\n",
            "Epoch 670/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5519 - val_loss: 0.5563\n",
            "Epoch 671/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5519 - val_loss: 0.5562\n",
            "Epoch 672/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5518 - val_loss: 0.5563\n",
            "Epoch 673/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5519 - val_loss: 0.5562\n",
            "Epoch 674/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5518 - val_loss: 0.5563\n",
            "Epoch 675/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5519 - val_loss: 0.5565\n",
            "Epoch 676/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5522 - val_loss: 0.5571\n",
            "Epoch 677/1000\n",
            "2/2 [==============================] - 2s 832ms/step - loss: 0.5528 - val_loss: 0.5569\n",
            "Epoch 678/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5524 - val_loss: 0.5568\n",
            "Epoch 679/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5526 - val_loss: 0.5566\n",
            "Epoch 680/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5522 - val_loss: 0.5567\n",
            "Epoch 681/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5523 - val_loss: 0.5564\n",
            "Epoch 682/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5520 - val_loss: 0.5563\n",
            "Epoch 683/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5519 - val_loss: 0.5564\n",
            "Epoch 684/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5519 - val_loss: 0.5563\n",
            "Epoch 685/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5519 - val_loss: 0.5562\n",
            "Epoch 686/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5518 - val_loss: 0.5563\n",
            "Epoch 687/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5519 - val_loss: 0.5565\n",
            "Epoch 688/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5521 - val_loss: 0.5565\n",
            "Epoch 689/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5522 - val_loss: 0.5565\n",
            "Epoch 690/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5521 - val_loss: 0.5566\n",
            "Epoch 691/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5521 - val_loss: 0.5567\n",
            "Epoch 692/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5522 - val_loss: 0.5566\n",
            "Epoch 693/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5521 - val_loss: 0.5566\n",
            "Epoch 694/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5522 - val_loss: 0.5564\n",
            "Epoch 695/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5519 - val_loss: 0.5565\n",
            "Epoch 696/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5521 - val_loss: 0.5564\n",
            "Epoch 697/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5519 - val_loss: 0.5563\n",
            "Epoch 698/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5518 - val_loss: 0.5563\n",
            "Epoch 699/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5518 - val_loss: 0.5562\n",
            "Epoch 700/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5518 - val_loss: 0.5563\n",
            "Epoch 701/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5517 - val_loss: 0.5562\n",
            "Epoch 702/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5517 - val_loss: 0.5561\n",
            "Epoch 703/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5516 - val_loss: 0.5562\n",
            "Epoch 704/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5517 - val_loss: 0.5564\n",
            "Epoch 705/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5519 - val_loss: 0.5564\n",
            "Epoch 706/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5519 - val_loss: 0.5565\n",
            "Epoch 707/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5520 - val_loss: 0.5564\n",
            "Epoch 708/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5519 - val_loss: 0.5566\n",
            "Epoch 709/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5522 - val_loss: 0.5567\n",
            "Epoch 710/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5521 - val_loss: 0.5566\n",
            "Epoch 711/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5521 - val_loss: 0.5564\n",
            "Epoch 712/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5518 - val_loss: 0.5565\n",
            "Epoch 713/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5519 - val_loss: 0.5561\n",
            "Epoch 714/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5517 - val_loss: 0.5562\n",
            "Epoch 715/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5516 - val_loss: 0.5563\n",
            "Epoch 716/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5517 - val_loss: 0.5562\n",
            "Epoch 717/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5517 - val_loss: 0.5561\n",
            "Epoch 718/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5516 - val_loss: 0.5561\n",
            "Epoch 719/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5515 - val_loss: 0.5561\n",
            "Epoch 720/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5515 - val_loss: 0.5560\n",
            "Epoch 721/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5515 - val_loss: 0.5561\n",
            "Epoch 722/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5515 - val_loss: 0.5561\n",
            "Epoch 723/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5516 - val_loss: 0.5562\n",
            "Epoch 724/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5516 - val_loss: 0.5563\n",
            "Epoch 725/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5517 - val_loss: 0.5563\n",
            "Epoch 726/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5518 - val_loss: 0.5567\n",
            "Epoch 727/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5521 - val_loss: 0.5566\n",
            "Epoch 728/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5520 - val_loss: 0.5564\n",
            "Epoch 729/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5520 - val_loss: 0.5564\n",
            "Epoch 730/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5518 - val_loss: 0.5564\n",
            "Epoch 731/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5518 - val_loss: 0.5562\n",
            "Epoch 732/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5516 - val_loss: 0.5561\n",
            "Epoch 733/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5516 - val_loss: 0.5560\n",
            "Epoch 734/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5514 - val_loss: 0.5561\n",
            "Epoch 735/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5514 - val_loss: 0.5560\n",
            "Epoch 736/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5514 - val_loss: 0.5560\n",
            "Epoch 737/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5514 - val_loss: 0.5561\n",
            "Epoch 738/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5515 - val_loss: 0.5562\n",
            "Epoch 739/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5516 - val_loss: 0.5565\n",
            "Epoch 740/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5520 - val_loss: 0.5564\n",
            "Epoch 741/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5517 - val_loss: 0.5562\n",
            "Epoch 742/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5516 - val_loss: 0.5565\n",
            "Epoch 743/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5520 - val_loss: 0.5567\n",
            "Epoch 744/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5521 - val_loss: 0.5562\n",
            "Epoch 745/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5518 - val_loss: 0.5566\n",
            "Epoch 746/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5518 - val_loss: 0.5563\n",
            "Epoch 747/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5517 - val_loss: 0.5562\n",
            "Epoch 748/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5516 - val_loss: 0.5562\n",
            "Epoch 749/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5516 - val_loss: 0.5560\n",
            "Epoch 750/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5514 - val_loss: 0.5561\n",
            "Epoch 751/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5514 - val_loss: 0.5561\n",
            "Epoch 752/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5514 - val_loss: 0.5559\n",
            "Epoch 753/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5513 - val_loss: 0.5561\n",
            "Epoch 754/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5513 - val_loss: 0.5560\n",
            "Epoch 755/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5513 - val_loss: 0.5560\n",
            "Epoch 756/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5513 - val_loss: 0.5561\n",
            "Epoch 757/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5515 - val_loss: 0.5561\n",
            "Epoch 758/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5515 - val_loss: 0.5562\n",
            "Epoch 759/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5514 - val_loss: 0.5560\n",
            "Epoch 760/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5513 - val_loss: 0.5561\n",
            "Epoch 761/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5515 - val_loss: 0.5561\n",
            "Epoch 762/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5514 - val_loss: 0.5559\n",
            "Epoch 763/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5512 - val_loss: 0.5559\n",
            "Epoch 764/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5513 - val_loss: 0.5560\n",
            "Epoch 765/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5513 - val_loss: 0.5563\n",
            "Epoch 766/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5516 - val_loss: 0.5564\n",
            "Epoch 767/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5517 - val_loss: 0.5562\n",
            "Epoch 768/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5515 - val_loss: 0.5563\n",
            "Epoch 769/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5515 - val_loss: 0.5561\n",
            "Epoch 770/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5514 - val_loss: 0.5561\n",
            "Epoch 771/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5514 - val_loss: 0.5560\n",
            "Epoch 772/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5513 - val_loss: 0.5561\n",
            "Epoch 773/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5514 - val_loss: 0.5559\n",
            "Epoch 774/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5512 - val_loss: 0.5560\n",
            "Epoch 775/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5513 - val_loss: 0.5558\n",
            "Epoch 776/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5511 - val_loss: 0.5560\n",
            "Epoch 777/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.5513 - val_loss: 0.5560\n",
            "Epoch 778/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5513 - val_loss: 0.5561\n",
            "Epoch 779/1000\n",
            "2/2 [==============================] - 2s 819ms/step - loss: 0.5513 - val_loss: 0.5559\n",
            "Epoch 780/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5511 - val_loss: 0.5561\n",
            "Epoch 781/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5514 - val_loss: 0.5564\n",
            "Epoch 782/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5516 - val_loss: 0.5565\n",
            "Epoch 783/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5516 - val_loss: 0.5563\n",
            "Epoch 784/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5517 - val_loss: 0.5570\n",
            "Epoch 785/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5522 - val_loss: 0.5565\n",
            "Epoch 786/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5517 - val_loss: 0.5562\n",
            "Epoch 787/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5515 - val_loss: 0.5562\n",
            "Epoch 788/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5514 - val_loss: 0.5560\n",
            "Epoch 789/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5513 - val_loss: 0.5559\n",
            "Epoch 790/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5512 - val_loss: 0.5560\n",
            "Epoch 791/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5512 - val_loss: 0.5558\n",
            "Epoch 792/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5511 - val_loss: 0.5559\n",
            "Epoch 793/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5510 - val_loss: 0.5559\n",
            "Epoch 794/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5511 - val_loss: 0.5557\n",
            "Epoch 795/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5509 - val_loss: 0.5558\n",
            "Epoch 796/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5510 - val_loss: 0.5558\n",
            "Epoch 797/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5510 - val_loss: 0.5558\n",
            "Epoch 798/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5510 - val_loss: 0.5559\n",
            "Epoch 799/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5511 - val_loss: 0.5560\n",
            "Epoch 800/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5512 - val_loss: 0.5559\n",
            "Epoch 801/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5511 - val_loss: 0.5559\n",
            "Epoch 802/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5511 - val_loss: 0.5561\n",
            "Epoch 803/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5513 - val_loss: 0.5560\n",
            "Epoch 804/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5511 - val_loss: 0.5561\n",
            "Epoch 805/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5513 - val_loss: 0.5561\n",
            "Epoch 806/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5513 - val_loss: 0.5561\n",
            "Epoch 807/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5513 - val_loss: 0.5562\n",
            "Epoch 808/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5513 - val_loss: 0.5559\n",
            "Epoch 809/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5511 - val_loss: 0.5559\n",
            "Epoch 810/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5511 - val_loss: 0.5558\n",
            "Epoch 811/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5510 - val_loss: 0.5558\n",
            "Epoch 812/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5510 - val_loss: 0.5558\n",
            "Epoch 813/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5510 - val_loss: 0.5558\n",
            "Epoch 814/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5510 - val_loss: 0.5560\n",
            "Epoch 815/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5511 - val_loss: 0.5559\n",
            "Epoch 816/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5511 - val_loss: 0.5562\n",
            "Epoch 817/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5514 - val_loss: 0.5562\n",
            "Epoch 818/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5515 - val_loss: 0.5562\n",
            "Epoch 819/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5513 - val_loss: 0.5560\n",
            "Epoch 820/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5512 - val_loss: 0.5561\n",
            "Epoch 821/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5512 - val_loss: 0.5559\n",
            "Epoch 822/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5510 - val_loss: 0.5559\n",
            "Epoch 823/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5510 - val_loss: 0.5556\n",
            "Epoch 824/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5508 - val_loss: 0.5558\n",
            "Epoch 825/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5509 - val_loss: 0.5557\n",
            "Epoch 826/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5508 - val_loss: 0.5558\n",
            "Epoch 827/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5510 - val_loss: 0.5560\n",
            "Epoch 828/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5511 - val_loss: 0.5560\n",
            "Epoch 829/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5512 - val_loss: 0.5560\n",
            "Epoch 830/1000\n",
            "2/2 [==============================] - 2s 854ms/step - loss: 0.5511 - val_loss: 0.5560\n",
            "Epoch 831/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5511 - val_loss: 0.5560\n",
            "Epoch 832/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5512 - val_loss: 0.5561\n",
            "Epoch 833/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5512 - val_loss: 0.5560\n",
            "Epoch 834/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5512 - val_loss: 0.5560\n",
            "Epoch 835/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5512 - val_loss: 0.5558\n",
            "Epoch 836/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5509 - val_loss: 0.5559\n",
            "Epoch 837/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5510 - val_loss: 0.5559\n",
            "Epoch 838/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5510 - val_loss: 0.5559\n",
            "Epoch 839/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5510 - val_loss: 0.5557\n",
            "Epoch 840/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5508 - val_loss: 0.5558\n",
            "Epoch 841/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5508 - val_loss: 0.5557\n",
            "Epoch 842/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5508 - val_loss: 0.5558\n",
            "Epoch 843/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5508 - val_loss: 0.5557\n",
            "Epoch 844/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5507 - val_loss: 0.5556\n",
            "Epoch 845/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5507 - val_loss: 0.5556\n",
            "Epoch 846/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5507 - val_loss: 0.5557\n",
            "Epoch 847/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5507 - val_loss: 0.5558\n",
            "Epoch 848/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5509 - val_loss: 0.5559\n",
            "Epoch 849/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5510 - val_loss: 0.5559\n",
            "Epoch 850/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5509 - val_loss: 0.5559\n",
            "Epoch 851/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5509 - val_loss: 0.5560\n",
            "Epoch 852/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5510 - val_loss: 0.5560\n",
            "Epoch 853/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5512 - val_loss: 0.5562\n",
            "Epoch 854/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5512 - val_loss: 0.5559\n",
            "Epoch 855/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5511 - val_loss: 0.5560\n",
            "Epoch 856/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5510 - val_loss: 0.5559\n",
            "Epoch 857/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5509 - val_loss: 0.5558\n",
            "Epoch 858/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5508 - val_loss: 0.5560\n",
            "Epoch 859/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5509 - val_loss: 0.5557\n",
            "Epoch 860/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5508 - val_loss: 0.5557\n",
            "Epoch 861/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5508 - val_loss: 0.5558\n",
            "Epoch 862/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5508 - val_loss: 0.5556\n",
            "Epoch 863/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5506 - val_loss: 0.5558\n",
            "Epoch 864/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5507 - val_loss: 0.5556\n",
            "Epoch 865/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5506 - val_loss: 0.5555\n",
            "Epoch 866/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5505 - val_loss: 0.5556\n",
            "Epoch 867/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5506 - val_loss: 0.5555\n",
            "Epoch 868/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5505 - val_loss: 0.5556\n",
            "Epoch 869/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5506 - val_loss: 0.5558\n",
            "Epoch 870/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5508 - val_loss: 0.5557\n",
            "Epoch 871/1000\n",
            "2/2 [==============================] - 2s 815ms/step - loss: 0.5507 - val_loss: 0.5556\n",
            "Epoch 872/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5506 - val_loss: 0.5558\n",
            "Epoch 873/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5508 - val_loss: 0.5559\n",
            "Epoch 874/1000\n",
            "2/2 [==============================] - 2s 851ms/step - loss: 0.5509 - val_loss: 0.5560\n",
            "Epoch 875/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5511 - val_loss: 0.5558\n",
            "Epoch 876/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5508 - val_loss: 0.5559\n",
            "Epoch 877/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5509 - val_loss: 0.5558\n",
            "Epoch 878/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5508 - val_loss: 0.5557\n",
            "Epoch 879/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5507 - val_loss: 0.5559\n",
            "Epoch 880/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5508 - val_loss: 0.5556\n",
            "Epoch 881/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5506 - val_loss: 0.5556\n",
            "Epoch 882/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5506 - val_loss: 0.5557\n",
            "Epoch 883/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5507 - val_loss: 0.5556\n",
            "Epoch 884/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5506 - val_loss: 0.5556\n",
            "Epoch 885/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5505 - val_loss: 0.5555\n",
            "Epoch 886/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5505 - val_loss: 0.5555\n",
            "Epoch 887/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5505 - val_loss: 0.5557\n",
            "Epoch 888/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5506 - val_loss: 0.5557\n",
            "Epoch 889/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5507 - val_loss: 0.5559\n",
            "Epoch 890/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5509 - val_loss: 0.5563\n",
            "Epoch 891/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5511 - val_loss: 0.5562\n",
            "Epoch 892/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5512 - val_loss: 0.5562\n",
            "Epoch 893/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5512 - val_loss: 0.5558\n",
            "Epoch 894/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5507 - val_loss: 0.5559\n",
            "Epoch 895/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5508 - val_loss: 0.5557\n",
            "Epoch 896/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5506 - val_loss: 0.5557\n",
            "Epoch 897/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5506 - val_loss: 0.5556\n",
            "Epoch 898/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5505 - val_loss: 0.5554\n",
            "Epoch 899/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5503 - val_loss: 0.5555\n",
            "Epoch 900/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5504 - val_loss: 0.5554\n",
            "Epoch 901/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5503 - val_loss: 0.5554\n",
            "Epoch 902/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5503 - val_loss: 0.5556\n",
            "Epoch 903/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5505 - val_loss: 0.5555\n",
            "Epoch 904/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5505 - val_loss: 0.5556\n",
            "Epoch 905/1000\n",
            "2/2 [==============================] - 2s 819ms/step - loss: 0.5505 - val_loss: 0.5556\n",
            "Epoch 906/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5505 - val_loss: 0.5557\n",
            "Epoch 907/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5507 - val_loss: 0.5559\n",
            "Epoch 908/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5507 - val_loss: 0.5556\n",
            "Epoch 909/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5505 - val_loss: 0.5560\n",
            "Epoch 910/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5510 - val_loss: 0.5558\n",
            "Epoch 911/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5507 - val_loss: 0.5560\n",
            "Epoch 912/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5509 - val_loss: 0.5555\n",
            "Epoch 913/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5505 - val_loss: 0.5556\n",
            "Epoch 914/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5505 - val_loss: 0.5555\n",
            "Epoch 915/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5504 - val_loss: 0.5555\n",
            "Epoch 916/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5504 - val_loss: 0.5555\n",
            "Epoch 917/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5504 - val_loss: 0.5556\n",
            "Epoch 918/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5505 - val_loss: 0.5557\n",
            "Epoch 919/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5506 - val_loss: 0.5558\n",
            "Epoch 920/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5506 - val_loss: 0.5556\n",
            "Epoch 921/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5505 - val_loss: 0.5555\n",
            "Epoch 922/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5504 - val_loss: 0.5556\n",
            "Epoch 923/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5504 - val_loss: 0.5556\n",
            "Epoch 924/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5504 - val_loss: 0.5554\n",
            "Epoch 925/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5502 - val_loss: 0.5555\n",
            "Epoch 926/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5503 - val_loss: 0.5557\n",
            "Epoch 927/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5505 - val_loss: 0.5555\n",
            "Epoch 928/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5503 - val_loss: 0.5555\n",
            "Epoch 929/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5504 - val_loss: 0.5558\n",
            "Epoch 930/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5505 - val_loss: 0.5555\n",
            "Epoch 931/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5503 - val_loss: 0.5557\n",
            "Epoch 932/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5505 - val_loss: 0.5556\n",
            "Epoch 933/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5503 - val_loss: 0.5553\n",
            "Epoch 934/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5503 - val_loss: 0.5556\n",
            "Epoch 935/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5504 - val_loss: 0.5555\n",
            "Epoch 936/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5503 - val_loss: 0.5556\n",
            "Epoch 937/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5505 - val_loss: 0.5555\n",
            "Epoch 938/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5503 - val_loss: 0.5556\n",
            "Epoch 939/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5504 - val_loss: 0.5555\n",
            "Epoch 940/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5503 - val_loss: 0.5559\n",
            "Epoch 941/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5506 - val_loss: 0.5557\n",
            "Epoch 942/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5505 - val_loss: 0.5556\n",
            "Epoch 943/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5505 - val_loss: 0.5555\n",
            "Epoch 944/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5503 - val_loss: 0.5556\n",
            "Epoch 945/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5504 - val_loss: 0.5554\n",
            "Epoch 946/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5502 - val_loss: 0.5553\n",
            "Epoch 947/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5501 - val_loss: 0.5553\n",
            "Epoch 948/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5501 - val_loss: 0.5554\n",
            "Epoch 949/1000\n",
            "2/2 [==============================] - 2s 854ms/step - loss: 0.5502 - val_loss: 0.5554\n",
            "Epoch 950/1000\n",
            "2/2 [==============================] - 2s 834ms/step - loss: 0.5502 - val_loss: 0.5554\n",
            "Epoch 951/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5502 - val_loss: 0.5557\n",
            "Epoch 952/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5505 - val_loss: 0.5556\n",
            "Epoch 953/1000\n",
            "2/2 [==============================] - 2s 853ms/step - loss: 0.5503 - val_loss: 0.5555\n",
            "Epoch 954/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5504 - val_loss: 0.5557\n",
            "Epoch 955/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.5506 - val_loss: 0.5556\n",
            "Epoch 956/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5503 - val_loss: 0.5556\n",
            "Epoch 957/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5504 - val_loss: 0.5554\n",
            "Epoch 958/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5502 - val_loss: 0.5554\n",
            "Epoch 959/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5502 - val_loss: 0.5554\n",
            "Epoch 960/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5502 - val_loss: 0.5553\n",
            "Epoch 961/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5501 - val_loss: 0.5553\n",
            "Epoch 962/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5501 - val_loss: 0.5553\n",
            "Epoch 963/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5501 - val_loss: 0.5554\n",
            "Epoch 964/1000\n",
            "2/2 [==============================] - 2s 839ms/step - loss: 0.5501 - val_loss: 0.5554\n",
            "Epoch 965/1000\n",
            "2/2 [==============================] - 2s 843ms/step - loss: 0.5501 - val_loss: 0.5554\n",
            "Epoch 966/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5501 - val_loss: 0.5554\n",
            "Epoch 967/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5503 - val_loss: 0.5557\n",
            "Epoch 968/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5504 - val_loss: 0.5554\n",
            "Epoch 969/1000\n",
            "2/2 [==============================] - 2s 852ms/step - loss: 0.5501 - val_loss: 0.5555\n",
            "Epoch 970/1000\n",
            "2/2 [==============================] - 2s 841ms/step - loss: 0.5503 - val_loss: 0.5554\n",
            "Epoch 971/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5502 - val_loss: 0.5554\n",
            "Epoch 972/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5502 - val_loss: 0.5553\n",
            "Epoch 973/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5501 - val_loss: 0.5554\n",
            "Epoch 974/1000\n",
            "2/2 [==============================] - 2s 840ms/step - loss: 0.5502 - val_loss: 0.5553\n",
            "Epoch 975/1000\n",
            "2/2 [==============================] - 2s 845ms/step - loss: 0.5501 - val_loss: 0.5555\n",
            "Epoch 976/1000\n",
            "2/2 [==============================] - 2s 853ms/step - loss: 0.5502 - val_loss: 0.5553\n",
            "Epoch 977/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5501 - val_loss: 0.5554\n",
            "Epoch 978/1000\n",
            "2/2 [==============================] - 2s 838ms/step - loss: 0.5501 - val_loss: 0.5552\n",
            "Epoch 979/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5500 - val_loss: 0.5552\n",
            "Epoch 980/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5499 - val_loss: 0.5552\n",
            "Epoch 981/1000\n",
            "2/2 [==============================] - 2s 846ms/step - loss: 0.5499 - val_loss: 0.5552\n",
            "Epoch 982/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5499 - val_loss: 0.5553\n",
            "Epoch 983/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5500 - val_loss: 0.5553\n",
            "Epoch 984/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5500 - val_loss: 0.5555\n",
            "Epoch 985/1000\n",
            "2/2 [==============================] - 2s 848ms/step - loss: 0.5501 - val_loss: 0.5554\n",
            "Epoch 986/1000\n",
            "2/2 [==============================] - 2s 849ms/step - loss: 0.5501 - val_loss: 0.5556\n",
            "Epoch 987/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5503 - val_loss: 0.5556\n",
            "Epoch 988/1000\n",
            "2/2 [==============================] - 2s 847ms/step - loss: 0.5504 - val_loss: 0.5558\n",
            "Epoch 989/1000\n",
            "2/2 [==============================] - 2s 837ms/step - loss: 0.5505 - val_loss: 0.5556\n",
            "Epoch 990/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5503 - val_loss: 0.5554\n",
            "Epoch 991/1000\n",
            "2/2 [==============================] - 2s 856ms/step - loss: 0.5501 - val_loss: 0.5555\n",
            "Epoch 992/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5501 - val_loss: 0.5553\n",
            "Epoch 993/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5500 - val_loss: 0.5553\n",
            "Epoch 994/1000\n",
            "2/2 [==============================] - 2s 850ms/step - loss: 0.5500 - val_loss: 0.5552\n",
            "Epoch 995/1000\n",
            "2/2 [==============================] - 2s 842ms/step - loss: 0.5499 - val_loss: 0.5552\n",
            "Epoch 996/1000\n",
            "2/2 [==============================] - 2s 844ms/step - loss: 0.5498 - val_loss: 0.5553\n",
            "Epoch 997/1000\n",
            "2/2 [==============================] - 2s 853ms/step - loss: 0.5500 - val_loss: 0.5553\n",
            "Epoch 998/1000\n",
            "2/2 [==============================] - 2s 836ms/step - loss: 0.5500 - val_loss: 0.5553\n",
            "Epoch 999/1000\n",
            "2/2 [==============================] - 2s 819ms/step - loss: 0.5500 - val_loss: 0.5552\n",
            "Epoch 1000/1000\n",
            "2/2 [==============================] - 2s 816ms/step - loss: 0.5499 - val_loss: 0.5553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f754eacb2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI5-9dhlwELN"
      },
      "source": [
        "if cnt==2:\n",
        " encoder = Model(input_layer, encoded3)\n",
        " dat_resaXautotr = encoder.predict(dat_resaXtr)\n",
        " dat_resaXautots = encoder.predict(dat_resaXts)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkFsMLAldDQ_"
      },
      "source": [
        "### dataset2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7XCesQQTroS"
      },
      "source": [
        "#dat_resa=dat_resa[dat_resa.Disease!='lung']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW02mQ-zTroS"
      },
      "source": [
        "#dat_resa=dat_resa[dat_resa.Disease!='colon']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOyT8zQTTroT"
      },
      "source": [
        "#dat_resa=dat_resa[dat_resa.Disease!='myeloma']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMPuleuTTroT",
        "outputId": "ae9d60a9-6af5-493e-f3b6-d9a65d7396d0"
      },
      "source": [
        "if cnt==3:\n",
        " dat_resa['Disease'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "leukaemia    3770\n",
              "normal       2400\n",
              "breast       1692\n",
              "lymphoma     1046\n",
              "Name: Disease, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSYqcfIOTroT"
      },
      "source": [
        "dat_resa.to_csv('dat_resa2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwsjrIZTQRD2"
      },
      "source": [
        "#### full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n-CANa5QRD2"
      },
      "source": [
        "if cnt==3: \n",
        " scaler=MinMaxScaler()\n",
        " dat_resaX=scaler.fit_transform(dat_resa.drop(['Disease'],axis=1))\n",
        " over_sampler = SMOTE(k_neighbors=10, sampling_strategy=\"auto\")\n",
        " dat_resaX, dat_resaY = over_sampler.fit_resample(dat_resaX, dat_resa['Disease'])\n",
        " selector = SelectKBest(score_func=f_classif, k=100)\n",
        " dat_resaXred=selector.fit_transform(dat_resaX,dat_resaY)\n",
        " dat_resaXred\n",
        " selector.pvalues_\n",
        " selector.scores_\n",
        " pca_transformer = PCA(n_components=100)\n",
        " dat_resaXpca1=pca_transformer.fit(dat_resaX)\n",
        " dat_resaXpca=dat_resaXpca1.transform(dat_resaX)\n",
        " dat_resaXpca1.explained_variance_ratio_\n",
        " dat_resaXpca1.explained_variance_ratio_.cumsum()\n",
        " dat_resaXpca.shape\n",
        " isomap = Isomap(n_components=100, n_neighbors=30,n_jobs=-1)\n",
        " dat_resaXiso=isomap.fit_transform(dat_resaX)\n",
        " isomap.reconstruction_error()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9bBQjCUQRD2"
      },
      "source": [
        "#over_sampler = SMOTE(k_neighbors=10, sampling_strategy=\"auto\")\n",
        "#dat_resaX, dat_resaY = over_sampler.fit_resample(dat_resaX, dat_resa['Disease'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-3Scr6GQRD3",
        "outputId": "3491e7e4-d7f3-4196-cb67-9ec6acb48cfa"
      },
      "source": [
        "#selector = SelectKBest(score_func=f_classif, k=100)\n",
        "#dat_resaXred=selector.fit_transform(dat_resaX,dat_resaY)\n",
        "#dat_resaXred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.07850676, 0.08692949, 0.75883453, ..., 0.03341346, 0.1324023 ,\n",
              "        0.35887988],\n",
              "       [0.57956096, 0.35269245, 0.83266069, ..., 0.05336538, 0.05629533,\n",
              "        0.3238762 ],\n",
              "       [0.07602629, 0.10210173, 0.08947731, ..., 0.75841346, 0.38399913,\n",
              "        0.11201179],\n",
              "       ...,\n",
              "       [0.4023634 , 0.12921121, 0.80159614, ..., 0.03768846, 0.12340426,\n",
              "        0.83006645],\n",
              "       [0.12746319, 0.15058441, 0.20562315, ..., 0.27286764, 0.1771782 ,\n",
              "        0.11423249],\n",
              "       [0.04725105, 0.11952191, 0.05338951, ..., 0.76076294, 0.42749272,\n",
              "        0.09006751]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKJQPK8_QRD4",
        "outputId": "11a64ab6-1047-439e-f79e-8fa13614e93e"
      },
      "source": [
        "#selector.pvalues_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00000000e+000, 0.00000000e+000, 1.35791082e-036, ...,\n",
              "       6.91691904e-323, 0.00000000e+000, 1.00088461e-128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_X7aqFRQRD4",
        "outputId": "2b4238f1-cf0c-45fc-daf6-5bee92cab68c"
      },
      "source": [
        "#selector.scores_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6599.34015992,  660.99659735,   56.94142246, ...,  522.19966551,\n",
              "       2380.01247117,  202.43134606])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7Bvoxi7QRD5"
      },
      "source": [
        "#pca_transformer = PCA(n_components=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiO8lp8KQRD5"
      },
      "source": [
        "#dat_resaXpca1=pca_transformer.fit(dat_resaX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh0ZLlcoQRD5"
      },
      "source": [
        "#dat_resaXpca=dat_resaXpca1.transform(dat_resaX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7uG3-pXQRD5",
        "outputId": "98693999-62a6-4161-81ac-835d23154b7c"
      },
      "source": [
        "#dat_resaXpca1.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.14015236, 0.08568239, 0.07604876, 0.05069316, 0.03852839,\n",
              "       0.02723593, 0.02572249, 0.02282794, 0.02107044, 0.0179895 ,\n",
              "       0.015154  , 0.01471306, 0.01271441, 0.01086885, 0.00998379,\n",
              "       0.00864941, 0.00804732, 0.00787021, 0.00725173, 0.00692444,\n",
              "       0.00672936, 0.00621492, 0.00578324, 0.00565799, 0.00510014,\n",
              "       0.0049209 , 0.00481283, 0.00417362, 0.00403483, 0.00388842,\n",
              "       0.00360471, 0.00338457, 0.00327139, 0.00314473, 0.00303803,\n",
              "       0.00293393, 0.00285559, 0.00276567, 0.0026925 , 0.00261874,\n",
              "       0.00249799, 0.00242189, 0.00235267, 0.00220556, 0.0021597 ,\n",
              "       0.00204519, 0.0020088 , 0.00188   , 0.00184044, 0.00177697,\n",
              "       0.00175092, 0.00172365, 0.00167988, 0.00165946, 0.00164525,\n",
              "       0.0015518 , 0.0015241 , 0.00151647, 0.00144185, 0.00143337,\n",
              "       0.00141198, 0.00137249, 0.0013495 , 0.00131764, 0.00129209,\n",
              "       0.00124831, 0.00123667, 0.00122661, 0.00121908, 0.001182  ,\n",
              "       0.00115399, 0.00113779, 0.00109751, 0.00106751, 0.00105763,\n",
              "       0.00103447, 0.00103352, 0.00102147, 0.00099885, 0.00097791,\n",
              "       0.00096052, 0.00094815, 0.00093156, 0.00091087, 0.00090261,\n",
              "       0.00089302, 0.00086926, 0.00086433, 0.00085343, 0.00083726,\n",
              "       0.00081992, 0.00080743, 0.00079435, 0.00077848, 0.00076808,\n",
              "       0.00074585, 0.00074068, 0.00073113, 0.00072982, 0.00071952])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g6FZYLSQRD5",
        "outputId": "ada4206a-0258-4806-f7c0-3657ffe6228e"
      },
      "source": [
        "#dat_resaXpca1.explained_variance_ratio_.cumsum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.14015236, 0.22583474, 0.30188351, 0.35257667, 0.39110505,\n",
              "       0.41834098, 0.44406347, 0.46689141, 0.48796185, 0.50595135,\n",
              "       0.52110535, 0.53581841, 0.54853282, 0.55940167, 0.56938546,\n",
              "       0.57803487, 0.58608219, 0.59395239, 0.60120413, 0.60812857,\n",
              "       0.61485792, 0.62107284, 0.62685608, 0.63251407, 0.63761421,\n",
              "       0.64253511, 0.64734793, 0.65152156, 0.65555639, 0.65944481,\n",
              "       0.66304952, 0.66643409, 0.66970548, 0.67285021, 0.67588823,\n",
              "       0.67882217, 0.68167775, 0.68444342, 0.68713592, 0.68975467,\n",
              "       0.69225266, 0.69467455, 0.69702722, 0.69923278, 0.70139248,\n",
              "       0.70343767, 0.70544647, 0.70732647, 0.70916692, 0.71094389,\n",
              "       0.71269481, 0.71441846, 0.71609834, 0.7177578 , 0.71940305,\n",
              "       0.72095485, 0.72247894, 0.72399541, 0.72543727, 0.72687064,\n",
              "       0.72828262, 0.72965511, 0.73100462, 0.73232226, 0.73361435,\n",
              "       0.73486266, 0.73609933, 0.73732594, 0.73854502, 0.73972702,\n",
              "       0.74088101, 0.7420188 , 0.74311631, 0.74418382, 0.74524146,\n",
              "       0.74627592, 0.74730944, 0.74833092, 0.74932977, 0.75030768,\n",
              "       0.7512682 , 0.75221635, 0.75314791, 0.75405878, 0.75496139,\n",
              "       0.75585441, 0.75672367, 0.757588  , 0.75844143, 0.75927869,\n",
              "       0.76009861, 0.76090604, 0.76170039, 0.76247888, 0.76324695,\n",
              "       0.7639928 , 0.76473348, 0.76546461, 0.76619443, 0.76691395])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qdfmuPrQRD5",
        "outputId": "f26daf91-fcc9-4a2b-ed80-fae16105df0a"
      },
      "source": [
        "#dat_resaXpca.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15080, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KHgODm-QRD6"
      },
      "source": [
        "#isomap = Isomap(n_components=100, n_neighbors=30,n_jobs=-1)\n",
        "dat_resaXiso=isomap.fit_transform(dat_resaX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNVcHUo4QRD6",
        "outputId": "34e4a004-2c2c-4736-826f-0b4fd74b6633"
      },
      "source": [
        "#isomap.reconstruction_error()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "679.0982026229731"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4cuzmYbQRD_"
      },
      "source": [
        "#### train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOyC8lmBQRD_"
      },
      "source": [
        "if cnt==3:\n",
        " dat_resaXtr, dat_resaXts, dat_resaYtr, dat_resaYts = train_test_split(dat_resa.drop(['Disease'],axis=1),dat_resa['Disease'], \n",
        "                                                    train_size=0.75, \n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=111,\n",
        "                                                    shuffle=True, \n",
        "                                                    stratify=dat_resa['Disease'])\n",
        " scaler=MinMaxScaler()\n",
        " dat_resaXtr=scaler.fit(dat_resaXtr).transform(dat_resaXtr)\n",
        " dat_resaXts=scaler.fit(dat_resaXts).transform(dat_resaXts)\n",
        " over_sampler = SMOTE(k_neighbors=10, sampling_strategy=\"auto\")\n",
        " dat_resaXtr, dat_resaYtr = over_sampler.fit_resample(dat_resaXtr,dat_resaYtr)\n",
        " dat_resaXts, dat_resaYts = over_sampler.fit_resample(dat_resaXts,dat_resaYts)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-euUVB3WQRD_"
      },
      "source": [
        "##### selectkbest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q5JDmkNQRD_",
        "outputId": "89d678bb-0b06-4557-ef63-d60422097d83"
      },
      "source": [
        "if cnt==3: \n",
        " selector = SelectKBest(score_func=f_classif, k=100)\n",
        " dat_resaXredtr=selector.fit(dat_resaXtr,dat_resaYtr).transform(dat_resaXtr)\n",
        " dat_resaXredts=selector.fit(dat_resaXtr,dat_resaYtr).transform(dat_resaXts)\n",
        " dat_resaXredts\n",
        " selector.pvalues_\n",
        " selector.scores_"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.79803948, 0.80455227, 0.7302466 , ..., 0.3679139 , 0.10473988,\n",
              "        0.57993868],\n",
              "       [0.46971935, 0.49434374, 0.85505788, ..., 0.02427731, 0.0950289 ,\n",
              "        0.55102935],\n",
              "       [0.03853901, 0.07591659, 0.0721691 , ..., 0.78162933, 0.56716763,\n",
              "        0.07139728],\n",
              "       ...,\n",
              "       [0.22232832, 0.30460252, 0.4757108 , ..., 0.42406636, 0.11894809,\n",
              "        0.16608992],\n",
              "       [0.12924588, 0.12434546, 0.46599136, ..., 0.10740756, 0.08361009,\n",
              "        0.111306  ],\n",
              "       [0.78004902, 0.55958641, 0.67469302, ..., 0.06977803, 0.06838756,\n",
              "        0.27139907]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ncij0wQQ0Fd"
      },
      "source": [
        "##### pca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDgCecpnRSSY"
      },
      "source": [
        "if cnt==3: \n",
        " pca_transformer = PCA(n_components=100)\n",
        " dat_resaXpca1=pca_transformer.fit(dat_resaXtr)\n",
        " dat_resaXpcatr=dat_resaXpca1.transform(dat_resaXtr)\n",
        " dat_resaXpcats=dat_resaXpca1.transform(dat_resaXts)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4btN_B_RSSZ",
        "outputId": "5b7cc34f-1621-43dc-ea28-f921d3d67251"
      },
      "source": [
        "if cnt==3: \n",
        " dat_resaXpca1.explained_variance_ratio_"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.13876115, 0.08577386, 0.0778555 , 0.05080879, 0.03836049,\n",
              "       0.02655832, 0.02513406, 0.02281309, 0.02091889, 0.01797797,\n",
              "       0.01507291, 0.0144946 , 0.01261146, 0.01077705, 0.01001338,\n",
              "       0.00873718, 0.00793486, 0.00768099, 0.00728374, 0.00689386,\n",
              "       0.00681306, 0.00616911, 0.00584839, 0.00564056, 0.00517347,\n",
              "       0.00491148, 0.00476871, 0.00424947, 0.00409763, 0.00383349,\n",
              "       0.00370612, 0.00346322, 0.00331437, 0.00320648, 0.00307764,\n",
              "       0.0030087 , 0.00283103, 0.00276338, 0.00271684, 0.00268082,\n",
              "       0.00252067, 0.00245223, 0.002362  , 0.00226517, 0.00218553,\n",
              "       0.00214054, 0.00202259, 0.00189838, 0.00186829, 0.00180813,\n",
              "       0.0017653 , 0.00175738, 0.00171714, 0.0016734 , 0.00163651,\n",
              "       0.00157304, 0.00153831, 0.00147837, 0.00147028, 0.00144841,\n",
              "       0.00139536, 0.00138162, 0.00136122, 0.00134841, 0.00129679,\n",
              "       0.00128302, 0.00124214, 0.00122408, 0.00120589, 0.00117731,\n",
              "       0.00116656, 0.00113916, 0.00110295, 0.0010842 , 0.00107527,\n",
              "       0.0010564 , 0.0010491 , 0.00101805, 0.00099821, 0.00098756,\n",
              "       0.00096732, 0.00094612, 0.00093525, 0.00092819, 0.00091265,\n",
              "       0.00090219, 0.00087245, 0.0008702 , 0.00085013, 0.00083393,\n",
              "       0.00081676, 0.00081498, 0.0008099 , 0.00080003, 0.000775  ,\n",
              "       0.00077014, 0.00076222, 0.00074694, 0.00073473, 0.00071713])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziAcplkeRSSa",
        "outputId": "00fe0bb6-5144-40af-a2f6-f77acc6fa531"
      },
      "source": [
        "if cnt==3: \n",
        " dat_resaXpca1.explained_variance_ratio_.cumsum()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.13876115, 0.22453501, 0.30239051, 0.3531993 , 0.39155979,\n",
              "       0.4181181 , 0.44325217, 0.46606526, 0.48698414, 0.50496211,\n",
              "       0.52003502, 0.53452962, 0.54714108, 0.55791813, 0.56793152,\n",
              "       0.57666869, 0.58460355, 0.59228454, 0.59956828, 0.60646214,\n",
              "       0.6132752 , 0.61944431, 0.6252927 , 0.63093326, 0.63610673,\n",
              "       0.64101821, 0.64578692, 0.65003638, 0.65413401, 0.65796751,\n",
              "       0.66167363, 0.66513685, 0.66845122, 0.6716577 , 0.67473534,\n",
              "       0.67774404, 0.68057507, 0.68333846, 0.68605529, 0.68873611,\n",
              "       0.69125679, 0.69370901, 0.69607101, 0.69833618, 0.70052171,\n",
              "       0.70266225, 0.70468483, 0.70658321, 0.7084515 , 0.71025963,\n",
              "       0.71202493, 0.71378231, 0.71549944, 0.71717284, 0.71880935,\n",
              "       0.72038239, 0.7219207 , 0.72339907, 0.72486935, 0.72631777,\n",
              "       0.72771313, 0.72909474, 0.73045597, 0.73180438, 0.73310117,\n",
              "       0.73438419, 0.73562633, 0.7368504 , 0.73805629, 0.7392336 ,\n",
              "       0.74040016, 0.74153932, 0.74264227, 0.74372647, 0.74480175,\n",
              "       0.74585815, 0.74690725, 0.7479253 , 0.7489235 , 0.74991107,\n",
              "       0.75087839, 0.75182451, 0.75275976, 0.75368795, 0.75460059,\n",
              "       0.75550279, 0.75637524, 0.75724543, 0.75809557, 0.75892949,\n",
              "       0.75974625, 0.76056123, 0.76137114, 0.76217116, 0.76294617,\n",
              "       0.76371631, 0.76447853, 0.76522546, 0.76596019, 0.76667732])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHznZnCLRSSa",
        "outputId": "d156f726-6e38-4caa-a971-b3a079656088"
      },
      "source": [
        "if cnt==3: \n",
        " dat_resaXpcats.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3772, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sp03pCjJ_Wn"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgBlTih_eAD2"
      },
      "source": [
        "Note:Choose only one type of datasets for model calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDo7yuF8KGge"
      },
      "source": [
        "### dataset1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPitm5FJMXC5"
      },
      "source": [
        "#### selectkbest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75V4AW8xLYxn"
      },
      "source": [
        "##### logistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaIaZJbPojuF"
      },
      "source": [
        "my_param_grid = [\n",
        "    {'solver': ['newton-cg', 'lbfgs', 'saga'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l2'], 'max_iter': [200]},\n",
        "    {'solver': ['liblinear'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l1', 'l2'], 'max_iter': [200]}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e71HzhZ9ojzt"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5Z56Km8n7o1"
      },
      "source": [
        "modellr = GridSearchCV(estimator=LogisticRegression(n_jobs=-1), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgarZbaJRQw0",
        "outputId": "4f569c85-dfcc-4547-d364-9823144e1ab3"
      },
      "source": [
        "modellr.fit(dat_resaXredtr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid=[{'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l2'],\n",
              "                          'solver': ['newton-cg', 'lbfgs', 'saga']},\n",
              "                         {'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l1', 'l2'], 'solver': ['liblinear']}],\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_log_loss', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ihOqb6ExUlE",
        "outputId": "44d7443c-43a0-41aa-98b9-b42eefd83be8"
      },
      "source": [
        "print(cross_val_score(modellr.best_estimator_,dat_resaXredtr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[-0.0363134  -0.0380124  -0.00484746 -0.00930007 -0.01160805 -0.02015559\n",
            " -0.02548266 -0.01851644 -0.03671539 -0.04070464 -0.02369346 -0.02609461\n",
            " -0.01096843 -0.02094147 -0.02011011 -0.02900834 -0.01348083 -0.01038949\n",
            " -0.04458787 -0.02176768 -0.01855972 -0.02918603 -0.0121573  -0.01156958\n",
            " -0.03347989 -0.01388913 -0.00801193 -0.02821159 -0.01028519 -0.03241766\n",
            " -0.03241649 -0.01940683 -0.0195101  -0.01655555 -0.01811073 -0.07047968\n",
            " -0.02350919 -0.02435546 -0.02214303 -0.014168   -0.03482114 -0.00570531\n",
            " -0.03752542 -0.02664255 -0.01518329 -0.01350785 -0.02347524 -0.02762231\n",
            " -0.01329772 -0.02986335]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tahK7Xfyq4s7",
        "outputId": "9d374e32-5027-47a2-9c47-220cf4c76cb4"
      },
      "source": [
        "modellr.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.022990238309843182"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZakeO0NS30t",
        "outputId": "ea2297f1-ff4d-4505-bb09-6a514e716db5"
      },
      "source": [
        "log_loss(dat_resaYts, modellr.predict_proba(dat_resaXredts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.024132557781455937"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOEj7J-5TAwv"
      },
      "source": [
        "modellr_best = modellr.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwcGBcOGUd-E",
        "outputId": "cf7277e4-8771-474f-92dd-109d318c512b"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modellr_best.predict(dat_resaXredtr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       1.00      1.00      1.00       730\n",
            "        breast adenocarcinoma       1.00      1.00      1.00       730\n",
            "                breast cancer       1.00      1.00      1.00       730\n",
            "chronic lymphocytic leukaemia       1.00      1.00      1.00       730\n",
            "diffuse large B-cell lymphoma       1.00      1.00      1.00       730\n",
            "             multiple myeloma       1.00      1.00      1.00       730\n",
            "\n",
            "                     accuracy                           1.00      4380\n",
            "                    macro avg       1.00      1.00      1.00      4380\n",
            "                 weighted avg       1.00      1.00      1.00      4380\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh4pWNphTL3w",
        "outputId": "cb4c163f-69d2-484e-b4a0-7c3ecc1136fb"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modellr_best.predict(dat_resaXredts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.99      1.00      0.99       244\n",
            "        breast adenocarcinoma       0.99      1.00      0.99       244\n",
            "                breast cancer       0.99      0.99      0.99       244\n",
            "chronic lymphocytic leukaemia       1.00      0.99      1.00       244\n",
            "diffuse large B-cell lymphoma       1.00      1.00      1.00       244\n",
            "             multiple myeloma       1.00      1.00      1.00       244\n",
            "\n",
            "                     accuracy                           1.00      1464\n",
            "                    macro avg       1.00      1.00      1.00      1464\n",
            "                 weighted avg       1.00      1.00      1.00      1464\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQrmEIL7MbZ-"
      },
      "source": [
        "##### random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h7LUeLmvqAf"
      },
      "source": [
        "my_param_grid = {'bootstrap': [True, False], \n",
        "                 'n_estimators': [10, 50], \n",
        "                 'min_samples_leaf': [20, 40, 60],\n",
        "                 'min_weight_fraction_leaf': [0.01, 0.02, 0.05],\n",
        "                 'criterion': ['gini', 'entropy'], \n",
        "                 'min_impurity_decrease': [1e-5, 1e-6, 1e-7]\n",
        "                 }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngZubKXPvqAk"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kdO4sdSvqAq"
      },
      "source": [
        "modelrf = GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1,warm_start=True), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJiINZdYdb_6",
        "outputId": "37cea925-47b4-49c3-8308-f15c62e71cf4"
      },
      "source": [
        "modelrf.fit(dat_resaXredtr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=50, n_splits=5, random_state=111),\n",
              "             estimator=RandomForestClassifier(n_jobs=-1, warm_start=True),\n",
              "             param_grid={'bootstrap': [True, False],\n",
              "                         'criterion': ['gini', 'entropy'],\n",
              "                         'min_impurity_decrease': [1e-05, 1e-06, 1e-07],\n",
              "                         'min_samples_leaf': [20, 40, 60],\n",
              "                         'min_weight_fraction_leaf': [0.01, 0.02, 0.05],\n",
              "                         'n_estimators': [10, 50]},\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UjlcdoIdJWm",
        "outputId": "056737f1-6cf6-4982-d422-729438de96f3"
      },
      "source": [
        "print(cross_val_score(modelrf.best_estimator_,dat_resaXredtr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.09140118 -0.06926422 -0.04841523 -0.06568339 -0.06783376 -0.06590496\n",
            " -0.07309382 -0.06900615 -0.0749752  -0.07352021 -0.0772558  -0.0763306\n",
            " -0.06143393 -0.06554341 -0.06265877 -0.07521142 -0.07227784 -0.0614367\n",
            " -0.06902609 -0.06360118 -0.06047002 -0.0699813  -0.05746229 -0.07113202\n",
            " -0.0819119  -0.06914228 -0.05650931 -0.08305982 -0.05632511 -0.07999056\n",
            " -0.06030609 -0.08121499 -0.072297   -0.06305989 -0.06486845 -0.11666555\n",
            " -0.06563971 -0.07266512 -0.06390953 -0.05998954 -0.07138692 -0.05748581\n",
            " -0.108854   -0.07461815 -0.0716482  -0.06508477 -0.06315274 -0.07834213\n",
            " -0.07525233 -0.06182582 -0.05245695 -0.07410859 -0.08785026 -0.10135336\n",
            " -0.06490007 -0.07048392 -0.06795742 -0.06377672 -0.07574584 -0.07202296\n",
            " -0.06640576 -0.07639656 -0.0664038  -0.07147906 -0.05941974 -0.08135893\n",
            " -0.06596617 -0.06808207 -0.06512888 -0.05931733 -0.06739793 -0.06122569\n",
            " -0.07740811 -0.05977265 -0.07698124 -0.07398328 -0.06787541 -0.0587436\n",
            " -0.07003473 -0.07327822 -0.06012174 -0.06742982 -0.07118737 -0.06966781\n",
            " -0.0785114  -0.05630168 -0.06848752 -0.07858123 -0.07708666 -0.06395352\n",
            " -0.07070417 -0.07012215 -0.06001504 -0.06660609 -0.07233629 -0.06294005\n",
            " -0.06839865 -0.08318712 -0.07002191 -0.06359626 -0.06865323 -0.06273357\n",
            " -0.10418141 -0.07896745 -0.05340138 -0.06528897 -0.05823654 -0.08522437\n",
            " -0.06091983 -0.07412301 -0.07015088 -0.07933711 -0.07068598 -0.05709004\n",
            " -0.06520203 -0.10229768 -0.06523155 -0.07464688 -0.0765808  -0.06500676\n",
            " -0.06349165 -0.06680725 -0.0677706  -0.10047263 -0.07271105 -0.11870644\n",
            " -0.06093202 -0.0663591  -0.05606944 -0.07457806 -0.10558661 -0.060227\n",
            " -0.06922817 -0.06650694 -0.07219811 -0.06978706 -0.05968734 -0.0809187\n",
            " -0.06403789 -0.0727558  -0.070237   -0.06830947 -0.05815397 -0.06849847\n",
            " -0.07670297 -0.06529273 -0.08051858 -0.07525833 -0.06012168 -0.05865361\n",
            " -0.0638563  -0.06112267 -0.06892614 -0.07405655 -0.07475136 -0.06404416\n",
            " -0.07050151 -0.06263187 -0.0822828  -0.05723262 -0.06714418 -0.0796045\n",
            " -0.07006359 -0.06563981 -0.0691841  -0.05713742 -0.07370389 -0.07160706\n",
            " -0.07225343 -0.06322203 -0.10572182 -0.06223468 -0.06452119 -0.08009829\n",
            " -0.07114604 -0.06547719 -0.06904004 -0.06272724 -0.06982623 -0.07867853\n",
            " -0.07331919 -0.07680913 -0.06489748 -0.06058455 -0.06083388 -0.06764395\n",
            " -0.07354721 -0.06000473 -0.11687401 -0.06397788 -0.06865974 -0.07425862\n",
            " -0.06831603 -0.09598594 -0.06658807 -0.06880007 -0.06965757 -0.06038494\n",
            " -0.11361378 -0.06245206 -0.07195641 -0.05845411 -0.0651146  -0.0751537\n",
            " -0.07188464 -0.07074785 -0.05364658 -0.07636996 -0.06979896 -0.06737623\n",
            " -0.06849711 -0.07641781 -0.06281796 -0.07013137 -0.06146245 -0.06107909\n",
            " -0.06786187 -0.07468248 -0.07769983 -0.06297884 -0.06630795 -0.06751237\n",
            " -0.06430183 -0.07530363 -0.07082462 -0.08231074 -0.05985814 -0.07864225\n",
            " -0.06745594 -0.05641307 -0.06601807 -0.06064721 -0.07195327 -0.07029983\n",
            " -0.07803036 -0.07211272 -0.07266552 -0.07393543 -0.06588858 -0.06293474\n",
            " -0.06170588 -0.07133972 -0.07522115 -0.06967468 -0.06324508 -0.071205\n",
            " -0.06153876 -0.07148699 -0.09801043 -0.06763302]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp11K8QtE-UU",
        "outputId": "b9654be3-fe8a-4e62-e5da-0749e22f1e8a"
      },
      "source": [
        "-modelrf.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07049632542201298"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UosAzqjE-UU",
        "outputId": "1774f8ef-2ae6-41e9-d7f2-1741a2d38256"
      },
      "source": [
        "log_loss(dat_resaYts, modelrf.predict_proba(dat_resaXredts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07095982458036235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iIsXnCfE-UU"
      },
      "source": [
        "modelrf_best = modelrf.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUZJzvUXE-UU",
        "outputId": "4aad4bd3-32f3-4a7d-d121-b0fcb8720393"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modelrf_best.predict(dat_resaXredtr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.98      0.99      0.99       730\n",
            "        breast adenocarcinoma       0.98      1.00      0.99       730\n",
            "                breast cancer       0.99      0.97      0.98       730\n",
            "chronic lymphocytic leukaemia       1.00      1.00      1.00       730\n",
            "diffuse large B-cell lymphoma       0.99      0.99      0.99       730\n",
            "             multiple myeloma       1.00      0.98      0.99       730\n",
            "\n",
            "                     accuracy                           0.99      4380\n",
            "                    macro avg       0.99      0.99      0.99      4380\n",
            "                 weighted avg       0.99      0.99      0.99      4380\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKygCG_pE-UU",
        "outputId": "42843cc9-0013-4368-c2e9-00d353638db0"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modelrf_best.predict(dat_resaXredts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.98      1.00      0.99       244\n",
            "        breast adenocarcinoma       0.98      1.00      0.99       244\n",
            "                breast cancer       0.98      0.97      0.98       244\n",
            "chronic lymphocytic leukaemia       1.00      0.99      1.00       244\n",
            "diffuse large B-cell lymphoma       0.99      1.00      0.99       244\n",
            "             multiple myeloma       1.00      0.98      0.99       244\n",
            "\n",
            "                     accuracy                           0.99      1464\n",
            "                    macro avg       0.99      0.99      0.99      1464\n",
            "                 weighted avg       0.99      0.99      0.99      1464\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQa0AtmiMhQG"
      },
      "source": [
        "##### gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy7t4J6j0eOE"
      },
      "source": [
        "my_param_grid = {\n",
        "    \"loss\":[\"deviance\"],\n",
        "    \"learning_rate\": [0.2],\n",
        "    \"min_samples_split\": np.linspace(0.1,0.5, 1),\n",
        "    \"min_samples_leaf\": np.linspace(0.1,0.5, 1),\n",
        "    \"max_depth\":[3,8],\n",
        "    \"max_features\":[\"log2\",\"sqrt\"],\n",
        "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
        "    \"subsample\":[0.8],\n",
        "    \"n_estimators\":[30]\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTKGrTGn0eOK"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR67gaOs0eOO"
      },
      "source": [
        "modelxgb = GridSearchCV(estimator=GradientBoostingClassifier(warm_start=True), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0z5of2tFx0S",
        "outputId": "f81b1be0-2b5b-4493-a63d-159d25c526d3"
      },
      "source": [
        "modelxgb.fit(dat_resaXredtr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=50, n_splits=5, random_state=111),\n",
              "             estimator=GradientBoostingClassifier(),\n",
              "             param_grid={'criterion': ['friedman_mse', 'mae'],\n",
              "                         'learning_rate': [0.2], 'loss': ['deviance'],\n",
              "                         'max_depth': [3, 8], 'max_features': ['log2', 'sqrt'],\n",
              "                         'min_samples_leaf': array([0.1]),\n",
              "                         'min_samples_split': array([0.1]),\n",
              "                         'n_estimators': [30], 'subsample': [0.8]},\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enk-NTuHFx0S",
        "outputId": "d035a85c-f2c1-43b4-c763-f8aa6fc1e5c4"
      },
      "source": [
        "print(cross_val_score(modelxgb.best_estimator_,dat_resaXredtr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.04824362 -0.03405919 -0.0122662  -0.01400052 -0.02308301 -0.02502405\n",
            " -0.02931554 -0.02226629 -0.03460429 -0.0456565  -0.02417366 -0.03798492\n",
            " -0.03027392 -0.02799129 -0.01905308 -0.03438877 -0.0219781  -0.02178417\n",
            " -0.03182891 -0.02241512 -0.02064743 -0.03626127 -0.01937369 -0.02859476\n",
            " -0.03720921 -0.0239959  -0.01343536 -0.03184108 -0.01330487 -0.04021115\n",
            " -0.01934534 -0.03083811 -0.02874769 -0.02922196 -0.02513203 -0.06077267\n",
            " -0.02391614 -0.02421741 -0.02201866 -0.02358198 -0.03100271 -0.02570454\n",
            " -0.04209114 -0.02816696 -0.02665368 -0.02900711 -0.02522384 -0.03322899\n",
            " -0.03202862 -0.02617575 -0.02299385 -0.03232979 -0.03160586 -0.03127332\n",
            " -0.01533528 -0.03779594 -0.02212962 -0.01270225 -0.03028782 -0.02489295\n",
            " -0.02160037 -0.02844634 -0.02287466 -0.0386932  -0.02029997 -0.03874164\n",
            " -0.02494383 -0.02431737 -0.02520614 -0.0137477  -0.01679336 -0.01933095\n",
            " -0.03724437 -0.02137508 -0.03426215 -0.01907608 -0.0346854  -0.01460412\n",
            " -0.03615428 -0.03714719 -0.01778183 -0.01389671 -0.03280105 -0.02154341\n",
            " -0.03997915 -0.01438449 -0.04241143 -0.03049461 -0.03675451 -0.01855288\n",
            " -0.02900317 -0.03894747 -0.01768277 -0.02519129 -0.03183892 -0.02467645\n",
            " -0.02467504 -0.03490415 -0.02751729 -0.02418455 -0.01313256 -0.02117996\n",
            " -0.02939612 -0.05470994 -0.00852497 -0.02451867 -0.01763593 -0.0669578\n",
            " -0.0115117  -0.0304749  -0.03457715 -0.03256525 -0.02809171 -0.01468446\n",
            " -0.02749094 -0.02773017 -0.02219011 -0.02207496 -0.03905215 -0.01944144\n",
            " -0.01858889 -0.02529383 -0.02349555 -0.02914349 -0.03770499 -0.04387324\n",
            " -0.01627857 -0.01862047 -0.02555729 -0.03448327 -0.03943342 -0.01549364\n",
            " -0.02775541 -0.03305857 -0.02257948 -0.02339344 -0.02038777 -0.05158358\n",
            " -0.02406277 -0.02385512 -0.02933358 -0.0314253  -0.01878111 -0.02153951\n",
            " -0.02733099 -0.02501234 -0.03960661 -0.04366603 -0.02278462 -0.01480033\n",
            " -0.0273807  -0.01376483 -0.03334679 -0.02832474 -0.0446252  -0.01889493\n",
            " -0.04112306 -0.01643055 -0.0404314  -0.01760942 -0.01543558 -0.04679403\n",
            " -0.02633823 -0.02374664 -0.01859522 -0.01964755 -0.02115969 -0.02254931\n",
            " -0.04326478 -0.02921125 -0.03134689 -0.01553577 -0.0213561  -0.04427417\n",
            " -0.024915   -0.02772137 -0.03283722 -0.01473348 -0.02449231 -0.03826688\n",
            " -0.0343187  -0.03983723 -0.02420283 -0.01436851 -0.02840913 -0.02113508\n",
            " -0.02853854 -0.01855496 -0.05041574 -0.01778904 -0.02716251 -0.03961022\n",
            " -0.01884865 -0.03027913 -0.01702354 -0.05411151 -0.02590299 -0.02275234\n",
            " -0.03633407 -0.02090527 -0.03210271 -0.02588757 -0.0194497  -0.02611435\n",
            " -0.03224906 -0.027938   -0.01424118 -0.02963947 -0.02829704 -0.03386656\n",
            " -0.02848164 -0.02794291 -0.02668429 -0.0253254  -0.01784827 -0.01724382\n",
            " -0.02550183 -0.03462502 -0.04386467 -0.02267678 -0.01981162 -0.02828145\n",
            " -0.02383736 -0.02444722 -0.03587644 -0.04419776 -0.01826998 -0.03197326\n",
            " -0.02090226 -0.01663065 -0.03335218 -0.01683221 -0.03780779 -0.0287545\n",
            " -0.03549165 -0.0320057  -0.02622803 -0.0321537  -0.01730006 -0.03080706\n",
            " -0.0152937  -0.03068356 -0.02826706 -0.04141053 -0.01911069 -0.03407838\n",
            " -0.02268214 -0.02177159 -0.03902851 -0.03083398]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v-N6RskFx0S",
        "outputId": "19e76798-51af-4b1b-a345-a89cf41796f9"
      },
      "source": [
        "-modelxgb.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.027669222518667002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt8apWD0Fx0S",
        "outputId": "044d0616-09bf-4bb5-ba78-3b3ae0f27854"
      },
      "source": [
        "log_loss(dat_resaYts, modelxgb.predict_proba(dat_resaXredts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03281231584747478"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDGlAmzJFx0T"
      },
      "source": [
        "modelxgb_best = modelxgb.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGfl7O0JFx0T",
        "outputId": "98175c9b-b4fb-4237-a763-ccdffd9649f1"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modelxgb_best.predict(dat_resaXredtr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       1.00      1.00      1.00       730\n",
            "        breast adenocarcinoma       1.00      1.00      1.00       730\n",
            "                breast cancer       1.00      0.99      1.00       730\n",
            "chronic lymphocytic leukaemia       1.00      1.00      1.00       730\n",
            "diffuse large B-cell lymphoma       1.00      1.00      1.00       730\n",
            "             multiple myeloma       1.00      1.00      1.00       730\n",
            "\n",
            "                     accuracy                           1.00      4380\n",
            "                    macro avg       1.00      1.00      1.00      4380\n",
            "                 weighted avg       1.00      1.00      1.00      4380\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q31KIhvyFx0T",
        "outputId": "07592741-02de-4b9b-807d-e73db7d81177"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modelxgb_best.predict(dat_resaXredts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.98      1.00      0.99       244\n",
            "        breast adenocarcinoma       0.99      1.00      0.99       244\n",
            "                breast cancer       0.98      0.98      0.98       244\n",
            "chronic lymphocytic leukaemia       1.00      0.99      1.00       244\n",
            "diffuse large B-cell lymphoma       1.00      1.00      1.00       244\n",
            "             multiple myeloma       1.00      0.98      0.99       244\n",
            "\n",
            "                     accuracy                           0.99      1464\n",
            "                    macro avg       0.99      0.99      0.99      1464\n",
            "                 weighted avg       0.99      0.99      0.99      1464\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUNrvH1uL0rR"
      },
      "source": [
        "##### fully connected ffnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KJOLXca3dhr",
        "outputId": "67977621-39d6-458b-a69a-1d2545accfcd"
      },
      "source": [
        "dat_resaYtr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['breast adenocarcinoma', 'acute myeloid leukaemia',\n",
              "       'diffuse large B-cell lymphoma', ..., 'multiple myeloma',\n",
              "       'multiple myeloma', 'multiple myeloma'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5x1p7T_u_ZV"
      },
      "source": [
        "num_classes=6\n",
        "dat_resaYtrx=pd.DataFrame(dat_resaYtr)\n",
        "dat_resaYtrx[0] = pd.Categorical(dat_resaYtrx[0])\n",
        "dat_resaYtrx['code'] = dat_resaYtrx[0].cat.codes\n",
        "dat_resaYtrx_=np.array(dat_resaYtrx['code']\n",
        "                       )\n",
        "dat_resaYtsx=pd.DataFrame(dat_resaYts)\n",
        "dat_resaYtsx[0] = pd.Categorical(dat_resaYtsx[0])\n",
        "dat_resaYtsx['code'] = dat_resaYtsx[0].cat.codes\n",
        "dat_resaYtsx_=np.array(dat_resaYtsx['code'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m90MfIFVgz_8",
        "outputId": "a37af19a-045a-493d-fb5b-d63cbe7d86da"
      },
      "source": [
        "dat_resaYtrcl = to_categorical(dat_resaYtrx_, num_classes)\n",
        "dat_resaYtscl = to_categorical(dat_resaYtsx_, num_classes)\n",
        "dat_resaYtrcl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anlvUD7iYIXg"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(30, activation='relu', input_shape=(60,)))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxFCEi_ZaJ1s"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMWWShx3F5vI",
        "outputId": "6c19242e-100b-4fa7-a0d3-1825c239b8dd"
      },
      "source": [
        "history_ = model.fit(dat_resaXredtr, dat_resaYtrcl, \n",
        "                            batch_size=2000, epochs=500, verbose=1, validation_split=0.2)\n",
        " \n",
        "score_ = model.evaluate(dat_resaXredts,dat_resaYtscl , verbose=0)\n",
        "print('\\nScore: ', score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 1.8251 - accuracy: 0.1895 - val_loss: 1.8364 - val_accuracy: 0.0491\n",
            "Epoch 2/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.7919 - accuracy: 0.2126 - val_loss: 1.8266 - val_accuracy: 0.0434\n",
            "Epoch 3/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.7634 - accuracy: 0.2671 - val_loss: 1.8166 - val_accuracy: 0.0411\n",
            "Epoch 4/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.7326 - accuracy: 0.3422 - val_loss: 1.8067 - val_accuracy: 0.0411\n",
            "Epoch 5/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.7038 - accuracy: 0.4055 - val_loss: 1.7982 - val_accuracy: 0.0422\n",
            "Epoch 6/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6775 - accuracy: 0.4406 - val_loss: 1.7919 - val_accuracy: 0.0457\n",
            "Epoch 7/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6451 - accuracy: 0.4709 - val_loss: 1.7887 - val_accuracy: 0.0445\n",
            "Epoch 8/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6146 - accuracy: 0.4709 - val_loss: 1.7888 - val_accuracy: 0.0285\n",
            "Epoch 9/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5845 - accuracy: 0.4809 - val_loss: 1.7911 - val_accuracy: 0.0240\n",
            "Epoch 10/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5554 - accuracy: 0.4866 - val_loss: 1.7949 - val_accuracy: 0.0217\n",
            "Epoch 11/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5278 - accuracy: 0.4923 - val_loss: 1.7981 - val_accuracy: 0.0126\n",
            "Epoch 12/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4993 - accuracy: 0.5157 - val_loss: 1.8000 - val_accuracy: 0.0057\n",
            "Epoch 13/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4700 - accuracy: 0.5277 - val_loss: 1.8003 - val_accuracy: 0.0046\n",
            "Epoch 14/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4492 - accuracy: 0.5488 - val_loss: 1.7985 - val_accuracy: 0.0034\n",
            "Epoch 15/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4181 - accuracy: 0.5702 - val_loss: 1.7952 - val_accuracy: 0.0011\n",
            "Epoch 16/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3879 - accuracy: 0.5811 - val_loss: 1.7912 - val_accuracy: 0.0023\n",
            "Epoch 17/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3634 - accuracy: 0.5933 - val_loss: 1.7868 - val_accuracy: 0.0023\n",
            "Epoch 18/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3338 - accuracy: 0.5967 - val_loss: 1.7820 - val_accuracy: 0.0023\n",
            "Epoch 19/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.3128 - accuracy: 0.6036 - val_loss: 1.7767 - val_accuracy: 0.0046\n",
            "Epoch 20/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.2907 - accuracy: 0.6090 - val_loss: 1.7697 - val_accuracy: 0.0046\n",
            "Epoch 21/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.2651 - accuracy: 0.6045 - val_loss: 1.7614 - val_accuracy: 0.0091\n",
            "Epoch 22/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2372 - accuracy: 0.6124 - val_loss: 1.7514 - val_accuracy: 0.0183\n",
            "Epoch 23/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2146 - accuracy: 0.6139 - val_loss: 1.7395 - val_accuracy: 0.0365\n",
            "Epoch 24/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1968 - accuracy: 0.6221 - val_loss: 1.7262 - val_accuracy: 0.0605\n",
            "Epoch 25/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1782 - accuracy: 0.6250 - val_loss: 1.7117 - val_accuracy: 0.0788\n",
            "Epoch 26/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1572 - accuracy: 0.6333 - val_loss: 1.6953 - val_accuracy: 0.0970\n",
            "Epoch 27/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1344 - accuracy: 0.6407 - val_loss: 1.6772 - val_accuracy: 0.1187\n",
            "Epoch 28/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1164 - accuracy: 0.6461 - val_loss: 1.6582 - val_accuracy: 0.1358\n",
            "Epoch 29/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0988 - accuracy: 0.6504 - val_loss: 1.6382 - val_accuracy: 0.1598\n",
            "Epoch 30/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0788 - accuracy: 0.6627 - val_loss: 1.6173 - val_accuracy: 0.2009\n",
            "Epoch 31/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0648 - accuracy: 0.6635 - val_loss: 1.5960 - val_accuracy: 0.2432\n",
            "Epoch 32/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0434 - accuracy: 0.6729 - val_loss: 1.5747 - val_accuracy: 0.2785\n",
            "Epoch 33/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0340 - accuracy: 0.6729 - val_loss: 1.5531 - val_accuracy: 0.3196\n",
            "Epoch 34/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0076 - accuracy: 0.6855 - val_loss: 1.5312 - val_accuracy: 0.3619\n",
            "Epoch 35/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9925 - accuracy: 0.6892 - val_loss: 1.5083 - val_accuracy: 0.4178\n",
            "Epoch 36/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9710 - accuracy: 0.6955 - val_loss: 1.4836 - val_accuracy: 0.4726\n",
            "Epoch 37/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9483 - accuracy: 0.7052 - val_loss: 1.4583 - val_accuracy: 0.5011\n",
            "Epoch 38/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9294 - accuracy: 0.7132 - val_loss: 1.4331 - val_accuracy: 0.5308\n",
            "Epoch 39/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9156 - accuracy: 0.7118 - val_loss: 1.4058 - val_accuracy: 0.5628\n",
            "Epoch 40/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9029 - accuracy: 0.7197 - val_loss: 1.3760 - val_accuracy: 0.6062\n",
            "Epoch 41/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8734 - accuracy: 0.7255 - val_loss: 1.3450 - val_accuracy: 0.6461\n",
            "Epoch 42/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8536 - accuracy: 0.7363 - val_loss: 1.3126 - val_accuracy: 0.6724\n",
            "Epoch 43/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8382 - accuracy: 0.7394 - val_loss: 1.2781 - val_accuracy: 0.7100\n",
            "Epoch 44/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8140 - accuracy: 0.7520 - val_loss: 1.2430 - val_accuracy: 0.7306\n",
            "Epoch 45/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8027 - accuracy: 0.7483 - val_loss: 1.2059 - val_accuracy: 0.7489\n",
            "Epoch 46/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7788 - accuracy: 0.7634 - val_loss: 1.1687 - val_accuracy: 0.7626\n",
            "Epoch 47/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7550 - accuracy: 0.7637 - val_loss: 1.1316 - val_accuracy: 0.7705\n",
            "Epoch 48/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7334 - accuracy: 0.7708 - val_loss: 1.0937 - val_accuracy: 0.7797\n",
            "Epoch 49/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7169 - accuracy: 0.7725 - val_loss: 1.0548 - val_accuracy: 0.7842\n",
            "Epoch 50/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7004 - accuracy: 0.7817 - val_loss: 1.0159 - val_accuracy: 0.7991\n",
            "Epoch 51/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6793 - accuracy: 0.7971 - val_loss: 0.9776 - val_accuracy: 0.8413\n",
            "Epoch 52/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6509 - accuracy: 0.8114 - val_loss: 0.9393 - val_accuracy: 0.8824\n",
            "Epoch 53/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6391 - accuracy: 0.8185 - val_loss: 0.9021 - val_accuracy: 0.9098\n",
            "Epoch 54/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6184 - accuracy: 0.8373 - val_loss: 0.8658 - val_accuracy: 0.9281\n",
            "Epoch 55/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5921 - accuracy: 0.8545 - val_loss: 0.8290 - val_accuracy: 0.9452\n",
            "Epoch 56/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5796 - accuracy: 0.8610 - val_loss: 0.7917 - val_accuracy: 0.9589\n",
            "Epoch 57/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5520 - accuracy: 0.8724 - val_loss: 0.7551 - val_accuracy: 0.9680\n",
            "Epoch 58/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5362 - accuracy: 0.8784 - val_loss: 0.7200 - val_accuracy: 0.9737\n",
            "Epoch 59/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5146 - accuracy: 0.8978 - val_loss: 0.6858 - val_accuracy: 0.9737\n",
            "Epoch 60/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4966 - accuracy: 0.9030 - val_loss: 0.6522 - val_accuracy: 0.9760\n",
            "Epoch 61/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4787 - accuracy: 0.9084 - val_loss: 0.6197 - val_accuracy: 0.9760\n",
            "Epoch 62/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4541 - accuracy: 0.9175 - val_loss: 0.5885 - val_accuracy: 0.9772\n",
            "Epoch 63/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4437 - accuracy: 0.9158 - val_loss: 0.5592 - val_accuracy: 0.9772\n",
            "Epoch 64/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4191 - accuracy: 0.9309 - val_loss: 0.5304 - val_accuracy: 0.9783\n",
            "Epoch 65/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4057 - accuracy: 0.9341 - val_loss: 0.5027 - val_accuracy: 0.9783\n",
            "Epoch 66/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3970 - accuracy: 0.9386 - val_loss: 0.4762 - val_accuracy: 0.9783\n",
            "Epoch 67/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3630 - accuracy: 0.9421 - val_loss: 0.4512 - val_accuracy: 0.9783\n",
            "Epoch 68/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3472 - accuracy: 0.9478 - val_loss: 0.4285 - val_accuracy: 0.9783\n",
            "Epoch 69/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3366 - accuracy: 0.9492 - val_loss: 0.4065 - val_accuracy: 0.9783\n",
            "Epoch 70/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3235 - accuracy: 0.9501 - val_loss: 0.3854 - val_accuracy: 0.9783\n",
            "Epoch 71/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3138 - accuracy: 0.9580 - val_loss: 0.3657 - val_accuracy: 0.9783\n",
            "Epoch 72/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2941 - accuracy: 0.9635 - val_loss: 0.3470 - val_accuracy: 0.9783\n",
            "Epoch 73/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2875 - accuracy: 0.9612 - val_loss: 0.3293 - val_accuracy: 0.9783\n",
            "Epoch 74/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2749 - accuracy: 0.9589 - val_loss: 0.3127 - val_accuracy: 0.9783\n",
            "Epoch 75/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2678 - accuracy: 0.9603 - val_loss: 0.2970 - val_accuracy: 0.9783\n",
            "Epoch 76/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2638 - accuracy: 0.9638 - val_loss: 0.2828 - val_accuracy: 0.9783\n",
            "Epoch 77/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2510 - accuracy: 0.9623 - val_loss: 0.2691 - val_accuracy: 0.9783\n",
            "Epoch 78/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2349 - accuracy: 0.9663 - val_loss: 0.2562 - val_accuracy: 0.9783\n",
            "Epoch 79/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2340 - accuracy: 0.9672 - val_loss: 0.2441 - val_accuracy: 0.9783\n",
            "Epoch 80/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2227 - accuracy: 0.9663 - val_loss: 0.2340 - val_accuracy: 0.9783\n",
            "Epoch 81/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2158 - accuracy: 0.9689 - val_loss: 0.2246 - val_accuracy: 0.9783\n",
            "Epoch 82/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2135 - accuracy: 0.9680 - val_loss: 0.2162 - val_accuracy: 0.9783\n",
            "Epoch 83/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2015 - accuracy: 0.9680 - val_loss: 0.2085 - val_accuracy: 0.9783\n",
            "Epoch 84/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1961 - accuracy: 0.9723 - val_loss: 0.2016 - val_accuracy: 0.9783\n",
            "Epoch 85/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1954 - accuracy: 0.9695 - val_loss: 0.1954 - val_accuracy: 0.9783\n",
            "Epoch 86/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1842 - accuracy: 0.9715 - val_loss: 0.1894 - val_accuracy: 0.9783\n",
            "Epoch 87/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1839 - accuracy: 0.9689 - val_loss: 0.1833 - val_accuracy: 0.9783\n",
            "Epoch 88/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1866 - accuracy: 0.9678 - val_loss: 0.1777 - val_accuracy: 0.9783\n",
            "Epoch 89/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1758 - accuracy: 0.9732 - val_loss: 0.1721 - val_accuracy: 0.9783\n",
            "Epoch 90/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1715 - accuracy: 0.9729 - val_loss: 0.1666 - val_accuracy: 0.9783\n",
            "Epoch 91/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1612 - accuracy: 0.9732 - val_loss: 0.1618 - val_accuracy: 0.9783\n",
            "Epoch 92/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1610 - accuracy: 0.9755 - val_loss: 0.1576 - val_accuracy: 0.9783\n",
            "Epoch 93/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1554 - accuracy: 0.9760 - val_loss: 0.1537 - val_accuracy: 0.9783\n",
            "Epoch 94/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1576 - accuracy: 0.9755 - val_loss: 0.1500 - val_accuracy: 0.9783\n",
            "Epoch 95/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1538 - accuracy: 0.9760 - val_loss: 0.1465 - val_accuracy: 0.9783\n",
            "Epoch 96/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1534 - accuracy: 0.9735 - val_loss: 0.1434 - val_accuracy: 0.9783\n",
            "Epoch 97/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1481 - accuracy: 0.9746 - val_loss: 0.1407 - val_accuracy: 0.9783\n",
            "Epoch 98/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1483 - accuracy: 0.9746 - val_loss: 0.1378 - val_accuracy: 0.9783\n",
            "Epoch 99/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1405 - accuracy: 0.9766 - val_loss: 0.1351 - val_accuracy: 0.9783\n",
            "Epoch 100/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1439 - accuracy: 0.9752 - val_loss: 0.1326 - val_accuracy: 0.9783\n",
            "Epoch 101/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1494 - accuracy: 0.9752 - val_loss: 0.1303 - val_accuracy: 0.9783\n",
            "Epoch 102/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1277 - accuracy: 0.9775 - val_loss: 0.1284 - val_accuracy: 0.9783\n",
            "Epoch 103/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1425 - accuracy: 0.9760 - val_loss: 0.1265 - val_accuracy: 0.9783\n",
            "Epoch 104/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1309 - accuracy: 0.9763 - val_loss: 0.1248 - val_accuracy: 0.9783\n",
            "Epoch 105/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1270 - accuracy: 0.9800 - val_loss: 0.1234 - val_accuracy: 0.9783\n",
            "Epoch 106/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1256 - accuracy: 0.9803 - val_loss: 0.1220 - val_accuracy: 0.9783\n",
            "Epoch 107/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1282 - accuracy: 0.9777 - val_loss: 0.1205 - val_accuracy: 0.9783\n",
            "Epoch 108/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1279 - accuracy: 0.9775 - val_loss: 0.1194 - val_accuracy: 0.9783\n",
            "Epoch 109/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1311 - accuracy: 0.9746 - val_loss: 0.1180 - val_accuracy: 0.9783\n",
            "Epoch 110/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1243 - accuracy: 0.9786 - val_loss: 0.1164 - val_accuracy: 0.9783\n",
            "Epoch 111/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1225 - accuracy: 0.9772 - val_loss: 0.1146 - val_accuracy: 0.9783\n",
            "Epoch 112/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1223 - accuracy: 0.9797 - val_loss: 0.1129 - val_accuracy: 0.9783\n",
            "Epoch 113/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1180 - accuracy: 0.9797 - val_loss: 0.1112 - val_accuracy: 0.9783\n",
            "Epoch 114/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1220 - accuracy: 0.9786 - val_loss: 0.1099 - val_accuracy: 0.9783\n",
            "Epoch 115/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1149 - accuracy: 0.9797 - val_loss: 0.1088 - val_accuracy: 0.9783\n",
            "Epoch 116/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1186 - accuracy: 0.9769 - val_loss: 0.1077 - val_accuracy: 0.9783\n",
            "Epoch 117/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1125 - accuracy: 0.9800 - val_loss: 0.1067 - val_accuracy: 0.9783\n",
            "Epoch 118/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1118 - accuracy: 0.9806 - val_loss: 0.1057 - val_accuracy: 0.9783\n",
            "Epoch 119/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1193 - accuracy: 0.9766 - val_loss: 0.1046 - val_accuracy: 0.9783\n",
            "Epoch 120/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1129 - accuracy: 0.9806 - val_loss: 0.1034 - val_accuracy: 0.9783\n",
            "Epoch 121/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1076 - accuracy: 0.9814 - val_loss: 0.1020 - val_accuracy: 0.9783\n",
            "Epoch 122/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1085 - accuracy: 0.9789 - val_loss: 0.1010 - val_accuracy: 0.9783\n",
            "Epoch 123/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1134 - accuracy: 0.9757 - val_loss: 0.0998 - val_accuracy: 0.9783\n",
            "Epoch 124/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1092 - accuracy: 0.9797 - val_loss: 0.0991 - val_accuracy: 0.9783\n",
            "Epoch 125/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1089 - accuracy: 0.9792 - val_loss: 0.0990 - val_accuracy: 0.9783\n",
            "Epoch 126/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1092 - accuracy: 0.9795 - val_loss: 0.0989 - val_accuracy: 0.9783\n",
            "Epoch 127/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1031 - accuracy: 0.9812 - val_loss: 0.0988 - val_accuracy: 0.9783\n",
            "Epoch 128/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1035 - accuracy: 0.9806 - val_loss: 0.0983 - val_accuracy: 0.9783\n",
            "Epoch 129/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1030 - accuracy: 0.9814 - val_loss: 0.0977 - val_accuracy: 0.9783\n",
            "Epoch 130/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1019 - accuracy: 0.9826 - val_loss: 0.0973 - val_accuracy: 0.9783\n",
            "Epoch 131/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0995 - accuracy: 0.9817 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
            "Epoch 132/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1067 - accuracy: 0.9795 - val_loss: 0.0962 - val_accuracy: 0.9783\n",
            "Epoch 133/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0958 - accuracy: 0.9843 - val_loss: 0.0949 - val_accuracy: 0.9783\n",
            "Epoch 134/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0960 - accuracy: 0.9826 - val_loss: 0.0934 - val_accuracy: 0.9783\n",
            "Epoch 135/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0982 - accuracy: 0.9823 - val_loss: 0.0919 - val_accuracy: 0.9783\n",
            "Epoch 136/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0993 - accuracy: 0.9812 - val_loss: 0.0903 - val_accuracy: 0.9783\n",
            "Epoch 137/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0979 - accuracy: 0.9840 - val_loss: 0.0890 - val_accuracy: 0.9783\n",
            "Epoch 138/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0986 - accuracy: 0.9795 - val_loss: 0.0885 - val_accuracy: 0.9783\n",
            "Epoch 139/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0973 - accuracy: 0.9823 - val_loss: 0.0882 - val_accuracy: 0.9783\n",
            "Epoch 140/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0919 - accuracy: 0.9817 - val_loss: 0.0882 - val_accuracy: 0.9783\n",
            "Epoch 141/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0929 - accuracy: 0.9820 - val_loss: 0.0883 - val_accuracy: 0.9783\n",
            "Epoch 142/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0877 - accuracy: 0.9837 - val_loss: 0.0885 - val_accuracy: 0.9783\n",
            "Epoch 143/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0936 - accuracy: 0.9826 - val_loss: 0.0887 - val_accuracy: 0.9783\n",
            "Epoch 144/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0959 - accuracy: 0.9809 - val_loss: 0.0889 - val_accuracy: 0.9783\n",
            "Epoch 145/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0923 - accuracy: 0.9829 - val_loss: 0.0886 - val_accuracy: 0.9783\n",
            "Epoch 146/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0900 - accuracy: 0.9817 - val_loss: 0.0880 - val_accuracy: 0.9783\n",
            "Epoch 147/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0888 - accuracy: 0.9837 - val_loss: 0.0874 - val_accuracy: 0.9783\n",
            "Epoch 148/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0929 - accuracy: 0.9812 - val_loss: 0.0869 - val_accuracy: 0.9783\n",
            "Epoch 149/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0891 - accuracy: 0.9834 - val_loss: 0.0861 - val_accuracy: 0.9783\n",
            "Epoch 150/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0913 - accuracy: 0.9832 - val_loss: 0.0853 - val_accuracy: 0.9783\n",
            "Epoch 151/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0913 - accuracy: 0.9809 - val_loss: 0.0843 - val_accuracy: 0.9795\n",
            "Epoch 152/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0913 - accuracy: 0.9834 - val_loss: 0.0839 - val_accuracy: 0.9795\n",
            "Epoch 153/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0870 - accuracy: 0.9840 - val_loss: 0.0835 - val_accuracy: 0.9795\n",
            "Epoch 154/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0909 - accuracy: 0.9829 - val_loss: 0.0832 - val_accuracy: 0.9795\n",
            "Epoch 155/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0901 - accuracy: 0.9795 - val_loss: 0.0832 - val_accuracy: 0.9795\n",
            "Epoch 156/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0871 - accuracy: 0.9846 - val_loss: 0.0830 - val_accuracy: 0.9795\n",
            "Epoch 157/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0830 - accuracy: 0.9846 - val_loss: 0.0827 - val_accuracy: 0.9795\n",
            "Epoch 158/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0829 - accuracy: 0.9840 - val_loss: 0.0823 - val_accuracy: 0.9795\n",
            "Epoch 159/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0877 - accuracy: 0.9834 - val_loss: 0.0820 - val_accuracy: 0.9795\n",
            "Epoch 160/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0866 - accuracy: 0.9829 - val_loss: 0.0820 - val_accuracy: 0.9795\n",
            "Epoch 161/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0838 - accuracy: 0.9814 - val_loss: 0.0818 - val_accuracy: 0.9795\n",
            "Epoch 162/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0816 - accuracy: 0.9866 - val_loss: 0.0817 - val_accuracy: 0.9795\n",
            "Epoch 163/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.9849 - val_loss: 0.0815 - val_accuracy: 0.9795\n",
            "Epoch 164/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0843 - accuracy: 0.9840 - val_loss: 0.0812 - val_accuracy: 0.9795\n",
            "Epoch 165/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0819 - accuracy: 0.9843 - val_loss: 0.0808 - val_accuracy: 0.9795\n",
            "Epoch 166/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0781 - accuracy: 0.9852 - val_loss: 0.0802 - val_accuracy: 0.9795\n",
            "Epoch 167/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.9832 - val_loss: 0.0793 - val_accuracy: 0.9795\n",
            "Epoch 168/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0798 - accuracy: 0.9849 - val_loss: 0.0788 - val_accuracy: 0.9795\n",
            "Epoch 169/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0791 - accuracy: 0.9846 - val_loss: 0.0784 - val_accuracy: 0.9795\n",
            "Epoch 170/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.9829 - val_loss: 0.0777 - val_accuracy: 0.9795\n",
            "Epoch 171/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0765 - accuracy: 0.9832 - val_loss: 0.0771 - val_accuracy: 0.9795\n",
            "Epoch 172/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0823 - accuracy: 0.9834 - val_loss: 0.0766 - val_accuracy: 0.9795\n",
            "Epoch 173/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0815 - accuracy: 0.9823 - val_loss: 0.0758 - val_accuracy: 0.9795\n",
            "Epoch 174/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0779 - accuracy: 0.9846 - val_loss: 0.0757 - val_accuracy: 0.9795\n",
            "Epoch 175/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0773 - accuracy: 0.9849 - val_loss: 0.0754 - val_accuracy: 0.9795\n",
            "Epoch 176/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0798 - accuracy: 0.9832 - val_loss: 0.0749 - val_accuracy: 0.9795\n",
            "Epoch 177/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0771 - accuracy: 0.9846 - val_loss: 0.0746 - val_accuracy: 0.9795\n",
            "Epoch 178/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0806 - accuracy: 0.9832 - val_loss: 0.0745 - val_accuracy: 0.9795\n",
            "Epoch 179/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0737 - accuracy: 0.9877 - val_loss: 0.0743 - val_accuracy: 0.9795\n",
            "Epoch 180/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0771 - accuracy: 0.9857 - val_loss: 0.0740 - val_accuracy: 0.9795\n",
            "Epoch 181/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0766 - accuracy: 0.9863 - val_loss: 0.0734 - val_accuracy: 0.9795\n",
            "Epoch 182/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0725 - accuracy: 0.9849 - val_loss: 0.0726 - val_accuracy: 0.9795\n",
            "Epoch 183/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0779 - accuracy: 0.9829 - val_loss: 0.0720 - val_accuracy: 0.9795\n",
            "Epoch 184/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0780 - accuracy: 0.9863 - val_loss: 0.0717 - val_accuracy: 0.9795\n",
            "Epoch 185/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0715 - accuracy: 0.9860 - val_loss: 0.0714 - val_accuracy: 0.9795\n",
            "Epoch 186/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0751 - accuracy: 0.9863 - val_loss: 0.0710 - val_accuracy: 0.9795\n",
            "Epoch 187/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0725 - accuracy: 0.9843 - val_loss: 0.0706 - val_accuracy: 0.9795\n",
            "Epoch 188/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0685 - accuracy: 0.9869 - val_loss: 0.0701 - val_accuracy: 0.9795\n",
            "Epoch 189/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0789 - accuracy: 0.9829 - val_loss: 0.0695 - val_accuracy: 0.9795\n",
            "Epoch 190/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0716 - accuracy: 0.9883 - val_loss: 0.0693 - val_accuracy: 0.9795\n",
            "Epoch 191/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0699 - accuracy: 0.9863 - val_loss: 0.0693 - val_accuracy: 0.9795\n",
            "Epoch 192/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0772 - accuracy: 0.9860 - val_loss: 0.0691 - val_accuracy: 0.9806\n",
            "Epoch 193/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0755 - accuracy: 0.9843 - val_loss: 0.0688 - val_accuracy: 0.9806\n",
            "Epoch 194/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0775 - accuracy: 0.9852 - val_loss: 0.0692 - val_accuracy: 0.9806\n",
            "Epoch 195/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0742 - accuracy: 0.9852 - val_loss: 0.0695 - val_accuracy: 0.9795\n",
            "Epoch 196/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0711 - accuracy: 0.9866 - val_loss: 0.0693 - val_accuracy: 0.9795\n",
            "Epoch 197/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0735 - accuracy: 0.9849 - val_loss: 0.0688 - val_accuracy: 0.9806\n",
            "Epoch 198/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0757 - accuracy: 0.9857 - val_loss: 0.0681 - val_accuracy: 0.9806\n",
            "Epoch 199/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0720 - accuracy: 0.9849 - val_loss: 0.0673 - val_accuracy: 0.9806\n",
            "Epoch 200/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0727 - accuracy: 0.9840 - val_loss: 0.0664 - val_accuracy: 0.9817\n",
            "Epoch 201/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0735 - accuracy: 0.9840 - val_loss: 0.0660 - val_accuracy: 0.9817\n",
            "Epoch 202/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0703 - accuracy: 0.9854 - val_loss: 0.0656 - val_accuracy: 0.9817\n",
            "Epoch 203/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0734 - accuracy: 0.9860 - val_loss: 0.0655 - val_accuracy: 0.9817\n",
            "Epoch 204/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0701 - accuracy: 0.9854 - val_loss: 0.0653 - val_accuracy: 0.9817\n",
            "Epoch 205/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0663 - accuracy: 0.9866 - val_loss: 0.0654 - val_accuracy: 0.9817\n",
            "Epoch 206/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0652 - accuracy: 0.9857 - val_loss: 0.0655 - val_accuracy: 0.9817\n",
            "Epoch 207/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0675 - accuracy: 0.9886 - val_loss: 0.0657 - val_accuracy: 0.9817\n",
            "Epoch 208/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0706 - accuracy: 0.9843 - val_loss: 0.0660 - val_accuracy: 0.9817\n",
            "Epoch 209/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0698 - accuracy: 0.9852 - val_loss: 0.0660 - val_accuracy: 0.9817\n",
            "Epoch 210/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0676 - accuracy: 0.9854 - val_loss: 0.0658 - val_accuracy: 0.9817\n",
            "Epoch 211/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0652 - accuracy: 0.9857 - val_loss: 0.0653 - val_accuracy: 0.9817\n",
            "Epoch 212/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0704 - accuracy: 0.9863 - val_loss: 0.0649 - val_accuracy: 0.9817\n",
            "Epoch 213/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0694 - accuracy: 0.9860 - val_loss: 0.0644 - val_accuracy: 0.9817\n",
            "Epoch 214/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0653 - accuracy: 0.9869 - val_loss: 0.0641 - val_accuracy: 0.9817\n",
            "Epoch 215/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0705 - accuracy: 0.9863 - val_loss: 0.0637 - val_accuracy: 0.9817\n",
            "Epoch 216/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0698 - accuracy: 0.9857 - val_loss: 0.0631 - val_accuracy: 0.9817\n",
            "Epoch 217/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0688 - accuracy: 0.9863 - val_loss: 0.0629 - val_accuracy: 0.9817\n",
            "Epoch 218/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0664 - accuracy: 0.9854 - val_loss: 0.0633 - val_accuracy: 0.9817\n",
            "Epoch 219/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0655 - accuracy: 0.9869 - val_loss: 0.0635 - val_accuracy: 0.9817\n",
            "Epoch 220/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0644 - accuracy: 0.9869 - val_loss: 0.0639 - val_accuracy: 0.9817\n",
            "Epoch 221/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0657 - accuracy: 0.9860 - val_loss: 0.0641 - val_accuracy: 0.9817\n",
            "Epoch 222/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0645 - accuracy: 0.9863 - val_loss: 0.0639 - val_accuracy: 0.9817\n",
            "Epoch 223/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0636 - accuracy: 0.9892 - val_loss: 0.0635 - val_accuracy: 0.9817\n",
            "Epoch 224/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0665 - accuracy: 0.9869 - val_loss: 0.0632 - val_accuracy: 0.9817\n",
            "Epoch 225/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0670 - accuracy: 0.9852 - val_loss: 0.0629 - val_accuracy: 0.9817\n",
            "Epoch 226/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0636 - accuracy: 0.9886 - val_loss: 0.0627 - val_accuracy: 0.9817\n",
            "Epoch 227/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0660 - accuracy: 0.9872 - val_loss: 0.0623 - val_accuracy: 0.9817\n",
            "Epoch 228/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0655 - accuracy: 0.9866 - val_loss: 0.0617 - val_accuracy: 0.9817\n",
            "Epoch 229/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0614 - accuracy: 0.9877 - val_loss: 0.0611 - val_accuracy: 0.9817\n",
            "Epoch 230/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0699 - accuracy: 0.9857 - val_loss: 0.0607 - val_accuracy: 0.9817\n",
            "Epoch 231/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0644 - accuracy: 0.9872 - val_loss: 0.0604 - val_accuracy: 0.9817\n",
            "Epoch 232/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0655 - accuracy: 0.9854 - val_loss: 0.0599 - val_accuracy: 0.9817\n",
            "Epoch 233/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0608 - accuracy: 0.9889 - val_loss: 0.0596 - val_accuracy: 0.9817\n",
            "Epoch 234/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0633 - accuracy: 0.9872 - val_loss: 0.0596 - val_accuracy: 0.9817\n",
            "Epoch 235/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0625 - accuracy: 0.9869 - val_loss: 0.0593 - val_accuracy: 0.9817\n",
            "Epoch 236/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0648 - accuracy: 0.9880 - val_loss: 0.0589 - val_accuracy: 0.9817\n",
            "Epoch 237/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0576 - accuracy: 0.9886 - val_loss: 0.0584 - val_accuracy: 0.9817\n",
            "Epoch 238/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0601 - accuracy: 0.9874 - val_loss: 0.0577 - val_accuracy: 0.9817\n",
            "Epoch 239/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0614 - accuracy: 0.9869 - val_loss: 0.0575 - val_accuracy: 0.9817\n",
            "Epoch 240/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0617 - accuracy: 0.9880 - val_loss: 0.0571 - val_accuracy: 0.9817\n",
            "Epoch 241/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0558 - accuracy: 0.9880 - val_loss: 0.0571 - val_accuracy: 0.9817\n",
            "Epoch 242/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0668 - accuracy: 0.9866 - val_loss: 0.0570 - val_accuracy: 0.9817\n",
            "Epoch 243/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0540 - accuracy: 0.9892 - val_loss: 0.0571 - val_accuracy: 0.9817\n",
            "Epoch 244/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0605 - accuracy: 0.9872 - val_loss: 0.0575 - val_accuracy: 0.9817\n",
            "Epoch 245/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0592 - accuracy: 0.9869 - val_loss: 0.0576 - val_accuracy: 0.9817\n",
            "Epoch 246/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0595 - accuracy: 0.9874 - val_loss: 0.0576 - val_accuracy: 0.9817\n",
            "Epoch 247/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0602 - accuracy: 0.9877 - val_loss: 0.0574 - val_accuracy: 0.9817\n",
            "Epoch 248/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0625 - accuracy: 0.9843 - val_loss: 0.0568 - val_accuracy: 0.9817\n",
            "Epoch 249/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0615 - accuracy: 0.9863 - val_loss: 0.0564 - val_accuracy: 0.9817\n",
            "Epoch 250/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0626 - accuracy: 0.9877 - val_loss: 0.0560 - val_accuracy: 0.9817\n",
            "Epoch 251/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0585 - accuracy: 0.9894 - val_loss: 0.0555 - val_accuracy: 0.9817\n",
            "Epoch 252/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0617 - accuracy: 0.9872 - val_loss: 0.0552 - val_accuracy: 0.9817\n",
            "Epoch 253/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0558 - accuracy: 0.9897 - val_loss: 0.0549 - val_accuracy: 0.9817\n",
            "Epoch 254/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0563 - accuracy: 0.9886 - val_loss: 0.0548 - val_accuracy: 0.9817\n",
            "Epoch 255/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0573 - accuracy: 0.9889 - val_loss: 0.0547 - val_accuracy: 0.9817\n",
            "Epoch 256/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0574 - accuracy: 0.9886 - val_loss: 0.0547 - val_accuracy: 0.9817\n",
            "Epoch 257/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0616 - accuracy: 0.9877 - val_loss: 0.0548 - val_accuracy: 0.9817\n",
            "Epoch 258/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0636 - accuracy: 0.9852 - val_loss: 0.0550 - val_accuracy: 0.9817\n",
            "Epoch 259/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0585 - accuracy: 0.9869 - val_loss: 0.0550 - val_accuracy: 0.9817\n",
            "Epoch 260/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0586 - accuracy: 0.9874 - val_loss: 0.0552 - val_accuracy: 0.9817\n",
            "Epoch 261/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0599 - accuracy: 0.9886 - val_loss: 0.0554 - val_accuracy: 0.9817\n",
            "Epoch 262/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0575 - accuracy: 0.9889 - val_loss: 0.0554 - val_accuracy: 0.9817\n",
            "Epoch 263/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0554 - accuracy: 0.9892 - val_loss: 0.0551 - val_accuracy: 0.9817\n",
            "Epoch 264/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0570 - accuracy: 0.9886 - val_loss: 0.0543 - val_accuracy: 0.9817\n",
            "Epoch 265/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0593 - accuracy: 0.9889 - val_loss: 0.0536 - val_accuracy: 0.9817\n",
            "Epoch 266/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0558 - accuracy: 0.9889 - val_loss: 0.0530 - val_accuracy: 0.9817\n",
            "Epoch 267/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0546 - accuracy: 0.9892 - val_loss: 0.0523 - val_accuracy: 0.9829\n",
            "Epoch 268/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0582 - accuracy: 0.9877 - val_loss: 0.0517 - val_accuracy: 0.9829\n",
            "Epoch 269/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0560 - accuracy: 0.9897 - val_loss: 0.0515 - val_accuracy: 0.9829\n",
            "Epoch 270/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0551 - accuracy: 0.9897 - val_loss: 0.0514 - val_accuracy: 0.9829\n",
            "Epoch 271/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0526 - accuracy: 0.9886 - val_loss: 0.0513 - val_accuracy: 0.9829\n",
            "Epoch 272/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0546 - accuracy: 0.9880 - val_loss: 0.0514 - val_accuracy: 0.9829\n",
            "Epoch 273/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0540 - accuracy: 0.9889 - val_loss: 0.0512 - val_accuracy: 0.9829\n",
            "Epoch 274/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0543 - accuracy: 0.9900 - val_loss: 0.0509 - val_accuracy: 0.9829\n",
            "Epoch 275/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0521 - accuracy: 0.9892 - val_loss: 0.0507 - val_accuracy: 0.9840\n",
            "Epoch 276/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0539 - accuracy: 0.9903 - val_loss: 0.0508 - val_accuracy: 0.9829\n",
            "Epoch 277/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0523 - accuracy: 0.9872 - val_loss: 0.0506 - val_accuracy: 0.9840\n",
            "Epoch 278/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0552 - accuracy: 0.9863 - val_loss: 0.0505 - val_accuracy: 0.9840\n",
            "Epoch 279/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0558 - accuracy: 0.9889 - val_loss: 0.0503 - val_accuracy: 0.9840\n",
            "Epoch 280/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0590 - accuracy: 0.9863 - val_loss: 0.0504 - val_accuracy: 0.9840\n",
            "Epoch 281/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0531 - accuracy: 0.9894 - val_loss: 0.0503 - val_accuracy: 0.9840\n",
            "Epoch 282/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0545 - accuracy: 0.9877 - val_loss: 0.0502 - val_accuracy: 0.9840\n",
            "Epoch 283/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0560 - accuracy: 0.9874 - val_loss: 0.0500 - val_accuracy: 0.9840\n",
            "Epoch 284/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0497 - accuracy: 0.9889 - val_loss: 0.0497 - val_accuracy: 0.9840\n",
            "Epoch 285/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0551 - accuracy: 0.9892 - val_loss: 0.0491 - val_accuracy: 0.9840\n",
            "Epoch 286/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0517 - accuracy: 0.9883 - val_loss: 0.0489 - val_accuracy: 0.9840\n",
            "Epoch 287/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0512 - accuracy: 0.9894 - val_loss: 0.0485 - val_accuracy: 0.9840\n",
            "Epoch 288/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0548 - accuracy: 0.9886 - val_loss: 0.0484 - val_accuracy: 0.9840\n",
            "Epoch 289/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0527 - accuracy: 0.9880 - val_loss: 0.0483 - val_accuracy: 0.9840\n",
            "Epoch 290/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0539 - accuracy: 0.9894 - val_loss: 0.0483 - val_accuracy: 0.9840\n",
            "Epoch 291/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0505 - accuracy: 0.9912 - val_loss: 0.0482 - val_accuracy: 0.9840\n",
            "Epoch 292/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0535 - accuracy: 0.9883 - val_loss: 0.0480 - val_accuracy: 0.9840\n",
            "Epoch 293/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0567 - accuracy: 0.9869 - val_loss: 0.0476 - val_accuracy: 0.9840\n",
            "Epoch 294/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0563 - accuracy: 0.9880 - val_loss: 0.0470 - val_accuracy: 0.9840\n",
            "Epoch 295/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0512 - accuracy: 0.9914 - val_loss: 0.0468 - val_accuracy: 0.9840\n",
            "Epoch 296/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9880 - val_loss: 0.0468 - val_accuracy: 0.9840\n",
            "Epoch 297/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0556 - accuracy: 0.9874 - val_loss: 0.0462 - val_accuracy: 0.9840\n",
            "Epoch 298/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0547 - accuracy: 0.9886 - val_loss: 0.0459 - val_accuracy: 0.9840\n",
            "Epoch 299/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0563 - accuracy: 0.9863 - val_loss: 0.0454 - val_accuracy: 0.9840\n",
            "Epoch 300/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0577 - accuracy: 0.9869 - val_loss: 0.0449 - val_accuracy: 0.9840\n",
            "Epoch 301/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0545 - accuracy: 0.9880 - val_loss: 0.0450 - val_accuracy: 0.9840\n",
            "Epoch 302/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0533 - accuracy: 0.9892 - val_loss: 0.0446 - val_accuracy: 0.9840\n",
            "Epoch 303/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0560 - accuracy: 0.9874 - val_loss: 0.0446 - val_accuracy: 0.9840\n",
            "Epoch 304/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0478 - accuracy: 0.9886 - val_loss: 0.0446 - val_accuracy: 0.9840\n",
            "Epoch 305/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0530 - accuracy: 0.9880 - val_loss: 0.0449 - val_accuracy: 0.9840\n",
            "Epoch 306/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0524 - accuracy: 0.9900 - val_loss: 0.0451 - val_accuracy: 0.9840\n",
            "Epoch 307/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0503 - accuracy: 0.9909 - val_loss: 0.0455 - val_accuracy: 0.9840\n",
            "Epoch 308/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0528 - accuracy: 0.9892 - val_loss: 0.0458 - val_accuracy: 0.9840\n",
            "Epoch 309/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0520 - accuracy: 0.9889 - val_loss: 0.0461 - val_accuracy: 0.9840\n",
            "Epoch 310/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0487 - accuracy: 0.9886 - val_loss: 0.0462 - val_accuracy: 0.9840\n",
            "Epoch 311/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0506 - accuracy: 0.9894 - val_loss: 0.0461 - val_accuracy: 0.9840\n",
            "Epoch 312/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0510 - accuracy: 0.9889 - val_loss: 0.0459 - val_accuracy: 0.9840\n",
            "Epoch 313/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0514 - accuracy: 0.9894 - val_loss: 0.0460 - val_accuracy: 0.9840\n",
            "Epoch 314/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0512 - accuracy: 0.9883 - val_loss: 0.0457 - val_accuracy: 0.9840\n",
            "Epoch 315/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0509 - accuracy: 0.9874 - val_loss: 0.0452 - val_accuracy: 0.9840\n",
            "Epoch 316/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0474 - accuracy: 0.9897 - val_loss: 0.0448 - val_accuracy: 0.9840\n",
            "Epoch 317/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0491 - accuracy: 0.9892 - val_loss: 0.0444 - val_accuracy: 0.9840\n",
            "Epoch 318/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0516 - accuracy: 0.9886 - val_loss: 0.0437 - val_accuracy: 0.9840\n",
            "Epoch 319/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0515 - accuracy: 0.9880 - val_loss: 0.0430 - val_accuracy: 0.9840\n",
            "Epoch 320/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0458 - accuracy: 0.9900 - val_loss: 0.0426 - val_accuracy: 0.9852\n",
            "Epoch 321/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0472 - accuracy: 0.9897 - val_loss: 0.0424 - val_accuracy: 0.9852\n",
            "Epoch 322/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0453 - accuracy: 0.9906 - val_loss: 0.0422 - val_accuracy: 0.9852\n",
            "Epoch 323/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0505 - accuracy: 0.9894 - val_loss: 0.0420 - val_accuracy: 0.9852\n",
            "Epoch 324/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0495 - accuracy: 0.9886 - val_loss: 0.0419 - val_accuracy: 0.9852\n",
            "Epoch 325/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0536 - accuracy: 0.9872 - val_loss: 0.0418 - val_accuracy: 0.9852\n",
            "Epoch 326/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0505 - accuracy: 0.9874 - val_loss: 0.0417 - val_accuracy: 0.9852\n",
            "Epoch 327/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0513 - accuracy: 0.9889 - val_loss: 0.0419 - val_accuracy: 0.9852\n",
            "Epoch 328/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0494 - accuracy: 0.9892 - val_loss: 0.0422 - val_accuracy: 0.9852\n",
            "Epoch 329/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0500 - accuracy: 0.9883 - val_loss: 0.0425 - val_accuracy: 0.9852\n",
            "Epoch 330/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0439 - accuracy: 0.9889 - val_loss: 0.0423 - val_accuracy: 0.9852\n",
            "Epoch 331/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0463 - accuracy: 0.9889 - val_loss: 0.0421 - val_accuracy: 0.9852\n",
            "Epoch 332/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0492 - accuracy: 0.9897 - val_loss: 0.0417 - val_accuracy: 0.9852\n",
            "Epoch 333/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0496 - accuracy: 0.9897 - val_loss: 0.0415 - val_accuracy: 0.9852\n",
            "Epoch 334/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0478 - accuracy: 0.9880 - val_loss: 0.0414 - val_accuracy: 0.9852\n",
            "Epoch 335/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0470 - accuracy: 0.9894 - val_loss: 0.0412 - val_accuracy: 0.9852\n",
            "Epoch 336/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0470 - accuracy: 0.9900 - val_loss: 0.0411 - val_accuracy: 0.9852\n",
            "Epoch 337/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0502 - accuracy: 0.9880 - val_loss: 0.0410 - val_accuracy: 0.9852\n",
            "Epoch 338/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0480 - accuracy: 0.9892 - val_loss: 0.0408 - val_accuracy: 0.9852\n",
            "Epoch 339/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0480 - accuracy: 0.9900 - val_loss: 0.0406 - val_accuracy: 0.9852\n",
            "Epoch 340/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0476 - accuracy: 0.9889 - val_loss: 0.0401 - val_accuracy: 0.9852\n",
            "Epoch 341/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0435 - accuracy: 0.9900 - val_loss: 0.0397 - val_accuracy: 0.9852\n",
            "Epoch 342/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0512 - accuracy: 0.9889 - val_loss: 0.0390 - val_accuracy: 0.9863\n",
            "Epoch 343/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0452 - accuracy: 0.9897 - val_loss: 0.0390 - val_accuracy: 0.9863\n",
            "Epoch 344/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0482 - accuracy: 0.9906 - val_loss: 0.0392 - val_accuracy: 0.9852\n",
            "Epoch 345/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0482 - accuracy: 0.9892 - val_loss: 0.0396 - val_accuracy: 0.9852\n",
            "Epoch 346/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9909 - val_loss: 0.0399 - val_accuracy: 0.9852\n",
            "Epoch 347/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0464 - accuracy: 0.9909 - val_loss: 0.0403 - val_accuracy: 0.9852\n",
            "Epoch 348/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0477 - accuracy: 0.9894 - val_loss: 0.0406 - val_accuracy: 0.9852\n",
            "Epoch 349/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0462 - accuracy: 0.9889 - val_loss: 0.0410 - val_accuracy: 0.9852\n",
            "Epoch 350/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0513 - accuracy: 0.9880 - val_loss: 0.0409 - val_accuracy: 0.9852\n",
            "Epoch 351/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9892 - val_loss: 0.0403 - val_accuracy: 0.9852\n",
            "Epoch 352/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0472 - accuracy: 0.9906 - val_loss: 0.0397 - val_accuracy: 0.9852\n",
            "Epoch 353/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0505 - accuracy: 0.9877 - val_loss: 0.0390 - val_accuracy: 0.9863\n",
            "Epoch 354/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0480 - accuracy: 0.9903 - val_loss: 0.0384 - val_accuracy: 0.9863\n",
            "Epoch 355/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0469 - accuracy: 0.9892 - val_loss: 0.0380 - val_accuracy: 0.9874\n",
            "Epoch 356/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0468 - accuracy: 0.9906 - val_loss: 0.0376 - val_accuracy: 0.9886\n",
            "Epoch 357/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0457 - accuracy: 0.9903 - val_loss: 0.0377 - val_accuracy: 0.9886\n",
            "Epoch 358/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0427 - accuracy: 0.9906 - val_loss: 0.0380 - val_accuracy: 0.9886\n",
            "Epoch 359/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0458 - accuracy: 0.9900 - val_loss: 0.0383 - val_accuracy: 0.9874\n",
            "Epoch 360/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0457 - accuracy: 0.9880 - val_loss: 0.0386 - val_accuracy: 0.9863\n",
            "Epoch 361/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.0386 - val_accuracy: 0.9863\n",
            "Epoch 362/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0478 - accuracy: 0.9892 - val_loss: 0.0383 - val_accuracy: 0.9863\n",
            "Epoch 363/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0431 - accuracy: 0.9903 - val_loss: 0.0379 - val_accuracy: 0.9874\n",
            "Epoch 364/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0442 - accuracy: 0.9920 - val_loss: 0.0376 - val_accuracy: 0.9874\n",
            "Epoch 365/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0458 - accuracy: 0.9914 - val_loss: 0.0375 - val_accuracy: 0.9874\n",
            "Epoch 366/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0444 - accuracy: 0.9897 - val_loss: 0.0376 - val_accuracy: 0.9874\n",
            "Epoch 367/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0446 - accuracy: 0.9900 - val_loss: 0.0377 - val_accuracy: 0.9863\n",
            "Epoch 368/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0470 - accuracy: 0.9909 - val_loss: 0.0376 - val_accuracy: 0.9863\n",
            "Epoch 369/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9903 - val_loss: 0.0373 - val_accuracy: 0.9874\n",
            "Epoch 370/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0463 - accuracy: 0.9894 - val_loss: 0.0372 - val_accuracy: 0.9886\n",
            "Epoch 371/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0468 - accuracy: 0.9906 - val_loss: 0.0369 - val_accuracy: 0.9886\n",
            "Epoch 372/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0457 - accuracy: 0.9894 - val_loss: 0.0366 - val_accuracy: 0.9886\n",
            "Epoch 373/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0494 - accuracy: 0.9892 - val_loss: 0.0363 - val_accuracy: 0.9886\n",
            "Epoch 374/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0461 - accuracy: 0.9897 - val_loss: 0.0358 - val_accuracy: 0.9886\n",
            "Epoch 375/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0440 - accuracy: 0.9917 - val_loss: 0.0356 - val_accuracy: 0.9886\n",
            "Epoch 376/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0457 - accuracy: 0.9900 - val_loss: 0.0357 - val_accuracy: 0.9886\n",
            "Epoch 377/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0442 - accuracy: 0.9906 - val_loss: 0.0358 - val_accuracy: 0.9886\n",
            "Epoch 378/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0446 - accuracy: 0.9892 - val_loss: 0.0359 - val_accuracy: 0.9886\n",
            "Epoch 379/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0460 - accuracy: 0.9892 - val_loss: 0.0360 - val_accuracy: 0.9886\n",
            "Epoch 380/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0411 - accuracy: 0.9894 - val_loss: 0.0360 - val_accuracy: 0.9886\n",
            "Epoch 381/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0452 - accuracy: 0.9903 - val_loss: 0.0359 - val_accuracy: 0.9886\n",
            "Epoch 382/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0463 - accuracy: 0.9897 - val_loss: 0.0358 - val_accuracy: 0.9886\n",
            "Epoch 383/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0425 - accuracy: 0.9906 - val_loss: 0.0356 - val_accuracy: 0.9886\n",
            "Epoch 384/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0451 - accuracy: 0.9900 - val_loss: 0.0354 - val_accuracy: 0.9886\n",
            "Epoch 385/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0415 - accuracy: 0.9909 - val_loss: 0.0352 - val_accuracy: 0.9886\n",
            "Epoch 386/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0409 - accuracy: 0.9912 - val_loss: 0.0351 - val_accuracy: 0.9886\n",
            "Epoch 387/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0429 - accuracy: 0.9900 - val_loss: 0.0351 - val_accuracy: 0.9886\n",
            "Epoch 388/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0433 - accuracy: 0.9903 - val_loss: 0.0350 - val_accuracy: 0.9886\n",
            "Epoch 389/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0465 - accuracy: 0.9883 - val_loss: 0.0348 - val_accuracy: 0.9886\n",
            "Epoch 390/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0441 - accuracy: 0.9912 - val_loss: 0.0349 - val_accuracy: 0.9886\n",
            "Epoch 391/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0435 - accuracy: 0.9903 - val_loss: 0.0349 - val_accuracy: 0.9886\n",
            "Epoch 392/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0438 - accuracy: 0.9897 - val_loss: 0.0350 - val_accuracy: 0.9886\n",
            "Epoch 393/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0420 - accuracy: 0.9900 - val_loss: 0.0350 - val_accuracy: 0.9886\n",
            "Epoch 394/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0412 - accuracy: 0.9906 - val_loss: 0.0350 - val_accuracy: 0.9886\n",
            "Epoch 395/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0448 - accuracy: 0.9894 - val_loss: 0.0346 - val_accuracy: 0.9886\n",
            "Epoch 396/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0441 - accuracy: 0.9897 - val_loss: 0.0342 - val_accuracy: 0.9897\n",
            "Epoch 397/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0430 - accuracy: 0.9909 - val_loss: 0.0340 - val_accuracy: 0.9897\n",
            "Epoch 398/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0442 - accuracy: 0.9903 - val_loss: 0.0338 - val_accuracy: 0.9897\n",
            "Epoch 399/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0421 - accuracy: 0.9892 - val_loss: 0.0335 - val_accuracy: 0.9897\n",
            "Epoch 400/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0408 - accuracy: 0.9906 - val_loss: 0.0335 - val_accuracy: 0.9897\n",
            "Epoch 401/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0431 - accuracy: 0.9903 - val_loss: 0.0334 - val_accuracy: 0.9897\n",
            "Epoch 402/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0447 - accuracy: 0.9892 - val_loss: 0.0333 - val_accuracy: 0.9897\n",
            "Epoch 403/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0402 - accuracy: 0.9912 - val_loss: 0.0332 - val_accuracy: 0.9897\n",
            "Epoch 404/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0420 - accuracy: 0.9914 - val_loss: 0.0331 - val_accuracy: 0.9897\n",
            "Epoch 405/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0424 - accuracy: 0.9877 - val_loss: 0.0330 - val_accuracy: 0.9897\n",
            "Epoch 406/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0421 - accuracy: 0.9900 - val_loss: 0.0326 - val_accuracy: 0.9897\n",
            "Epoch 407/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0429 - accuracy: 0.9897 - val_loss: 0.0324 - val_accuracy: 0.9897\n",
            "Epoch 408/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0444 - accuracy: 0.9886 - val_loss: 0.0323 - val_accuracy: 0.9897\n",
            "Epoch 409/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0397 - accuracy: 0.9909 - val_loss: 0.0322 - val_accuracy: 0.9897\n",
            "Epoch 410/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0373 - accuracy: 0.9929 - val_loss: 0.0322 - val_accuracy: 0.9897\n",
            "Epoch 411/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0415 - accuracy: 0.9900 - val_loss: 0.0322 - val_accuracy: 0.9897\n",
            "Epoch 412/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0405 - accuracy: 0.9903 - val_loss: 0.0325 - val_accuracy: 0.9897\n",
            "Epoch 413/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0398 - accuracy: 0.9917 - val_loss: 0.0327 - val_accuracy: 0.9897\n",
            "Epoch 414/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0427 - accuracy: 0.9900 - val_loss: 0.0329 - val_accuracy: 0.9897\n",
            "Epoch 415/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0401 - accuracy: 0.9912 - val_loss: 0.0329 - val_accuracy: 0.9897\n",
            "Epoch 416/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0407 - accuracy: 0.9909 - val_loss: 0.0330 - val_accuracy: 0.9897\n",
            "Epoch 417/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0405 - accuracy: 0.9894 - val_loss: 0.0328 - val_accuracy: 0.9897\n",
            "Epoch 418/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0417 - accuracy: 0.9909 - val_loss: 0.0324 - val_accuracy: 0.9897\n",
            "Epoch 419/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0419 - accuracy: 0.9892 - val_loss: 0.0318 - val_accuracy: 0.9897\n",
            "Epoch 420/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0398 - accuracy: 0.9912 - val_loss: 0.0315 - val_accuracy: 0.9897\n",
            "Epoch 421/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0362 - accuracy: 0.9914 - val_loss: 0.0312 - val_accuracy: 0.9897\n",
            "Epoch 422/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0382 - accuracy: 0.9900 - val_loss: 0.0310 - val_accuracy: 0.9909\n",
            "Epoch 423/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0400 - accuracy: 0.9909 - val_loss: 0.0310 - val_accuracy: 0.9909\n",
            "Epoch 424/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0407 - accuracy: 0.9909 - val_loss: 0.0312 - val_accuracy: 0.9897\n",
            "Epoch 425/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0422 - accuracy: 0.9900 - val_loss: 0.0314 - val_accuracy: 0.9897\n",
            "Epoch 426/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0398 - accuracy: 0.9900 - val_loss: 0.0316 - val_accuracy: 0.9897\n",
            "Epoch 427/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0394 - accuracy: 0.9900 - val_loss: 0.0319 - val_accuracy: 0.9897\n",
            "Epoch 428/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0395 - accuracy: 0.9903 - val_loss: 0.0320 - val_accuracy: 0.9897\n",
            "Epoch 429/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0378 - accuracy: 0.9909 - val_loss: 0.0320 - val_accuracy: 0.9897\n",
            "Epoch 430/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0416 - accuracy: 0.9903 - val_loss: 0.0316 - val_accuracy: 0.9897\n",
            "Epoch 431/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0417 - accuracy: 0.9912 - val_loss: 0.0309 - val_accuracy: 0.9897\n",
            "Epoch 432/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0424 - accuracy: 0.9889 - val_loss: 0.0303 - val_accuracy: 0.9897\n",
            "Epoch 433/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0391 - accuracy: 0.9900 - val_loss: 0.0300 - val_accuracy: 0.9909\n",
            "Epoch 434/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0416 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9909\n",
            "Epoch 435/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0416 - accuracy: 0.9897 - val_loss: 0.0293 - val_accuracy: 0.9909\n",
            "Epoch 436/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0418 - accuracy: 0.9906 - val_loss: 0.0292 - val_accuracy: 0.9909\n",
            "Epoch 437/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0379 - accuracy: 0.9906 - val_loss: 0.0290 - val_accuracy: 0.9909\n",
            "Epoch 438/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0424 - accuracy: 0.9894 - val_loss: 0.0291 - val_accuracy: 0.9909\n",
            "Epoch 439/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0372 - accuracy: 0.9917 - val_loss: 0.0292 - val_accuracy: 0.9909\n",
            "Epoch 440/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0388 - accuracy: 0.9923 - val_loss: 0.0295 - val_accuracy: 0.9909\n",
            "Epoch 441/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0385 - accuracy: 0.9912 - val_loss: 0.0300 - val_accuracy: 0.9897\n",
            "Epoch 442/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0372 - accuracy: 0.9917 - val_loss: 0.0304 - val_accuracy: 0.9897\n",
            "Epoch 443/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0402 - accuracy: 0.9909 - val_loss: 0.0305 - val_accuracy: 0.9897\n",
            "Epoch 444/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0416 - accuracy: 0.9900 - val_loss: 0.0305 - val_accuracy: 0.9897\n",
            "Epoch 445/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0405 - accuracy: 0.9900 - val_loss: 0.0304 - val_accuracy: 0.9897\n",
            "Epoch 446/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0389 - accuracy: 0.9914 - val_loss: 0.0303 - val_accuracy: 0.9897\n",
            "Epoch 447/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0419 - accuracy: 0.9906 - val_loss: 0.0300 - val_accuracy: 0.9897\n",
            "Epoch 448/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0369 - accuracy: 0.9914 - val_loss: 0.0298 - val_accuracy: 0.9897\n",
            "Epoch 449/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0376 - accuracy: 0.9909 - val_loss: 0.0294 - val_accuracy: 0.9909\n",
            "Epoch 450/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0431 - accuracy: 0.9894 - val_loss: 0.0290 - val_accuracy: 0.9909\n",
            "Epoch 451/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0379 - accuracy: 0.9912 - val_loss: 0.0287 - val_accuracy: 0.9909\n",
            "Epoch 452/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0392 - accuracy: 0.9914 - val_loss: 0.0283 - val_accuracy: 0.9909\n",
            "Epoch 453/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0385 - accuracy: 0.9914 - val_loss: 0.0280 - val_accuracy: 0.9909\n",
            "Epoch 454/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0389 - accuracy: 0.9912 - val_loss: 0.0279 - val_accuracy: 0.9909\n",
            "Epoch 455/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0419 - accuracy: 0.9909 - val_loss: 0.0279 - val_accuracy: 0.9909\n",
            "Epoch 456/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0401 - accuracy: 0.9914 - val_loss: 0.0279 - val_accuracy: 0.9909\n",
            "Epoch 457/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0368 - accuracy: 0.9912 - val_loss: 0.0280 - val_accuracy: 0.9909\n",
            "Epoch 458/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0372 - accuracy: 0.9920 - val_loss: 0.0283 - val_accuracy: 0.9909\n",
            "Epoch 459/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0360 - accuracy: 0.9912 - val_loss: 0.0285 - val_accuracy: 0.9909\n",
            "Epoch 460/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0393 - accuracy: 0.9906 - val_loss: 0.0287 - val_accuracy: 0.9909\n",
            "Epoch 461/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0379 - accuracy: 0.9906 - val_loss: 0.0289 - val_accuracy: 0.9909\n",
            "Epoch 462/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0406 - accuracy: 0.9909 - val_loss: 0.0289 - val_accuracy: 0.9909\n",
            "Epoch 463/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0390 - accuracy: 0.9900 - val_loss: 0.0287 - val_accuracy: 0.9909\n",
            "Epoch 464/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0367 - accuracy: 0.9914 - val_loss: 0.0286 - val_accuracy: 0.9909\n",
            "Epoch 465/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0402 - accuracy: 0.9903 - val_loss: 0.0284 - val_accuracy: 0.9909\n",
            "Epoch 466/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0376 - accuracy: 0.9906 - val_loss: 0.0281 - val_accuracy: 0.9909\n",
            "Epoch 467/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0373 - accuracy: 0.9917 - val_loss: 0.0279 - val_accuracy: 0.9909\n",
            "Epoch 468/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0408 - accuracy: 0.9903 - val_loss: 0.0277 - val_accuracy: 0.9909\n",
            "Epoch 469/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0414 - accuracy: 0.9892 - val_loss: 0.0275 - val_accuracy: 0.9909\n",
            "Epoch 470/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0361 - accuracy: 0.9917 - val_loss: 0.0275 - val_accuracy: 0.9909\n",
            "Epoch 471/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0361 - accuracy: 0.9920 - val_loss: 0.0275 - val_accuracy: 0.9909\n",
            "Epoch 472/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0386 - accuracy: 0.9897 - val_loss: 0.0273 - val_accuracy: 0.9909\n",
            "Epoch 473/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0359 - accuracy: 0.9920 - val_loss: 0.0271 - val_accuracy: 0.9909\n",
            "Epoch 474/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0364 - accuracy: 0.9914 - val_loss: 0.0269 - val_accuracy: 0.9909\n",
            "Epoch 475/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0343 - accuracy: 0.9917 - val_loss: 0.0265 - val_accuracy: 0.9909\n",
            "Epoch 476/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0346 - accuracy: 0.9917 - val_loss: 0.0262 - val_accuracy: 0.9909\n",
            "Epoch 477/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0360 - accuracy: 0.9920 - val_loss: 0.0260 - val_accuracy: 0.9909\n",
            "Epoch 478/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0366 - accuracy: 0.9914 - val_loss: 0.0261 - val_accuracy: 0.9909\n",
            "Epoch 479/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0358 - accuracy: 0.9914 - val_loss: 0.0261 - val_accuracy: 0.9909\n",
            "Epoch 480/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 0.9900 - val_loss: 0.0263 - val_accuracy: 0.9909\n",
            "Epoch 481/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0348 - accuracy: 0.9917 - val_loss: 0.0264 - val_accuracy: 0.9909\n",
            "Epoch 482/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0393 - accuracy: 0.9912 - val_loss: 0.0265 - val_accuracy: 0.9909\n",
            "Epoch 483/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0363 - accuracy: 0.9909 - val_loss: 0.0266 - val_accuracy: 0.9909\n",
            "Epoch 484/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0337 - accuracy: 0.9926 - val_loss: 0.0267 - val_accuracy: 0.9909\n",
            "Epoch 485/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0321 - accuracy: 0.9920 - val_loss: 0.0268 - val_accuracy: 0.9909\n",
            "Epoch 486/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0362 - accuracy: 0.9912 - val_loss: 0.0267 - val_accuracy: 0.9909\n",
            "Epoch 487/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0374 - accuracy: 0.9909 - val_loss: 0.0266 - val_accuracy: 0.9909\n",
            "Epoch 488/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0370 - accuracy: 0.9917 - val_loss: 0.0266 - val_accuracy: 0.9909\n",
            "Epoch 489/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0353 - accuracy: 0.9920 - val_loss: 0.0266 - val_accuracy: 0.9909\n",
            "Epoch 490/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0403 - accuracy: 0.9897 - val_loss: 0.0265 - val_accuracy: 0.9909\n",
            "Epoch 491/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0366 - accuracy: 0.9912 - val_loss: 0.0264 - val_accuracy: 0.9909\n",
            "Epoch 492/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0356 - accuracy: 0.9914 - val_loss: 0.0261 - val_accuracy: 0.9909\n",
            "Epoch 493/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0326 - accuracy: 0.9917 - val_loss: 0.0258 - val_accuracy: 0.9909\n",
            "Epoch 494/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0365 - accuracy: 0.9903 - val_loss: 0.0256 - val_accuracy: 0.9909\n",
            "Epoch 495/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0352 - accuracy: 0.9906 - val_loss: 0.0253 - val_accuracy: 0.9909\n",
            "Epoch 496/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0364 - accuracy: 0.9920 - val_loss: 0.0252 - val_accuracy: 0.9909\n",
            "Epoch 497/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0360 - accuracy: 0.9906 - val_loss: 0.0253 - val_accuracy: 0.9909\n",
            "Epoch 498/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0359 - accuracy: 0.9914 - val_loss: 0.0256 - val_accuracy: 0.9909\n",
            "Epoch 499/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0359 - accuracy: 0.9909 - val_loss: 0.0258 - val_accuracy: 0.9909\n",
            "Epoch 500/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0355 - accuracy: 0.9923 - val_loss: 0.0257 - val_accuracy: 0.9909\n",
            "\n",
            "Score:  [0.04225073754787445, 0.9897540807723999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEcYFgZkkmUo",
        "outputId": "4f043fc7-c598-4c3c-ec89-27ad2e3a7e89"
      },
      "source": [
        "np.round(model.predict(dat_resaXredts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "fEuayo-ClZ4c",
        "outputId": "8d5eb852-22c1-4792-fddc-c1c190197f8c"
      },
      "source": [
        "ffnn_res=pd.DataFrame(dat_resaXredts)\n",
        "ffnn_res['Disease']=dat_resaYts\n",
        "ffnn_res=pd.concat([ffnn_res,pd.DataFrame(np.round(model.predict(dat_resaXredts))).astype(int)],axis=1)\n",
        "ffnn_res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>Disease</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.87e-02</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.13</td>\n",
              "      <td>3.79e-02</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.16</td>\n",
              "      <td>7.07e-02</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.12</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.38e-02</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.07</td>\n",
              "      <td>2.81e-02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>4.45e-03</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.73e-01</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1.95e-01</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.09</td>\n",
              "      <td>5.59e-02</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.08</td>\n",
              "      <td>acute myeloid leukaemia</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.57e-03</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.05</td>\n",
              "      <td>7.20e-03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>3.92e-02</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.53</td>\n",
              "      <td>chronic lymphocytic leukaemia</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.22e-01</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.03</td>\n",
              "      <td>6.59e-01</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.47</td>\n",
              "      <td>2.12e-01</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.39</td>\n",
              "      <td>diffuse large B-cell lymphoma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>2.19e-02</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.95e-02</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.21</td>\n",
              "      <td>6.14e-02</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.08</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1460</th>\n",
              "      <td>9.93e-02</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.08</td>\n",
              "      <td>1.03e-01</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.14</td>\n",
              "      <td>2.90e-02</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.08</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1461</th>\n",
              "      <td>1.26e-01</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.13</td>\n",
              "      <td>7.52e-02</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.12</td>\n",
              "      <td>3.92e-02</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.11</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1462</th>\n",
              "      <td>4.68e-02</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.91e-02</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.24</td>\n",
              "      <td>3.04e-02</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1463</th>\n",
              "      <td>1.80e-01</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.74e-02</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.26</td>\n",
              "      <td>2.41e-02</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1464 rows × 67 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0     1     2     3     4     5  ...  0  1  2  3  4  5\n",
              "0     5.87e-02  0.17  0.35  0.25  0.09  0.94  ...  0  0  0  0  0  1\n",
              "1     6.38e-02  0.12  0.18  0.17  0.05  0.77  ...  0  0  0  0  0  1\n",
              "2     3.73e-01  0.13  0.17  0.13  0.22  0.12  ...  1  0  0  0  0  0\n",
              "3     1.57e-03  0.10  0.20  0.14  0.04  0.05  ...  0  0  0  1  0  0\n",
              "4     8.22e-01  0.23  0.46  0.25  0.62  0.07  ...  0  0  0  0  1  0\n",
              "...        ...   ...   ...   ...   ...   ...  ... .. .. .. .. .. ..\n",
              "1459  2.19e-02  0.18  0.30  0.21  0.09  0.66  ...  0  0  0  0  0  1\n",
              "1460  9.93e-02  0.19  0.24  0.15  0.06  0.85  ...  0  0  0  0  0  1\n",
              "1461  1.26e-01  0.12  0.34  0.21  0.12  0.90  ...  0  0  0  0  0  1\n",
              "1462  4.68e-02  0.20  0.18  0.22  0.13  0.74  ...  0  0  0  0  0  1\n",
              "1463  1.80e-01  0.16  0.14  0.14  0.07  0.31  ...  0  0  0  0  0  1\n",
              "\n",
              "[1464 rows x 67 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YXPavDgo47J"
      },
      "source": [
        "#### autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVgx1gzyrfo6"
      },
      "source": [
        "##### logistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoPJhMWmrfo7"
      },
      "source": [
        "my_param_grid = [\n",
        "    {'solver': ['newton-cg', 'lbfgs', 'saga'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l2'], 'max_iter': [200]},\n",
        "    {'solver': ['liblinear'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l1', 'l2'], 'max_iter': [200]}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qQ2CPvgrfo7"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVluISsarfo7"
      },
      "source": [
        "modellr = GridSearchCV(estimator=LogisticRegression(n_jobs=-1), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pc-qB5-rfo7",
        "outputId": "5a78d2a4-39b9-4b74-c980-36826eeeab4b"
      },
      "source": [
        "modellr.fit(dat_resaXautotr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
            "  \"number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid=[{'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l2'],\n",
              "                          'solver': ['newton-cg', 'lbfgs', 'saga']},\n",
              "                         {'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l1', 'l2'], 'solver': ['liblinear']}],\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_log_loss', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRa_EElirfo8",
        "outputId": "8aad1ef2-8617-445d-d6a2-72fbcbb19a24"
      },
      "source": [
        "print(cross_val_score(modellr.best_estimator_,dat_resaXautotr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[-0.02702654 -0.0665087  -0.02498477 -0.02724754 -0.06032169 -0.02758417\n",
            " -0.02691971 -0.03496796 -0.07548475 -0.07327274 -0.02938876 -0.05086247\n",
            " -0.02382428 -0.02054398 -0.05372249 -0.03865834 -0.02488365 -0.03243123\n",
            " -0.07160156 -0.02436901 -0.01671451 -0.03019913 -0.0244376  -0.02882095\n",
            " -0.07970846 -0.02473796 -0.03878676 -0.05684553 -0.01850898 -0.04360088\n",
            " -0.02430168 -0.07668567 -0.040952   -0.02699234 -0.02653753 -0.17121676\n",
            " -0.0410935  -0.02558574 -0.03109221 -0.03806265 -0.04033502 -0.02402109\n",
            " -0.05539536 -0.03874135 -0.03871081 -0.03767918 -0.01553576 -0.05621779\n",
            " -0.04029203 -0.03257724]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMbhROhMrfo8",
        "outputId": "3cf5e868-22eb-4397-da8a-c0d0b6e9bd88"
      },
      "source": [
        "modellr.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.04120349802656355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQkTI9Vxrfo8",
        "outputId": "2030adef-bae6-4087-9770-e6e62275426e"
      },
      "source": [
        "log_loss(dat_resaYts, modellr.predict_proba(dat_resaXautots))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.026779305470094716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTIhnHxlrfo8"
      },
      "source": [
        "modellr_best = modellr.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUFqvDMDrfo8",
        "outputId": "ae312180-45f8-461d-8f7d-c7064a7a9431"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modellr_best.predict(dat_resaXautotr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       1.00      1.00      1.00       730\n",
            "        breast adenocarcinoma       0.99      1.00      0.99       730\n",
            "                breast cancer       1.00      0.99      0.99       730\n",
            "chronic lymphocytic leukaemia       1.00      1.00      1.00       730\n",
            "diffuse large B-cell lymphoma       1.00      1.00      1.00       730\n",
            "             multiple myeloma       1.00      1.00      1.00       730\n",
            "\n",
            "                     accuracy                           1.00      4380\n",
            "                    macro avg       1.00      1.00      1.00      4380\n",
            "                 weighted avg       1.00      1.00      1.00      4380\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9wTZIIErfo9",
        "outputId": "cbaa8154-79a0-4632-fc28-5e3e8868fdb5"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modellr_best.predict(dat_resaXautots)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.99      0.99      0.99       244\n",
            "        breast adenocarcinoma       0.99      1.00      0.99       244\n",
            "                breast cancer       0.99      0.98      0.99       244\n",
            "chronic lymphocytic leukaemia       0.99      0.99      0.99       244\n",
            "diffuse large B-cell lymphoma       1.00      1.00      1.00       244\n",
            "             multiple myeloma       1.00      1.00      1.00       244\n",
            "\n",
            "                     accuracy                           0.99      1464\n",
            "                    macro avg       0.99      0.99      0.99      1464\n",
            "                 weighted avg       0.99      0.99      0.99      1464\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfZiqVtQYocv"
      },
      "source": [
        "##### random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWJ0UPAWYocw"
      },
      "source": [
        "my_param_grid = {'bootstrap': [True, False], \n",
        "                 'n_estimators': [10, 50], \n",
        "                 'min_samples_leaf': [20, 40, 60],\n",
        "                 'min_weight_fraction_leaf': [0.01, 0.02, 0.05],\n",
        "                 'criterion': ['gini', 'entropy'], \n",
        "                 'min_impurity_decrease': [1e-5, 1e-6, 1e-7]\n",
        "                 }"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUm9AX_1Yocw"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEvvAuUkYocw"
      },
      "source": [
        "modelrf = GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1,warm_start=True), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ2mHgwuYocw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226bff56-f694-4e18-828d-43e3fe1127d2"
      },
      "source": [
        "modelrf.fit(dat_resaXautotr, dat_resaYtr)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             error_score=nan,\n",
              "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                              class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              max_samples=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_sp...\n",
              "                                              random_state=None, verbose=0,\n",
              "                                              warm_start=True),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'bootstrap': [True, False],\n",
              "                         'criterion': ['gini', 'entropy'],\n",
              "                         'min_impurity_decrease': [1e-05, 1e-06, 1e-07],\n",
              "                         'min_samples_leaf': [20, 40, 60],\n",
              "                         'min_weight_fraction_leaf': [0.01, 0.02, 0.05],\n",
              "                         'n_estimators': [10, 50]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_log_loss', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifMnXIPtYocy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb9184e-1153-4e29-9d50-f904327006ab"
      },
      "source": [
        "print(cross_val_score(modelrf.best_estimator_,dat_resaXautotr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.23578534 -0.23934034 -0.27339516 -0.22959534 -0.23707166 -0.21966192\n",
            " -0.22151988 -0.24780756 -0.23672663 -0.23223694 -0.24200554 -0.27763362\n",
            " -0.23935717 -0.23294902 -0.28006083 -0.27425267 -0.21253829 -0.26190139\n",
            " -0.23809897 -0.22120413 -0.23318725 -0.22682019 -0.22098105 -0.23850386\n",
            " -0.22901245 -0.22735265 -0.22238821 -0.25752129 -0.2355892  -0.23753792\n",
            " -0.24974004 -0.25603421 -0.26601149 -0.23419911 -0.22481855 -0.22185319\n",
            " -0.23021091 -0.24318935 -0.23388264 -0.22021842 -0.25415431 -0.23395436\n",
            " -0.23578929 -0.24423211 -0.22667922 -0.23610575 -0.24583928 -0.24441401\n",
            " -0.25028572 -0.24544136]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksMRVrGCYocy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0377299-f4a8-4311-d8fe-f7f9aa3d263f"
      },
      "source": [
        "-modelrf.best_score_"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2366377716232509"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bxKv6bpYocy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132e7eed-bb40-4ef1-d8a9-72ba514df82a"
      },
      "source": [
        "log_loss(dat_resaYts, modelrf.predict_proba(dat_resaXautots))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2540928330286705"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGTvVy0eYocy"
      },
      "source": [
        "modelrf_best = modelrf.best_estimator_"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4WNNWquYocy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f58db5e-f109-4072-85cb-0bc4a7e38c15"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modelrf_best.predict(dat_resaXautotr)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.98      0.99      0.98       730\n",
            "        breast adenocarcinoma       0.98      0.99      0.99       730\n",
            "                breast cancer       0.99      0.98      0.99       730\n",
            "chronic lymphocytic leukaemia       1.00      0.98      0.99       730\n",
            "diffuse large B-cell lymphoma       0.99      0.99      0.99       730\n",
            "             multiple myeloma       0.99      0.98      0.99       730\n",
            "\n",
            "                     accuracy                           0.99      4380\n",
            "                    macro avg       0.99      0.99      0.99      4380\n",
            "                 weighted avg       0.99      0.99      0.99      4380\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GXBtMoeYocy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65de277c-7f6a-49a5-b8f2-e018f2644f90"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modelrf_best.predict(dat_resaXautots)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.98      0.99      0.98       244\n",
            "        breast adenocarcinoma       0.97      0.99      0.98       244\n",
            "                breast cancer       0.99      0.98      0.98       244\n",
            "chronic lymphocytic leukaemia       1.00      0.98      0.99       244\n",
            "diffuse large B-cell lymphoma       0.98      1.00      0.99       244\n",
            "             multiple myeloma       0.98      0.97      0.98       244\n",
            "\n",
            "                     accuracy                           0.98      1464\n",
            "                    macro avg       0.98      0.98      0.98      1464\n",
            "                 weighted avg       0.98      0.98      0.98      1464\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2trVqPTRD4iQ"
      },
      "source": [
        "##### fully connected ffnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoE7jby6D4iQ",
        "outputId": "db5ffcfc-73c8-43e1-c42c-55e87c4a1c12"
      },
      "source": [
        "dat_resaYtr"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['breast adenocarcinoma', 'acute myeloid leukaemia',\n",
              "       'diffuse large B-cell lymphoma', ..., 'multiple myeloma',\n",
              "       'multiple myeloma', 'multiple myeloma'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcNAFarqD4iS"
      },
      "source": [
        "num_classes=6\n",
        "dat_resaYtrx=pd.DataFrame(dat_resaYtr)\n",
        "dat_resaYtrx[0] = pd.Categorical(dat_resaYtrx[0])\n",
        "dat_resaYtrx['code'] = dat_resaYtrx[0].cat.codes\n",
        "dat_resaYtrx_=np.array(dat_resaYtrx['code']\n",
        "                       )\n",
        "dat_resaYtsx=pd.DataFrame(dat_resaYts)\n",
        "dat_resaYtsx[0] = pd.Categorical(dat_resaYtsx[0])\n",
        "dat_resaYtsx['code'] = dat_resaYtsx[0].cat.codes\n",
        "dat_resaYtsx_=np.array(dat_resaYtsx['code'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTW-Eu7iD4iS",
        "outputId": "fb43fa54-b560-41c8-ee64-1981b6bea41d"
      },
      "source": [
        "dat_resaYtrcl = to_categorical(dat_resaYtrx_, num_classes)\n",
        "dat_resaYtscl = to_categorical(dat_resaYtsx_, num_classes)\n",
        "dat_resaYtrcl"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjfmxPqwD4iS"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(30, activation='relu', input_shape=(60,)))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG-GKsTxD4iS"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLjIVHzjD4iS",
        "outputId": "44d10545-5294-4e37-fa20-2ec5542d3a94"
      },
      "source": [
        "history_ = model.fit(dat_resaXautotr, dat_resaYtrcl, \n",
        "                            batch_size=2000, epochs=500, verbose=1, validation_split=0.2)\n",
        " \n",
        "score_ = model.evaluate(dat_resaXautots,dat_resaYtscl , verbose=0)\n",
        "print('\\nScore: ', score_)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 12.7607 - accuracy: 0.1356 - val_loss: 8.1308 - val_accuracy: 0.0913\n",
            "Epoch 2/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 11.2579 - accuracy: 0.1393 - val_loss: 6.9727 - val_accuracy: 0.0959\n",
            "Epoch 3/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 9.8452 - accuracy: 0.1521 - val_loss: 6.0542 - val_accuracy: 0.1016\n",
            "Epoch 4/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8.5480 - accuracy: 0.1461 - val_loss: 5.3333 - val_accuracy: 0.1096\n",
            "Epoch 5/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 7.4584 - accuracy: 0.1604 - val_loss: 4.8392 - val_accuracy: 0.1096\n",
            "Epoch 6/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6.4221 - accuracy: 0.1852 - val_loss: 4.5328 - val_accuracy: 0.1016\n",
            "Epoch 7/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5.5544 - accuracy: 0.2043 - val_loss: 4.3313 - val_accuracy: 0.1107\n",
            "Epoch 8/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.9429 - accuracy: 0.2186 - val_loss: 4.1471 - val_accuracy: 0.1221\n",
            "Epoch 9/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.2353 - accuracy: 0.2549 - val_loss: 3.8886 - val_accuracy: 0.1324\n",
            "Epoch 10/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.7362 - accuracy: 0.3065 - val_loss: 3.5482 - val_accuracy: 0.1450\n",
            "Epoch 11/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.2350 - accuracy: 0.3359 - val_loss: 3.1163 - val_accuracy: 0.1655\n",
            "Epoch 12/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.8664 - accuracy: 0.3816 - val_loss: 2.6657 - val_accuracy: 0.1952\n",
            "Epoch 13/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.5520 - accuracy: 0.4061 - val_loss: 2.2522 - val_accuracy: 0.2489\n",
            "Epoch 14/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.3038 - accuracy: 0.4346 - val_loss: 1.9096 - val_accuracy: 0.3242\n",
            "Epoch 15/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0476 - accuracy: 0.4609 - val_loss: 1.6545 - val_accuracy: 0.3927\n",
            "Epoch 16/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.9419 - accuracy: 0.4966 - val_loss: 1.4890 - val_accuracy: 0.4498\n",
            "Epoch 17/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.7638 - accuracy: 0.5140 - val_loss: 1.3894 - val_accuracy: 0.5011\n",
            "Epoch 18/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6637 - accuracy: 0.5394 - val_loss: 1.3311 - val_accuracy: 0.5194\n",
            "Epoch 19/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5869 - accuracy: 0.5634 - val_loss: 1.3030 - val_accuracy: 0.5342\n",
            "Epoch 20/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5120 - accuracy: 0.5713 - val_loss: 1.2829 - val_accuracy: 0.5354\n",
            "Epoch 21/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4890 - accuracy: 0.5825 - val_loss: 1.2655 - val_accuracy: 0.5434\n",
            "Epoch 22/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.3674 - accuracy: 0.6090 - val_loss: 1.2447 - val_accuracy: 0.5514\n",
            "Epoch 23/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3052 - accuracy: 0.6090 - val_loss: 1.2293 - val_accuracy: 0.5628\n",
            "Epoch 24/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.2936 - accuracy: 0.6130 - val_loss: 1.2121 - val_accuracy: 0.5799\n",
            "Epoch 25/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2389 - accuracy: 0.6259 - val_loss: 1.1846 - val_accuracy: 0.6005\n",
            "Epoch 26/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.2007 - accuracy: 0.6373 - val_loss: 1.1541 - val_accuracy: 0.6142\n",
            "Epoch 27/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1852 - accuracy: 0.6410 - val_loss: 1.1160 - val_accuracy: 0.6244\n",
            "Epoch 28/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1047 - accuracy: 0.6544 - val_loss: 1.0735 - val_accuracy: 0.6484\n",
            "Epoch 29/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1061 - accuracy: 0.6592 - val_loss: 1.0335 - val_accuracy: 0.6598\n",
            "Epoch 30/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0159 - accuracy: 0.6712 - val_loss: 0.9917 - val_accuracy: 0.6747\n",
            "Epoch 31/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0035 - accuracy: 0.6872 - val_loss: 0.9532 - val_accuracy: 0.6861\n",
            "Epoch 32/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0432 - accuracy: 0.6784 - val_loss: 0.9191 - val_accuracy: 0.6941\n",
            "Epoch 33/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9300 - accuracy: 0.7063 - val_loss: 0.8876 - val_accuracy: 0.7021\n",
            "Epoch 34/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9344 - accuracy: 0.6961 - val_loss: 0.8522 - val_accuracy: 0.7123\n",
            "Epoch 35/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9225 - accuracy: 0.7063 - val_loss: 0.8204 - val_accuracy: 0.7215\n",
            "Epoch 36/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8874 - accuracy: 0.7149 - val_loss: 0.7918 - val_accuracy: 0.7306\n",
            "Epoch 37/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8588 - accuracy: 0.7175 - val_loss: 0.7671 - val_accuracy: 0.7386\n",
            "Epoch 38/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9017 - accuracy: 0.7215 - val_loss: 0.7400 - val_accuracy: 0.7489\n",
            "Epoch 39/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8141 - accuracy: 0.7412 - val_loss: 0.7136 - val_accuracy: 0.7511\n",
            "Epoch 40/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8263 - accuracy: 0.7337 - val_loss: 0.6857 - val_accuracy: 0.7568\n",
            "Epoch 41/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8185 - accuracy: 0.7420 - val_loss: 0.6598 - val_accuracy: 0.7637\n",
            "Epoch 42/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7636 - accuracy: 0.7537 - val_loss: 0.6349 - val_accuracy: 0.7683\n",
            "Epoch 43/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7242 - accuracy: 0.7646 - val_loss: 0.6124 - val_accuracy: 0.7763\n",
            "Epoch 44/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7090 - accuracy: 0.7683 - val_loss: 0.5902 - val_accuracy: 0.7808\n",
            "Epoch 45/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7231 - accuracy: 0.7620 - val_loss: 0.5689 - val_accuracy: 0.7877\n",
            "Epoch 46/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7028 - accuracy: 0.7674 - val_loss: 0.5432 - val_accuracy: 0.7957\n",
            "Epoch 47/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7066 - accuracy: 0.7768 - val_loss: 0.5186 - val_accuracy: 0.8071\n",
            "Epoch 48/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6843 - accuracy: 0.7763 - val_loss: 0.4966 - val_accuracy: 0.8151\n",
            "Epoch 49/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6565 - accuracy: 0.7882 - val_loss: 0.4766 - val_accuracy: 0.8208\n",
            "Epoch 50/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6512 - accuracy: 0.7917 - val_loss: 0.4569 - val_accuracy: 0.8322\n",
            "Epoch 51/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6468 - accuracy: 0.7862 - val_loss: 0.4387 - val_accuracy: 0.8379\n",
            "Epoch 52/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5878 - accuracy: 0.8128 - val_loss: 0.4210 - val_accuracy: 0.8413\n",
            "Epoch 53/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5993 - accuracy: 0.8039 - val_loss: 0.4021 - val_accuracy: 0.8470\n",
            "Epoch 54/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5846 - accuracy: 0.8096 - val_loss: 0.3846 - val_accuracy: 0.8596\n",
            "Epoch 55/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5662 - accuracy: 0.8116 - val_loss: 0.3699 - val_accuracy: 0.8756\n",
            "Epoch 56/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5762 - accuracy: 0.8134 - val_loss: 0.3573 - val_accuracy: 0.8916\n",
            "Epoch 57/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5413 - accuracy: 0.8285 - val_loss: 0.3446 - val_accuracy: 0.8961\n",
            "Epoch 58/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5339 - accuracy: 0.8322 - val_loss: 0.3330 - val_accuracy: 0.9030\n",
            "Epoch 59/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5318 - accuracy: 0.8325 - val_loss: 0.3227 - val_accuracy: 0.9087\n",
            "Epoch 60/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5554 - accuracy: 0.8231 - val_loss: 0.3133 - val_accuracy: 0.9144\n",
            "Epoch 61/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5014 - accuracy: 0.8382 - val_loss: 0.3041 - val_accuracy: 0.9167\n",
            "Epoch 62/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4938 - accuracy: 0.8399 - val_loss: 0.2942 - val_accuracy: 0.9212\n",
            "Epoch 63/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5455 - accuracy: 0.8342 - val_loss: 0.2875 - val_accuracy: 0.9212\n",
            "Epoch 64/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4545 - accuracy: 0.8579 - val_loss: 0.2799 - val_accuracy: 0.9247\n",
            "Epoch 65/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4611 - accuracy: 0.8564 - val_loss: 0.2739 - val_accuracy: 0.9247\n",
            "Epoch 66/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4596 - accuracy: 0.8542 - val_loss: 0.2683 - val_accuracy: 0.9258\n",
            "Epoch 67/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4650 - accuracy: 0.8570 - val_loss: 0.2641 - val_accuracy: 0.9281\n",
            "Epoch 68/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4414 - accuracy: 0.8690 - val_loss: 0.2600 - val_accuracy: 0.9292\n",
            "Epoch 69/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4530 - accuracy: 0.8593 - val_loss: 0.2553 - val_accuracy: 0.9315\n",
            "Epoch 70/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4230 - accuracy: 0.8662 - val_loss: 0.2507 - val_accuracy: 0.9338\n",
            "Epoch 71/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4471 - accuracy: 0.8624 - val_loss: 0.2465 - val_accuracy: 0.9349\n",
            "Epoch 72/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4353 - accuracy: 0.8696 - val_loss: 0.2410 - val_accuracy: 0.9349\n",
            "Epoch 73/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4289 - accuracy: 0.8761 - val_loss: 0.2330 - val_accuracy: 0.9372\n",
            "Epoch 74/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4025 - accuracy: 0.8838 - val_loss: 0.2231 - val_accuracy: 0.9406\n",
            "Epoch 75/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3964 - accuracy: 0.8799 - val_loss: 0.2137 - val_accuracy: 0.9463\n",
            "Epoch 76/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3687 - accuracy: 0.8850 - val_loss: 0.2065 - val_accuracy: 0.9486\n",
            "Epoch 77/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3823 - accuracy: 0.8779 - val_loss: 0.1980 - val_accuracy: 0.9532\n",
            "Epoch 78/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3785 - accuracy: 0.8913 - val_loss: 0.1915 - val_accuracy: 0.9543\n",
            "Epoch 79/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3651 - accuracy: 0.8930 - val_loss: 0.1866 - val_accuracy: 0.9566\n",
            "Epoch 80/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3611 - accuracy: 0.8898 - val_loss: 0.1829 - val_accuracy: 0.9566\n",
            "Epoch 81/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3548 - accuracy: 0.8918 - val_loss: 0.1786 - val_accuracy: 0.9589\n",
            "Epoch 82/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3604 - accuracy: 0.8924 - val_loss: 0.1750 - val_accuracy: 0.9600\n",
            "Epoch 83/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3598 - accuracy: 0.8910 - val_loss: 0.1724 - val_accuracy: 0.9612\n",
            "Epoch 84/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3533 - accuracy: 0.8955 - val_loss: 0.1675 - val_accuracy: 0.9612\n",
            "Epoch 85/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3369 - accuracy: 0.9044 - val_loss: 0.1629 - val_accuracy: 0.9623\n",
            "Epoch 86/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3446 - accuracy: 0.9018 - val_loss: 0.1593 - val_accuracy: 0.9635\n",
            "Epoch 87/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3519 - accuracy: 0.8975 - val_loss: 0.1567 - val_accuracy: 0.9646\n",
            "Epoch 88/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3312 - accuracy: 0.9027 - val_loss: 0.1540 - val_accuracy: 0.9658\n",
            "Epoch 89/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3290 - accuracy: 0.9038 - val_loss: 0.1525 - val_accuracy: 0.9658\n",
            "Epoch 90/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3101 - accuracy: 0.9024 - val_loss: 0.1501 - val_accuracy: 0.9669\n",
            "Epoch 91/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3244 - accuracy: 0.9004 - val_loss: 0.1465 - val_accuracy: 0.9669\n",
            "Epoch 92/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3268 - accuracy: 0.9075 - val_loss: 0.1436 - val_accuracy: 0.9669\n",
            "Epoch 93/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3212 - accuracy: 0.9035 - val_loss: 0.1408 - val_accuracy: 0.9680\n",
            "Epoch 94/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3076 - accuracy: 0.9033 - val_loss: 0.1363 - val_accuracy: 0.9680\n",
            "Epoch 95/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2769 - accuracy: 0.9138 - val_loss: 0.1304 - val_accuracy: 0.9703\n",
            "Epoch 96/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3061 - accuracy: 0.9130 - val_loss: 0.1249 - val_accuracy: 0.9726\n",
            "Epoch 97/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3007 - accuracy: 0.9144 - val_loss: 0.1215 - val_accuracy: 0.9726\n",
            "Epoch 98/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2945 - accuracy: 0.9112 - val_loss: 0.1184 - val_accuracy: 0.9726\n",
            "Epoch 99/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2970 - accuracy: 0.9161 - val_loss: 0.1164 - val_accuracy: 0.9715\n",
            "Epoch 100/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2614 - accuracy: 0.9241 - val_loss: 0.1152 - val_accuracy: 0.9726\n",
            "Epoch 101/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2713 - accuracy: 0.9229 - val_loss: 0.1150 - val_accuracy: 0.9726\n",
            "Epoch 102/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2762 - accuracy: 0.9178 - val_loss: 0.1140 - val_accuracy: 0.9726\n",
            "Epoch 103/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2724 - accuracy: 0.9218 - val_loss: 0.1121 - val_accuracy: 0.9726\n",
            "Epoch 104/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2619 - accuracy: 0.9195 - val_loss: 0.1096 - val_accuracy: 0.9737\n",
            "Epoch 105/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2443 - accuracy: 0.9235 - val_loss: 0.1071 - val_accuracy: 0.9737\n",
            "Epoch 106/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2388 - accuracy: 0.9281 - val_loss: 0.1035 - val_accuracy: 0.9749\n",
            "Epoch 107/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2643 - accuracy: 0.9178 - val_loss: 0.1003 - val_accuracy: 0.9772\n",
            "Epoch 108/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2488 - accuracy: 0.9289 - val_loss: 0.0972 - val_accuracy: 0.9795\n",
            "Epoch 109/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2547 - accuracy: 0.9289 - val_loss: 0.0947 - val_accuracy: 0.9806\n",
            "Epoch 110/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2457 - accuracy: 0.9258 - val_loss: 0.0915 - val_accuracy: 0.9806\n",
            "Epoch 111/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2432 - accuracy: 0.9192 - val_loss: 0.0881 - val_accuracy: 0.9829\n",
            "Epoch 112/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2289 - accuracy: 0.9315 - val_loss: 0.0854 - val_accuracy: 0.9852\n",
            "Epoch 113/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2276 - accuracy: 0.9281 - val_loss: 0.0831 - val_accuracy: 0.9852\n",
            "Epoch 114/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2239 - accuracy: 0.9352 - val_loss: 0.0809 - val_accuracy: 0.9852\n",
            "Epoch 115/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2297 - accuracy: 0.9292 - val_loss: 0.0790 - val_accuracy: 0.9852\n",
            "Epoch 116/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2228 - accuracy: 0.9381 - val_loss: 0.0775 - val_accuracy: 0.9852\n",
            "Epoch 117/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2379 - accuracy: 0.9301 - val_loss: 0.0771 - val_accuracy: 0.9863\n",
            "Epoch 118/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2091 - accuracy: 0.9384 - val_loss: 0.0774 - val_accuracy: 0.9863\n",
            "Epoch 119/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2290 - accuracy: 0.9355 - val_loss: 0.0782 - val_accuracy: 0.9863\n",
            "Epoch 120/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2158 - accuracy: 0.9326 - val_loss: 0.0789 - val_accuracy: 0.9863\n",
            "Epoch 121/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2188 - accuracy: 0.9404 - val_loss: 0.0792 - val_accuracy: 0.9829\n",
            "Epoch 122/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2044 - accuracy: 0.9418 - val_loss: 0.0778 - val_accuracy: 0.9817\n",
            "Epoch 123/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2085 - accuracy: 0.9349 - val_loss: 0.0760 - val_accuracy: 0.9817\n",
            "Epoch 124/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1976 - accuracy: 0.9392 - val_loss: 0.0745 - val_accuracy: 0.9817\n",
            "Epoch 125/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2054 - accuracy: 0.9355 - val_loss: 0.0733 - val_accuracy: 0.9817\n",
            "Epoch 126/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2040 - accuracy: 0.9341 - val_loss: 0.0713 - val_accuracy: 0.9817\n",
            "Epoch 127/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2006 - accuracy: 0.9398 - val_loss: 0.0691 - val_accuracy: 0.9852\n",
            "Epoch 128/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2143 - accuracy: 0.9412 - val_loss: 0.0669 - val_accuracy: 0.9863\n",
            "Epoch 129/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1862 - accuracy: 0.9432 - val_loss: 0.0645 - val_accuracy: 0.9863\n",
            "Epoch 130/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2026 - accuracy: 0.9384 - val_loss: 0.0628 - val_accuracy: 0.9874\n",
            "Epoch 131/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2044 - accuracy: 0.9384 - val_loss: 0.0617 - val_accuracy: 0.9874\n",
            "Epoch 132/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1986 - accuracy: 0.9424 - val_loss: 0.0612 - val_accuracy: 0.9886\n",
            "Epoch 133/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1806 - accuracy: 0.9463 - val_loss: 0.0611 - val_accuracy: 0.9886\n",
            "Epoch 134/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1858 - accuracy: 0.9475 - val_loss: 0.0607 - val_accuracy: 0.9886\n",
            "Epoch 135/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1745 - accuracy: 0.9458 - val_loss: 0.0604 - val_accuracy: 0.9886\n",
            "Epoch 136/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1935 - accuracy: 0.9409 - val_loss: 0.0602 - val_accuracy: 0.9886\n",
            "Epoch 137/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1787 - accuracy: 0.9509 - val_loss: 0.0602 - val_accuracy: 0.9886\n",
            "Epoch 138/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1822 - accuracy: 0.9509 - val_loss: 0.0606 - val_accuracy: 0.9886\n",
            "Epoch 139/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1826 - accuracy: 0.9483 - val_loss: 0.0612 - val_accuracy: 0.9886\n",
            "Epoch 140/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1820 - accuracy: 0.9466 - val_loss: 0.0617 - val_accuracy: 0.9886\n",
            "Epoch 141/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1758 - accuracy: 0.9475 - val_loss: 0.0619 - val_accuracy: 0.9886\n",
            "Epoch 142/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1700 - accuracy: 0.9478 - val_loss: 0.0614 - val_accuracy: 0.9886\n",
            "Epoch 143/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1684 - accuracy: 0.9463 - val_loss: 0.0605 - val_accuracy: 0.9886\n",
            "Epoch 144/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1812 - accuracy: 0.9432 - val_loss: 0.0594 - val_accuracy: 0.9886\n",
            "Epoch 145/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1792 - accuracy: 0.9452 - val_loss: 0.0577 - val_accuracy: 0.9886\n",
            "Epoch 146/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1634 - accuracy: 0.9512 - val_loss: 0.0563 - val_accuracy: 0.9886\n",
            "Epoch 147/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1732 - accuracy: 0.9475 - val_loss: 0.0551 - val_accuracy: 0.9886\n",
            "Epoch 148/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1627 - accuracy: 0.9518 - val_loss: 0.0542 - val_accuracy: 0.9886\n",
            "Epoch 149/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1580 - accuracy: 0.9526 - val_loss: 0.0535 - val_accuracy: 0.9886\n",
            "Epoch 150/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1724 - accuracy: 0.9481 - val_loss: 0.0528 - val_accuracy: 0.9886\n",
            "Epoch 151/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1647 - accuracy: 0.9523 - val_loss: 0.0523 - val_accuracy: 0.9886\n",
            "Epoch 152/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1659 - accuracy: 0.9532 - val_loss: 0.0518 - val_accuracy: 0.9886\n",
            "Epoch 153/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1456 - accuracy: 0.9592 - val_loss: 0.0514 - val_accuracy: 0.9886\n",
            "Epoch 154/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1699 - accuracy: 0.9521 - val_loss: 0.0508 - val_accuracy: 0.9886\n",
            "Epoch 155/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1545 - accuracy: 0.9555 - val_loss: 0.0502 - val_accuracy: 0.9886\n",
            "Epoch 156/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1520 - accuracy: 0.9549 - val_loss: 0.0499 - val_accuracy: 0.9886\n",
            "Epoch 157/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1516 - accuracy: 0.9561 - val_loss: 0.0500 - val_accuracy: 0.9886\n",
            "Epoch 158/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1450 - accuracy: 0.9586 - val_loss: 0.0499 - val_accuracy: 0.9886\n",
            "Epoch 159/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1542 - accuracy: 0.9523 - val_loss: 0.0496 - val_accuracy: 0.9886\n",
            "Epoch 160/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1503 - accuracy: 0.9580 - val_loss: 0.0493 - val_accuracy: 0.9886\n",
            "Epoch 161/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1533 - accuracy: 0.9538 - val_loss: 0.0489 - val_accuracy: 0.9886\n",
            "Epoch 162/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1494 - accuracy: 0.9549 - val_loss: 0.0486 - val_accuracy: 0.9886\n",
            "Epoch 163/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1633 - accuracy: 0.9523 - val_loss: 0.0480 - val_accuracy: 0.9886\n",
            "Epoch 164/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1488 - accuracy: 0.9546 - val_loss: 0.0481 - val_accuracy: 0.9886\n",
            "Epoch 165/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1478 - accuracy: 0.9546 - val_loss: 0.0482 - val_accuracy: 0.9886\n",
            "Epoch 166/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1472 - accuracy: 0.9541 - val_loss: 0.0483 - val_accuracy: 0.9886\n",
            "Epoch 167/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1484 - accuracy: 0.9561 - val_loss: 0.0483 - val_accuracy: 0.9886\n",
            "Epoch 168/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1452 - accuracy: 0.9569 - val_loss: 0.0482 - val_accuracy: 0.9886\n",
            "Epoch 169/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1449 - accuracy: 0.9538 - val_loss: 0.0474 - val_accuracy: 0.9886\n",
            "Epoch 170/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1348 - accuracy: 0.9589 - val_loss: 0.0461 - val_accuracy: 0.9897\n",
            "Epoch 171/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1344 - accuracy: 0.9578 - val_loss: 0.0449 - val_accuracy: 0.9897\n",
            "Epoch 172/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1344 - accuracy: 0.9603 - val_loss: 0.0443 - val_accuracy: 0.9897\n",
            "Epoch 173/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1453 - accuracy: 0.9535 - val_loss: 0.0440 - val_accuracy: 0.9897\n",
            "Epoch 174/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9529 - val_loss: 0.0438 - val_accuracy: 0.9886\n",
            "Epoch 175/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1361 - accuracy: 0.9609 - val_loss: 0.0438 - val_accuracy: 0.9886\n",
            "Epoch 176/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1456 - accuracy: 0.9543 - val_loss: 0.0436 - val_accuracy: 0.9886\n",
            "Epoch 177/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1354 - accuracy: 0.9595 - val_loss: 0.0435 - val_accuracy: 0.9886\n",
            "Epoch 178/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1250 - accuracy: 0.9635 - val_loss: 0.0432 - val_accuracy: 0.9886\n",
            "Epoch 179/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1417 - accuracy: 0.9526 - val_loss: 0.0427 - val_accuracy: 0.9886\n",
            "Epoch 180/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1335 - accuracy: 0.9595 - val_loss: 0.0421 - val_accuracy: 0.9886\n",
            "Epoch 181/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1384 - accuracy: 0.9566 - val_loss: 0.0412 - val_accuracy: 0.9897\n",
            "Epoch 182/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1373 - accuracy: 0.9578 - val_loss: 0.0404 - val_accuracy: 0.9897\n",
            "Epoch 183/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1312 - accuracy: 0.9592 - val_loss: 0.0400 - val_accuracy: 0.9897\n",
            "Epoch 184/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1230 - accuracy: 0.9575 - val_loss: 0.0398 - val_accuracy: 0.9897\n",
            "Epoch 185/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1365 - accuracy: 0.9598 - val_loss: 0.0398 - val_accuracy: 0.9897\n",
            "Epoch 186/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1380 - accuracy: 0.9586 - val_loss: 0.0396 - val_accuracy: 0.9897\n",
            "Epoch 187/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1328 - accuracy: 0.9626 - val_loss: 0.0395 - val_accuracy: 0.9897\n",
            "Epoch 188/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1295 - accuracy: 0.9580 - val_loss: 0.0394 - val_accuracy: 0.9897\n",
            "Epoch 189/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1313 - accuracy: 0.9572 - val_loss: 0.0393 - val_accuracy: 0.9897\n",
            "Epoch 190/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1248 - accuracy: 0.9612 - val_loss: 0.0392 - val_accuracy: 0.9897\n",
            "Epoch 191/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1295 - accuracy: 0.9609 - val_loss: 0.0390 - val_accuracy: 0.9897\n",
            "Epoch 192/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1499 - accuracy: 0.9558 - val_loss: 0.0389 - val_accuracy: 0.9909\n",
            "Epoch 193/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1254 - accuracy: 0.9620 - val_loss: 0.0389 - val_accuracy: 0.9909\n",
            "Epoch 194/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1200 - accuracy: 0.9598 - val_loss: 0.0391 - val_accuracy: 0.9897\n",
            "Epoch 195/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1287 - accuracy: 0.9635 - val_loss: 0.0395 - val_accuracy: 0.9897\n",
            "Epoch 196/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1267 - accuracy: 0.9632 - val_loss: 0.0399 - val_accuracy: 0.9897\n",
            "Epoch 197/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1298 - accuracy: 0.9623 - val_loss: 0.0401 - val_accuracy: 0.9897\n",
            "Epoch 198/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1301 - accuracy: 0.9578 - val_loss: 0.0400 - val_accuracy: 0.9897\n",
            "Epoch 199/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1261 - accuracy: 0.9578 - val_loss: 0.0397 - val_accuracy: 0.9897\n",
            "Epoch 200/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1103 - accuracy: 0.9663 - val_loss: 0.0389 - val_accuracy: 0.9897\n",
            "Epoch 201/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1252 - accuracy: 0.9623 - val_loss: 0.0379 - val_accuracy: 0.9909\n",
            "Epoch 202/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1371 - accuracy: 0.9575 - val_loss: 0.0372 - val_accuracy: 0.9920\n",
            "Epoch 203/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1189 - accuracy: 0.9635 - val_loss: 0.0366 - val_accuracy: 0.9920\n",
            "Epoch 204/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1128 - accuracy: 0.9663 - val_loss: 0.0361 - val_accuracy: 0.9920\n",
            "Epoch 205/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1135 - accuracy: 0.9655 - val_loss: 0.0354 - val_accuracy: 0.9920\n",
            "Epoch 206/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1079 - accuracy: 0.9660 - val_loss: 0.0349 - val_accuracy: 0.9932\n",
            "Epoch 207/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1083 - accuracy: 0.9638 - val_loss: 0.0346 - val_accuracy: 0.9932\n",
            "Epoch 208/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1113 - accuracy: 0.9658 - val_loss: 0.0344 - val_accuracy: 0.9943\n",
            "Epoch 209/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1122 - accuracy: 0.9663 - val_loss: 0.0342 - val_accuracy: 0.9943\n",
            "Epoch 210/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1106 - accuracy: 0.9669 - val_loss: 0.0342 - val_accuracy: 0.9943\n",
            "Epoch 211/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1166 - accuracy: 0.9635 - val_loss: 0.0341 - val_accuracy: 0.9943\n",
            "Epoch 212/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1155 - accuracy: 0.9652 - val_loss: 0.0342 - val_accuracy: 0.9943\n",
            "Epoch 213/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1215 - accuracy: 0.9652 - val_loss: 0.0345 - val_accuracy: 0.9943\n",
            "Epoch 214/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1185 - accuracy: 0.9652 - val_loss: 0.0349 - val_accuracy: 0.9943\n",
            "Epoch 215/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1141 - accuracy: 0.9672 - val_loss: 0.0350 - val_accuracy: 0.9943\n",
            "Epoch 216/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1044 - accuracy: 0.9683 - val_loss: 0.0349 - val_accuracy: 0.9943\n",
            "Epoch 217/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1055 - accuracy: 0.9697 - val_loss: 0.0347 - val_accuracy: 0.9943\n",
            "Epoch 218/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1073 - accuracy: 0.9675 - val_loss: 0.0344 - val_accuracy: 0.9943\n",
            "Epoch 219/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1092 - accuracy: 0.9658 - val_loss: 0.0342 - val_accuracy: 0.9943\n",
            "Epoch 220/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1043 - accuracy: 0.9669 - val_loss: 0.0341 - val_accuracy: 0.9943\n",
            "Epoch 221/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1078 - accuracy: 0.9692 - val_loss: 0.0342 - val_accuracy: 0.9943\n",
            "Epoch 222/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1060 - accuracy: 0.9680 - val_loss: 0.0343 - val_accuracy: 0.9943\n",
            "Epoch 223/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1062 - accuracy: 0.9663 - val_loss: 0.0343 - val_accuracy: 0.9943\n",
            "Epoch 224/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1074 - accuracy: 0.9660 - val_loss: 0.0340 - val_accuracy: 0.9943\n",
            "Epoch 225/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1194 - accuracy: 0.9620 - val_loss: 0.0338 - val_accuracy: 0.9943\n",
            "Epoch 226/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1107 - accuracy: 0.9635 - val_loss: 0.0338 - val_accuracy: 0.9943\n",
            "Epoch 227/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1081 - accuracy: 0.9660 - val_loss: 0.0338 - val_accuracy: 0.9943\n",
            "Epoch 228/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1110 - accuracy: 0.9643 - val_loss: 0.0336 - val_accuracy: 0.9943\n",
            "Epoch 229/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1055 - accuracy: 0.9686 - val_loss: 0.0333 - val_accuracy: 0.9943\n",
            "Epoch 230/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1006 - accuracy: 0.9686 - val_loss: 0.0331 - val_accuracy: 0.9943\n",
            "Epoch 231/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1098 - accuracy: 0.9660 - val_loss: 0.0330 - val_accuracy: 0.9943\n",
            "Epoch 232/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1041 - accuracy: 0.9715 - val_loss: 0.0326 - val_accuracy: 0.9943\n",
            "Epoch 233/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1149 - accuracy: 0.9638 - val_loss: 0.0323 - val_accuracy: 0.9943\n",
            "Epoch 234/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0984 - accuracy: 0.9695 - val_loss: 0.0324 - val_accuracy: 0.9943\n",
            "Epoch 235/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1083 - accuracy: 0.9632 - val_loss: 0.0324 - val_accuracy: 0.9943\n",
            "Epoch 236/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1036 - accuracy: 0.9652 - val_loss: 0.0322 - val_accuracy: 0.9943\n",
            "Epoch 237/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1007 - accuracy: 0.9692 - val_loss: 0.0321 - val_accuracy: 0.9943\n",
            "Epoch 238/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1054 - accuracy: 0.9652 - val_loss: 0.0318 - val_accuracy: 0.9943\n",
            "Epoch 239/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1071 - accuracy: 0.9689 - val_loss: 0.0314 - val_accuracy: 0.9943\n",
            "Epoch 240/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1075 - accuracy: 0.9680 - val_loss: 0.0309 - val_accuracy: 0.9943\n",
            "Epoch 241/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1008 - accuracy: 0.9695 - val_loss: 0.0305 - val_accuracy: 0.9943\n",
            "Epoch 242/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1086 - accuracy: 0.9678 - val_loss: 0.0301 - val_accuracy: 0.9943\n",
            "Epoch 243/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1012 - accuracy: 0.9703 - val_loss: 0.0299 - val_accuracy: 0.9943\n",
            "Epoch 244/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0971 - accuracy: 0.9712 - val_loss: 0.0294 - val_accuracy: 0.9943\n",
            "Epoch 245/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0990 - accuracy: 0.9700 - val_loss: 0.0292 - val_accuracy: 0.9943\n",
            "Epoch 246/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1063 - accuracy: 0.9680 - val_loss: 0.0291 - val_accuracy: 0.9943\n",
            "Epoch 247/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0938 - accuracy: 0.9692 - val_loss: 0.0294 - val_accuracy: 0.9943\n",
            "Epoch 248/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0908 - accuracy: 0.9700 - val_loss: 0.0295 - val_accuracy: 0.9943\n",
            "Epoch 249/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0945 - accuracy: 0.9737 - val_loss: 0.0296 - val_accuracy: 0.9943\n",
            "Epoch 250/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0898 - accuracy: 0.9726 - val_loss: 0.0296 - val_accuracy: 0.9943\n",
            "Epoch 251/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1042 - accuracy: 0.9683 - val_loss: 0.0297 - val_accuracy: 0.9943\n",
            "Epoch 252/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1037 - accuracy: 0.9683 - val_loss: 0.0295 - val_accuracy: 0.9943\n",
            "Epoch 253/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0920 - accuracy: 0.9735 - val_loss: 0.0292 - val_accuracy: 0.9943\n",
            "Epoch 254/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0941 - accuracy: 0.9715 - val_loss: 0.0292 - val_accuracy: 0.9943\n",
            "Epoch 255/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0946 - accuracy: 0.9683 - val_loss: 0.0294 - val_accuracy: 0.9943\n",
            "Epoch 256/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0941 - accuracy: 0.9729 - val_loss: 0.0294 - val_accuracy: 0.9943\n",
            "Epoch 257/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0953 - accuracy: 0.9717 - val_loss: 0.0295 - val_accuracy: 0.9943\n",
            "Epoch 258/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0934 - accuracy: 0.9723 - val_loss: 0.0297 - val_accuracy: 0.9943\n",
            "Epoch 259/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1052 - accuracy: 0.9643 - val_loss: 0.0298 - val_accuracy: 0.9943\n",
            "Epoch 260/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0997 - accuracy: 0.9695 - val_loss: 0.0300 - val_accuracy: 0.9943\n",
            "Epoch 261/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0937 - accuracy: 0.9706 - val_loss: 0.0302 - val_accuracy: 0.9943\n",
            "Epoch 262/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0943 - accuracy: 0.9715 - val_loss: 0.0302 - val_accuracy: 0.9943\n",
            "Epoch 263/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0929 - accuracy: 0.9737 - val_loss: 0.0301 - val_accuracy: 0.9943\n",
            "Epoch 264/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0997 - accuracy: 0.9666 - val_loss: 0.0298 - val_accuracy: 0.9943\n",
            "Epoch 265/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0957 - accuracy: 0.9703 - val_loss: 0.0295 - val_accuracy: 0.9943\n",
            "Epoch 266/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0892 - accuracy: 0.9723 - val_loss: 0.0291 - val_accuracy: 0.9943\n",
            "Epoch 267/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1006 - accuracy: 0.9680 - val_loss: 0.0289 - val_accuracy: 0.9943\n",
            "Epoch 268/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0877 - accuracy: 0.9726 - val_loss: 0.0289 - val_accuracy: 0.9943\n",
            "Epoch 269/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0928 - accuracy: 0.9703 - val_loss: 0.0288 - val_accuracy: 0.9943\n",
            "Epoch 270/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0923 - accuracy: 0.9737 - val_loss: 0.0287 - val_accuracy: 0.9943\n",
            "Epoch 271/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0990 - accuracy: 0.9709 - val_loss: 0.0287 - val_accuracy: 0.9943\n",
            "Epoch 272/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0763 - accuracy: 0.9763 - val_loss: 0.0288 - val_accuracy: 0.9943\n",
            "Epoch 273/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0920 - accuracy: 0.9706 - val_loss: 0.0287 - val_accuracy: 0.9943\n",
            "Epoch 274/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0894 - accuracy: 0.9720 - val_loss: 0.0283 - val_accuracy: 0.9943\n",
            "Epoch 275/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0872 - accuracy: 0.9717 - val_loss: 0.0280 - val_accuracy: 0.9943\n",
            "Epoch 276/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0983 - accuracy: 0.9683 - val_loss: 0.0278 - val_accuracy: 0.9943\n",
            "Epoch 277/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0860 - accuracy: 0.9746 - val_loss: 0.0276 - val_accuracy: 0.9943\n",
            "Epoch 278/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0938 - accuracy: 0.9689 - val_loss: 0.0274 - val_accuracy: 0.9943\n",
            "Epoch 279/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0888 - accuracy: 0.9717 - val_loss: 0.0273 - val_accuracy: 0.9943\n",
            "Epoch 280/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0831 - accuracy: 0.9732 - val_loss: 0.0273 - val_accuracy: 0.9943\n",
            "Epoch 281/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0815 - accuracy: 0.9746 - val_loss: 0.0272 - val_accuracy: 0.9943\n",
            "Epoch 282/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0855 - accuracy: 0.9715 - val_loss: 0.0274 - val_accuracy: 0.9943\n",
            "Epoch 283/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0902 - accuracy: 0.9712 - val_loss: 0.0274 - val_accuracy: 0.9943\n",
            "Epoch 284/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0854 - accuracy: 0.9706 - val_loss: 0.0271 - val_accuracy: 0.9943\n",
            "Epoch 285/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0846 - accuracy: 0.9737 - val_loss: 0.0269 - val_accuracy: 0.9943\n",
            "Epoch 286/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0847 - accuracy: 0.9777 - val_loss: 0.0270 - val_accuracy: 0.9943\n",
            "Epoch 287/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0807 - accuracy: 0.9720 - val_loss: 0.0271 - val_accuracy: 0.9943\n",
            "Epoch 288/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0833 - accuracy: 0.9743 - val_loss: 0.0274 - val_accuracy: 0.9943\n",
            "Epoch 289/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0903 - accuracy: 0.9692 - val_loss: 0.0276 - val_accuracy: 0.9943\n",
            "Epoch 290/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0813 - accuracy: 0.9775 - val_loss: 0.0281 - val_accuracy: 0.9943\n",
            "Epoch 291/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0912 - accuracy: 0.9717 - val_loss: 0.0285 - val_accuracy: 0.9943\n",
            "Epoch 292/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0946 - accuracy: 0.9683 - val_loss: 0.0288 - val_accuracy: 0.9943\n",
            "Epoch 293/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0967 - accuracy: 0.9717 - val_loss: 0.0287 - val_accuracy: 0.9943\n",
            "Epoch 294/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0873 - accuracy: 0.9737 - val_loss: 0.0284 - val_accuracy: 0.9943\n",
            "Epoch 295/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0891 - accuracy: 0.9703 - val_loss: 0.0282 - val_accuracy: 0.9943\n",
            "Epoch 296/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0852 - accuracy: 0.9729 - val_loss: 0.0278 - val_accuracy: 0.9943\n",
            "Epoch 297/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.9743 - val_loss: 0.0275 - val_accuracy: 0.9943\n",
            "Epoch 298/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0816 - accuracy: 0.9777 - val_loss: 0.0272 - val_accuracy: 0.9943\n",
            "Epoch 299/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0839 - accuracy: 0.9743 - val_loss: 0.0270 - val_accuracy: 0.9943\n",
            "Epoch 300/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0849 - accuracy: 0.9757 - val_loss: 0.0268 - val_accuracy: 0.9943\n",
            "Epoch 301/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0741 - accuracy: 0.9743 - val_loss: 0.0265 - val_accuracy: 0.9943\n",
            "Epoch 302/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0811 - accuracy: 0.9766 - val_loss: 0.0264 - val_accuracy: 0.9943\n",
            "Epoch 303/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0821 - accuracy: 0.9723 - val_loss: 0.0264 - val_accuracy: 0.9943\n",
            "Epoch 304/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0765 - accuracy: 0.9749 - val_loss: 0.0266 - val_accuracy: 0.9943\n",
            "Epoch 305/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0826 - accuracy: 0.9726 - val_loss: 0.0268 - val_accuracy: 0.9943\n",
            "Epoch 306/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0950 - accuracy: 0.9689 - val_loss: 0.0269 - val_accuracy: 0.9943\n",
            "Epoch 307/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0778 - accuracy: 0.9772 - val_loss: 0.0268 - val_accuracy: 0.9943\n",
            "Epoch 308/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0817 - accuracy: 0.9755 - val_loss: 0.0266 - val_accuracy: 0.9943\n",
            "Epoch 309/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0771 - accuracy: 0.9766 - val_loss: 0.0262 - val_accuracy: 0.9954\n",
            "Epoch 310/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0792 - accuracy: 0.9755 - val_loss: 0.0260 - val_accuracy: 0.9954\n",
            "Epoch 311/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0826 - accuracy: 0.9737 - val_loss: 0.0258 - val_accuracy: 0.9954\n",
            "Epoch 312/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0749 - accuracy: 0.9792 - val_loss: 0.0257 - val_accuracy: 0.9954\n",
            "Epoch 313/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0823 - accuracy: 0.9752 - val_loss: 0.0254 - val_accuracy: 0.9954\n",
            "Epoch 314/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0792 - accuracy: 0.9752 - val_loss: 0.0251 - val_accuracy: 0.9954\n",
            "Epoch 315/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0762 - accuracy: 0.9775 - val_loss: 0.0246 - val_accuracy: 0.9954\n",
            "Epoch 316/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0880 - accuracy: 0.9775 - val_loss: 0.0241 - val_accuracy: 0.9954\n",
            "Epoch 317/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0764 - accuracy: 0.9737 - val_loss: 0.0241 - val_accuracy: 0.9954\n",
            "Epoch 318/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0768 - accuracy: 0.9737 - val_loss: 0.0244 - val_accuracy: 0.9954\n",
            "Epoch 319/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0828 - accuracy: 0.9743 - val_loss: 0.0250 - val_accuracy: 0.9954\n",
            "Epoch 320/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0839 - accuracy: 0.9732 - val_loss: 0.0259 - val_accuracy: 0.9943\n",
            "Epoch 321/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0747 - accuracy: 0.9766 - val_loss: 0.0265 - val_accuracy: 0.9943\n",
            "Epoch 322/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0758 - accuracy: 0.9766 - val_loss: 0.0270 - val_accuracy: 0.9943\n",
            "Epoch 323/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0756 - accuracy: 0.9757 - val_loss: 0.0272 - val_accuracy: 0.9943\n",
            "Epoch 324/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0717 - accuracy: 0.9760 - val_loss: 0.0273 - val_accuracy: 0.9943\n",
            "Epoch 325/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0773 - accuracy: 0.9789 - val_loss: 0.0269 - val_accuracy: 0.9943\n",
            "Epoch 326/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0756 - accuracy: 0.9777 - val_loss: 0.0260 - val_accuracy: 0.9943\n",
            "Epoch 327/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0790 - accuracy: 0.9755 - val_loss: 0.0250 - val_accuracy: 0.9943\n",
            "Epoch 328/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0791 - accuracy: 0.9755 - val_loss: 0.0243 - val_accuracy: 0.9943\n",
            "Epoch 329/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0766 - accuracy: 0.9777 - val_loss: 0.0238 - val_accuracy: 0.9943\n",
            "Epoch 330/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0720 - accuracy: 0.9809 - val_loss: 0.0233 - val_accuracy: 0.9954\n",
            "Epoch 331/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0839 - accuracy: 0.9749 - val_loss: 0.0232 - val_accuracy: 0.9954\n",
            "Epoch 332/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0806 - accuracy: 0.9752 - val_loss: 0.0234 - val_accuracy: 0.9943\n",
            "Epoch 333/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0745 - accuracy: 0.9760 - val_loss: 0.0239 - val_accuracy: 0.9943\n",
            "Epoch 334/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0749 - accuracy: 0.9755 - val_loss: 0.0247 - val_accuracy: 0.9943\n",
            "Epoch 335/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0728 - accuracy: 0.9783 - val_loss: 0.0253 - val_accuracy: 0.9943\n",
            "Epoch 336/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0742 - accuracy: 0.9763 - val_loss: 0.0256 - val_accuracy: 0.9943\n",
            "Epoch 337/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0792 - accuracy: 0.9749 - val_loss: 0.0258 - val_accuracy: 0.9943\n",
            "Epoch 338/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0715 - accuracy: 0.9797 - val_loss: 0.0257 - val_accuracy: 0.9943\n",
            "Epoch 339/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0810 - accuracy: 0.9772 - val_loss: 0.0258 - val_accuracy: 0.9943\n",
            "Epoch 340/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0701 - accuracy: 0.9803 - val_loss: 0.0257 - val_accuracy: 0.9943\n",
            "Epoch 341/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0676 - accuracy: 0.9814 - val_loss: 0.0256 - val_accuracy: 0.9943\n",
            "Epoch 342/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0726 - accuracy: 0.9772 - val_loss: 0.0257 - val_accuracy: 0.9943\n",
            "Epoch 343/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0699 - accuracy: 0.9769 - val_loss: 0.0256 - val_accuracy: 0.9943\n",
            "Epoch 344/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0766 - accuracy: 0.9763 - val_loss: 0.0254 - val_accuracy: 0.9943\n",
            "Epoch 345/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0729 - accuracy: 0.9789 - val_loss: 0.0252 - val_accuracy: 0.9943\n",
            "Epoch 346/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0697 - accuracy: 0.9792 - val_loss: 0.0250 - val_accuracy: 0.9943\n",
            "Epoch 347/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0660 - accuracy: 0.9812 - val_loss: 0.0250 - val_accuracy: 0.9943\n",
            "Epoch 348/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0733 - accuracy: 0.9775 - val_loss: 0.0248 - val_accuracy: 0.9943\n",
            "Epoch 349/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0698 - accuracy: 0.9803 - val_loss: 0.0247 - val_accuracy: 0.9943\n",
            "Epoch 350/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0735 - accuracy: 0.9757 - val_loss: 0.0245 - val_accuracy: 0.9954\n",
            "Epoch 351/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0694 - accuracy: 0.9792 - val_loss: 0.0243 - val_accuracy: 0.9954\n",
            "Epoch 352/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0800 - accuracy: 0.9737 - val_loss: 0.0242 - val_accuracy: 0.9954\n",
            "Epoch 353/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0729 - accuracy: 0.9772 - val_loss: 0.0240 - val_accuracy: 0.9954\n",
            "Epoch 354/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0812 - accuracy: 0.9752 - val_loss: 0.0239 - val_accuracy: 0.9954\n",
            "Epoch 355/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0749 - accuracy: 0.9797 - val_loss: 0.0241 - val_accuracy: 0.9954\n",
            "Epoch 356/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0745 - accuracy: 0.9777 - val_loss: 0.0243 - val_accuracy: 0.9954\n",
            "Epoch 357/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0724 - accuracy: 0.9789 - val_loss: 0.0242 - val_accuracy: 0.9954\n",
            "Epoch 358/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0779 - accuracy: 0.9769 - val_loss: 0.0240 - val_accuracy: 0.9954\n",
            "Epoch 359/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0719 - accuracy: 0.9780 - val_loss: 0.0235 - val_accuracy: 0.9954\n",
            "Epoch 360/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0727 - accuracy: 0.9792 - val_loss: 0.0233 - val_accuracy: 0.9954\n",
            "Epoch 361/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0667 - accuracy: 0.9806 - val_loss: 0.0232 - val_accuracy: 0.9954\n",
            "Epoch 362/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0828 - accuracy: 0.9780 - val_loss: 0.0232 - val_accuracy: 0.9954\n",
            "Epoch 363/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0820 - accuracy: 0.9740 - val_loss: 0.0233 - val_accuracy: 0.9954\n",
            "Epoch 364/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0700 - accuracy: 0.9792 - val_loss: 0.0235 - val_accuracy: 0.9954\n",
            "Epoch 365/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0655 - accuracy: 0.9817 - val_loss: 0.0235 - val_accuracy: 0.9954\n",
            "Epoch 366/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0687 - accuracy: 0.9812 - val_loss: 0.0236 - val_accuracy: 0.9954\n",
            "Epoch 367/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0726 - accuracy: 0.9783 - val_loss: 0.0238 - val_accuracy: 0.9954\n",
            "Epoch 368/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0710 - accuracy: 0.9797 - val_loss: 0.0238 - val_accuracy: 0.9954\n",
            "Epoch 369/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0709 - accuracy: 0.9792 - val_loss: 0.0238 - val_accuracy: 0.9954\n",
            "Epoch 370/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0750 - accuracy: 0.9786 - val_loss: 0.0240 - val_accuracy: 0.9954\n",
            "Epoch 371/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0686 - accuracy: 0.9806 - val_loss: 0.0241 - val_accuracy: 0.9954\n",
            "Epoch 372/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0620 - accuracy: 0.9820 - val_loss: 0.0242 - val_accuracy: 0.9954\n",
            "Epoch 373/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0672 - accuracy: 0.9795 - val_loss: 0.0242 - val_accuracy: 0.9954\n",
            "Epoch 374/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0684 - accuracy: 0.9795 - val_loss: 0.0243 - val_accuracy: 0.9954\n",
            "Epoch 375/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0740 - accuracy: 0.9803 - val_loss: 0.0241 - val_accuracy: 0.9954\n",
            "Epoch 376/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0714 - accuracy: 0.9795 - val_loss: 0.0238 - val_accuracy: 0.9954\n",
            "Epoch 377/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0664 - accuracy: 0.9789 - val_loss: 0.0235 - val_accuracy: 0.9954\n",
            "Epoch 378/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0723 - accuracy: 0.9795 - val_loss: 0.0235 - val_accuracy: 0.9954\n",
            "Epoch 379/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0693 - accuracy: 0.9792 - val_loss: 0.0234 - val_accuracy: 0.9954\n",
            "Epoch 380/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0597 - accuracy: 0.9814 - val_loss: 0.0232 - val_accuracy: 0.9954\n",
            "Epoch 381/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0756 - accuracy: 0.9732 - val_loss: 0.0229 - val_accuracy: 0.9954\n",
            "Epoch 382/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0679 - accuracy: 0.9809 - val_loss: 0.0226 - val_accuracy: 0.9954\n",
            "Epoch 383/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0722 - accuracy: 0.9780 - val_loss: 0.0223 - val_accuracy: 0.9954\n",
            "Epoch 384/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0693 - accuracy: 0.9769 - val_loss: 0.0221 - val_accuracy: 0.9954\n",
            "Epoch 385/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0687 - accuracy: 0.9795 - val_loss: 0.0220 - val_accuracy: 0.9954\n",
            "Epoch 386/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0693 - accuracy: 0.9777 - val_loss: 0.0221 - val_accuracy: 0.9954\n",
            "Epoch 387/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0655 - accuracy: 0.9777 - val_loss: 0.0226 - val_accuracy: 0.9954\n",
            "Epoch 388/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0703 - accuracy: 0.9786 - val_loss: 0.0230 - val_accuracy: 0.9954\n",
            "Epoch 389/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0654 - accuracy: 0.9809 - val_loss: 0.0230 - val_accuracy: 0.9954\n",
            "Epoch 390/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0619 - accuracy: 0.9809 - val_loss: 0.0228 - val_accuracy: 0.9954\n",
            "Epoch 391/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0658 - accuracy: 0.9780 - val_loss: 0.0227 - val_accuracy: 0.9954\n",
            "Epoch 392/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0651 - accuracy: 0.9803 - val_loss: 0.0227 - val_accuracy: 0.9954\n",
            "Epoch 393/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0661 - accuracy: 0.9783 - val_loss: 0.0227 - val_accuracy: 0.9954\n",
            "Epoch 394/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0664 - accuracy: 0.9814 - val_loss: 0.0227 - val_accuracy: 0.9954\n",
            "Epoch 395/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0642 - accuracy: 0.9809 - val_loss: 0.0226 - val_accuracy: 0.9954\n",
            "Epoch 396/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0734 - accuracy: 0.9795 - val_loss: 0.0224 - val_accuracy: 0.9954\n",
            "Epoch 397/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0683 - accuracy: 0.9806 - val_loss: 0.0223 - val_accuracy: 0.9954\n",
            "Epoch 398/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0714 - accuracy: 0.9800 - val_loss: 0.0222 - val_accuracy: 0.9954\n",
            "Epoch 399/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0671 - accuracy: 0.9809 - val_loss: 0.0221 - val_accuracy: 0.9954\n",
            "Epoch 400/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0661 - accuracy: 0.9792 - val_loss: 0.0224 - val_accuracy: 0.9954\n",
            "Epoch 401/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0624 - accuracy: 0.9817 - val_loss: 0.0227 - val_accuracy: 0.9954\n",
            "Epoch 402/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0608 - accuracy: 0.9809 - val_loss: 0.0230 - val_accuracy: 0.9954\n",
            "Epoch 403/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0649 - accuracy: 0.9803 - val_loss: 0.0232 - val_accuracy: 0.9954\n",
            "Epoch 404/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0650 - accuracy: 0.9806 - val_loss: 0.0230 - val_accuracy: 0.9954\n",
            "Epoch 405/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0666 - accuracy: 0.9780 - val_loss: 0.0228 - val_accuracy: 0.9954\n",
            "Epoch 406/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0683 - accuracy: 0.9789 - val_loss: 0.0226 - val_accuracy: 0.9954\n",
            "Epoch 407/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0626 - accuracy: 0.9809 - val_loss: 0.0222 - val_accuracy: 0.9954\n",
            "Epoch 408/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0626 - accuracy: 0.9814 - val_loss: 0.0218 - val_accuracy: 0.9954\n",
            "Epoch 409/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0215 - val_accuracy: 0.9954\n",
            "Epoch 410/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0605 - accuracy: 0.9823 - val_loss: 0.0218 - val_accuracy: 0.9954\n",
            "Epoch 411/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0655 - accuracy: 0.9797 - val_loss: 0.0221 - val_accuracy: 0.9954\n",
            "Epoch 412/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0621 - accuracy: 0.9803 - val_loss: 0.0224 - val_accuracy: 0.9954\n",
            "Epoch 413/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0574 - accuracy: 0.9832 - val_loss: 0.0228 - val_accuracy: 0.9954\n",
            "Epoch 414/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0583 - accuracy: 0.9829 - val_loss: 0.0230 - val_accuracy: 0.9954\n",
            "Epoch 415/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0618 - accuracy: 0.9806 - val_loss: 0.0232 - val_accuracy: 0.9954\n",
            "Epoch 416/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0608 - accuracy: 0.9814 - val_loss: 0.0232 - val_accuracy: 0.9954\n",
            "Epoch 417/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0631 - accuracy: 0.9789 - val_loss: 0.0231 - val_accuracy: 0.9954\n",
            "Epoch 418/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0594 - accuracy: 0.9812 - val_loss: 0.0230 - val_accuracy: 0.9954\n",
            "Epoch 419/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0618 - accuracy: 0.9843 - val_loss: 0.0228 - val_accuracy: 0.9954\n",
            "Epoch 420/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0608 - accuracy: 0.9817 - val_loss: 0.0224 - val_accuracy: 0.9954\n",
            "Epoch 421/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0644 - accuracy: 0.9795 - val_loss: 0.0220 - val_accuracy: 0.9954\n",
            "Epoch 422/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0605 - accuracy: 0.9820 - val_loss: 0.0214 - val_accuracy: 0.9954\n",
            "Epoch 423/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0594 - accuracy: 0.9829 - val_loss: 0.0209 - val_accuracy: 0.9954\n",
            "Epoch 424/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0673 - accuracy: 0.9786 - val_loss: 0.0206 - val_accuracy: 0.9954\n",
            "Epoch 425/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0637 - accuracy: 0.9800 - val_loss: 0.0205 - val_accuracy: 0.9954\n",
            "Epoch 426/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0568 - accuracy: 0.9840 - val_loss: 0.0205 - val_accuracy: 0.9954\n",
            "Epoch 427/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0618 - accuracy: 0.9806 - val_loss: 0.0204 - val_accuracy: 0.9954\n",
            "Epoch 428/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0610 - accuracy: 0.9829 - val_loss: 0.0204 - val_accuracy: 0.9954\n",
            "Epoch 429/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0569 - accuracy: 0.9849 - val_loss: 0.0205 - val_accuracy: 0.9954\n",
            "Epoch 430/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0685 - accuracy: 0.9803 - val_loss: 0.0207 - val_accuracy: 0.9954\n",
            "Epoch 431/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0650 - accuracy: 0.9809 - val_loss: 0.0212 - val_accuracy: 0.9954\n",
            "Epoch 432/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0662 - accuracy: 0.9797 - val_loss: 0.0217 - val_accuracy: 0.9954\n",
            "Epoch 433/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0617 - accuracy: 0.9806 - val_loss: 0.0221 - val_accuracy: 0.9954\n",
            "Epoch 434/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0566 - accuracy: 0.9829 - val_loss: 0.0225 - val_accuracy: 0.9954\n",
            "Epoch 435/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0617 - accuracy: 0.9812 - val_loss: 0.0226 - val_accuracy: 0.9954\n",
            "Epoch 436/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0544 - accuracy: 0.9823 - val_loss: 0.0224 - val_accuracy: 0.9954\n",
            "Epoch 437/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0676 - accuracy: 0.9806 - val_loss: 0.0220 - val_accuracy: 0.9954\n",
            "Epoch 438/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0611 - accuracy: 0.9812 - val_loss: 0.0217 - val_accuracy: 0.9954\n",
            "Epoch 439/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0577 - accuracy: 0.9837 - val_loss: 0.0215 - val_accuracy: 0.9954\n",
            "Epoch 440/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0698 - accuracy: 0.9797 - val_loss: 0.0214 - val_accuracy: 0.9954\n",
            "Epoch 441/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0639 - accuracy: 0.9809 - val_loss: 0.0212 - val_accuracy: 0.9954\n",
            "Epoch 442/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0580 - accuracy: 0.9834 - val_loss: 0.0211 - val_accuracy: 0.9954\n",
            "Epoch 443/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0572 - accuracy: 0.9840 - val_loss: 0.0211 - val_accuracy: 0.9954\n",
            "Epoch 444/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0647 - accuracy: 0.9789 - val_loss: 0.0212 - val_accuracy: 0.9954\n",
            "Epoch 445/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0579 - accuracy: 0.9817 - val_loss: 0.0213 - val_accuracy: 0.9954\n",
            "Epoch 446/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0642 - accuracy: 0.9775 - val_loss: 0.0214 - val_accuracy: 0.9954\n",
            "Epoch 447/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0587 - accuracy: 0.9823 - val_loss: 0.0212 - val_accuracy: 0.9954\n",
            "Epoch 448/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0578 - accuracy: 0.9832 - val_loss: 0.0209 - val_accuracy: 0.9954\n",
            "Epoch 449/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0598 - accuracy: 0.9826 - val_loss: 0.0207 - val_accuracy: 0.9954\n",
            "Epoch 450/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0595 - accuracy: 0.9809 - val_loss: 0.0204 - val_accuracy: 0.9954\n",
            "Epoch 451/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0557 - accuracy: 0.9849 - val_loss: 0.0201 - val_accuracy: 0.9954\n",
            "Epoch 452/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0616 - accuracy: 0.9817 - val_loss: 0.0199 - val_accuracy: 0.9954\n",
            "Epoch 453/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0621 - accuracy: 0.9826 - val_loss: 0.0199 - val_accuracy: 0.9954\n",
            "Epoch 454/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0582 - accuracy: 0.9849 - val_loss: 0.0200 - val_accuracy: 0.9954\n",
            "Epoch 455/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0585 - accuracy: 0.9840 - val_loss: 0.0201 - val_accuracy: 0.9954\n",
            "Epoch 456/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0593 - accuracy: 0.9806 - val_loss: 0.0202 - val_accuracy: 0.9954\n",
            "Epoch 457/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0591 - accuracy: 0.9837 - val_loss: 0.0201 - val_accuracy: 0.9954\n",
            "Epoch 458/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0554 - accuracy: 0.9826 - val_loss: 0.0199 - val_accuracy: 0.9954\n",
            "Epoch 459/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0512 - accuracy: 0.9834 - val_loss: 0.0197 - val_accuracy: 0.9954\n",
            "Epoch 460/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0588 - accuracy: 0.9832 - val_loss: 0.0196 - val_accuracy: 0.9954\n",
            "Epoch 461/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0618 - accuracy: 0.9814 - val_loss: 0.0196 - val_accuracy: 0.9954\n",
            "Epoch 462/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0648 - accuracy: 0.9800 - val_loss: 0.0197 - val_accuracy: 0.9954\n",
            "Epoch 463/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0621 - accuracy: 0.9817 - val_loss: 0.0200 - val_accuracy: 0.9954\n",
            "Epoch 464/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0568 - accuracy: 0.9817 - val_loss: 0.0203 - val_accuracy: 0.9954\n",
            "Epoch 465/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0564 - accuracy: 0.9846 - val_loss: 0.0204 - val_accuracy: 0.9954\n",
            "Epoch 466/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0658 - accuracy: 0.9806 - val_loss: 0.0205 - val_accuracy: 0.9954\n",
            "Epoch 467/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0588 - accuracy: 0.9826 - val_loss: 0.0206 - val_accuracy: 0.9954\n",
            "Epoch 468/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0583 - accuracy: 0.9820 - val_loss: 0.0207 - val_accuracy: 0.9954\n",
            "Epoch 469/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0603 - accuracy: 0.9817 - val_loss: 0.0207 - val_accuracy: 0.9954\n",
            "Epoch 470/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0570 - accuracy: 0.9843 - val_loss: 0.0203 - val_accuracy: 0.9954\n",
            "Epoch 471/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0514 - accuracy: 0.9829 - val_loss: 0.0197 - val_accuracy: 0.9954\n",
            "Epoch 472/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0575 - accuracy: 0.9834 - val_loss: 0.0191 - val_accuracy: 0.9954\n",
            "Epoch 473/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0593 - accuracy: 0.9812 - val_loss: 0.0188 - val_accuracy: 0.9954\n",
            "Epoch 474/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0549 - accuracy: 0.9843 - val_loss: 0.0185 - val_accuracy: 0.9954\n",
            "Epoch 475/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0620 - accuracy: 0.9800 - val_loss: 0.0184 - val_accuracy: 0.9954\n",
            "Epoch 476/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0538 - accuracy: 0.9837 - val_loss: 0.0186 - val_accuracy: 0.9954\n",
            "Epoch 477/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0554 - accuracy: 0.9814 - val_loss: 0.0189 - val_accuracy: 0.9954\n",
            "Epoch 478/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0573 - accuracy: 0.9823 - val_loss: 0.0191 - val_accuracy: 0.9954\n",
            "Epoch 479/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0571 - accuracy: 0.9846 - val_loss: 0.0192 - val_accuracy: 0.9954\n",
            "Epoch 480/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0556 - accuracy: 0.9834 - val_loss: 0.0193 - val_accuracy: 0.9954\n",
            "Epoch 481/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0559 - accuracy: 0.9823 - val_loss: 0.0194 - val_accuracy: 0.9954\n",
            "Epoch 482/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0531 - accuracy: 0.9852 - val_loss: 0.0194 - val_accuracy: 0.9954\n",
            "Epoch 483/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0570 - accuracy: 0.9823 - val_loss: 0.0193 - val_accuracy: 0.9954\n",
            "Epoch 484/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0598 - accuracy: 0.9812 - val_loss: 0.0191 - val_accuracy: 0.9954\n",
            "Epoch 485/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0583 - accuracy: 0.9840 - val_loss: 0.0187 - val_accuracy: 0.9954\n",
            "Epoch 486/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0532 - accuracy: 0.9854 - val_loss: 0.0183 - val_accuracy: 0.9954\n",
            "Epoch 487/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0588 - accuracy: 0.9829 - val_loss: 0.0177 - val_accuracy: 0.9954\n",
            "Epoch 488/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0536 - accuracy: 0.9854 - val_loss: 0.0173 - val_accuracy: 0.9954\n",
            "Epoch 489/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0605 - accuracy: 0.9820 - val_loss: 0.0172 - val_accuracy: 0.9954\n",
            "Epoch 490/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0561 - accuracy: 0.9826 - val_loss: 0.0170 - val_accuracy: 0.9954\n",
            "Epoch 491/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0532 - accuracy: 0.9846 - val_loss: 0.0171 - val_accuracy: 0.9954\n",
            "Epoch 492/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0519 - accuracy: 0.9823 - val_loss: 0.0173 - val_accuracy: 0.9954\n",
            "Epoch 493/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0545 - accuracy: 0.9832 - val_loss: 0.0175 - val_accuracy: 0.9954\n",
            "Epoch 494/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0553 - accuracy: 0.9846 - val_loss: 0.0179 - val_accuracy: 0.9954\n",
            "Epoch 495/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0572 - accuracy: 0.9823 - val_loss: 0.0183 - val_accuracy: 0.9954\n",
            "Epoch 496/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0465 - accuracy: 0.9872 - val_loss: 0.0186 - val_accuracy: 0.9954\n",
            "Epoch 497/500\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0568 - accuracy: 0.9840 - val_loss: 0.0189 - val_accuracy: 0.9954\n",
            "Epoch 498/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0633 - accuracy: 0.9795 - val_loss: 0.0194 - val_accuracy: 0.9954\n",
            "Epoch 499/500\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0503 - accuracy: 0.9837 - val_loss: 0.0196 - val_accuracy: 0.9954\n",
            "Epoch 500/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0595 - accuracy: 0.9814 - val_loss: 0.0193 - val_accuracy: 0.9954\n",
            "\n",
            "Score:  [0.0228873323649168, 0.9965847134590149]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxxzbpMMD4iS",
        "outputId": "7657353e-a912-4762-99f5-ab899a1df654"
      },
      "source": [
        "np.round(model.predict(dat_resaXautots))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "m84KHVMaD4iT",
        "outputId": "84237405-239f-4ba3-8d39-12315ec512da"
      },
      "source": [
        "ffnn_res=pd.DataFrame(dat_resaXautots)\n",
        "ffnn_res['Disease']=dat_resaYts\n",
        "ffnn_res=pd.concat([ffnn_res,pd.DataFrame(np.round(model.predict(dat_resaXautots))).astype(int)],axis=1)\n",
        "ffnn_res"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>Disease</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.14</td>\n",
              "      <td>12.08</td>\n",
              "      <td>38.65</td>\n",
              "      <td>48.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.16</td>\n",
              "      <td>34.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.31</td>\n",
              "      <td>35.17</td>\n",
              "      <td>12.61</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.09</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.02</td>\n",
              "      <td>56.05</td>\n",
              "      <td>14.66</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>66.67</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.27</td>\n",
              "      <td>1.42</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.59</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.31</td>\n",
              "      <td>21.93</td>\n",
              "      <td>15.66</td>\n",
              "      <td>32.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.44</td>\n",
              "      <td>32.16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.45</td>\n",
              "      <td>28.10</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.91</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>19.54</td>\n",
              "      <td>38.00</td>\n",
              "      <td>14.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>71.83</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.44</td>\n",
              "      <td>29.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.98</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.66</td>\n",
              "      <td>57.20</td>\n",
              "      <td>45.28</td>\n",
              "      <td>40.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.94</td>\n",
              "      <td>6.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.61</td>\n",
              "      <td>2.11</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>81.08</td>\n",
              "      <td>3.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>27.14</td>\n",
              "      <td>26.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.68</td>\n",
              "      <td>3.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.06</td>\n",
              "      <td>42.84</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>acute myeloid leukaemia</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.49</td>\n",
              "      <td>79.46</td>\n",
              "      <td>30.20</td>\n",
              "      <td>46.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.39</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.90</td>\n",
              "      <td>19.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.22</td>\n",
              "      <td>24.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.39</td>\n",
              "      <td>4.90</td>\n",
              "      <td>18.92</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>19.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.16</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>chronic lymphocytic leukaemia</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.98</td>\n",
              "      <td>66.50</td>\n",
              "      <td>5.44</td>\n",
              "      <td>62.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>34.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.66</td>\n",
              "      <td>2.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>16.02</td>\n",
              "      <td>23.80</td>\n",
              "      <td>46.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>23.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.54</td>\n",
              "      <td>27.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>diffuse large B-cell lymphoma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.41</td>\n",
              "      <td>35.95</td>\n",
              "      <td>21.01</td>\n",
              "      <td>26.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.03</td>\n",
              "      <td>35.68</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.82</td>\n",
              "      <td>22.16</td>\n",
              "      <td>7.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.62</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.38</td>\n",
              "      <td>39.22</td>\n",
              "      <td>17.92</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>56.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.87</td>\n",
              "      <td>20.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.84</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1460</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.51</td>\n",
              "      <td>29.62</td>\n",
              "      <td>25.84</td>\n",
              "      <td>49.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.07</td>\n",
              "      <td>32.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.72</td>\n",
              "      <td>16.35</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.13</td>\n",
              "      <td>52.38</td>\n",
              "      <td>23.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>27.74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.94</td>\n",
              "      <td>26.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1461</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.71</td>\n",
              "      <td>18.86</td>\n",
              "      <td>34.25</td>\n",
              "      <td>39.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.12</td>\n",
              "      <td>40.53</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49.90</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.12</td>\n",
              "      <td>21.20</td>\n",
              "      <td>11.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.58</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.23</td>\n",
              "      <td>11.28</td>\n",
              "      <td>53.07</td>\n",
              "      <td>27.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>70.82</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.88</td>\n",
              "      <td>20.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.50</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1462</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.22</td>\n",
              "      <td>23.36</td>\n",
              "      <td>23.67</td>\n",
              "      <td>64.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.41</td>\n",
              "      <td>16.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.18</td>\n",
              "      <td>39.84</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>16.48</td>\n",
              "      <td>46.99</td>\n",
              "      <td>14.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>74.29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.00</td>\n",
              "      <td>26.37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47.29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.21</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1463</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.49</td>\n",
              "      <td>15.44</td>\n",
              "      <td>48.19</td>\n",
              "      <td>64.84</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.22</td>\n",
              "      <td>24.22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.00</td>\n",
              "      <td>30.79</td>\n",
              "      <td>13.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.03</td>\n",
              "      <td>74.83</td>\n",
              "      <td>16.97</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>54.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.82</td>\n",
              "      <td>14.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.54</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1464 rows × 67 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0    1      2      3      4      5  ...  0  1  2  3  4  5\n",
              "0     0.0  0.0  15.14  12.08  38.65  48.95  ...  0  0  0  0  0  1\n",
              "1     0.0  0.0   1.31  21.93  15.66  32.14  ...  0  0  0  0  0  1\n",
              "2     0.0  0.0  41.66  57.20  45.28  40.28  ...  1  0  0  0  0  0\n",
              "3     0.0  0.0  21.49  79.46  30.20  46.62  ...  0  0  0  1  0  0\n",
              "4     0.0  0.0   9.98  66.50   5.44  62.56  ...  0  0  0  0  1  0\n",
              "...   ...  ...    ...    ...    ...    ...  ... .. .. .. .. .. ..\n",
              "1459  0.0  0.0  14.41  35.95  21.01  26.44  ...  0  0  0  0  0  1\n",
              "1460  0.0  0.0   6.51  29.62  25.84  49.79  ...  0  0  0  0  0  1\n",
              "1461  0.0  0.0  10.71  18.86  34.25  39.59  ...  0  0  0  0  0  1\n",
              "1462  0.0  0.0  16.22  23.36  23.67  64.75  ...  0  0  0  0  0  1\n",
              "1463  0.0  0.0  12.49  15.44  48.19  64.84  ...  0  0  0  0  0  1\n",
              "\n",
              "[1464 rows x 67 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo1I6NH3wSST"
      },
      "source": [
        "### dataset1+normal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f5DywIewtqv"
      },
      "source": [
        "#### selectkbest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY4TsSpJwtqv"
      },
      "source": [
        "##### logistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IywxG1_wtqv"
      },
      "source": [
        "my_param_grid = [\n",
        "    {'solver': ['newton-cg', 'lbfgs', 'saga'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l2'], 'max_iter': [200]},\n",
        "    {'solver': ['liblinear'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l1', 'l2'], 'max_iter': [200]}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z5hpHNxwtqw"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BdCK-SFwtqw"
      },
      "source": [
        "modellr = GridSearchCV(estimator=LogisticRegression(n_jobs=-1), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lrq8AM3wtqw",
        "outputId": "19b6fd78-2e31-4655-a806-b331680951b9"
      },
      "source": [
        "modellr.fit(dat_resaXredtr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             estimator=LogisticRegression(),\n",
              "             param_grid=[{'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l2'],\n",
              "                          'solver': ['newton-cg', 'lbfgs', 'saga']},\n",
              "                         {'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l1', 'l2'], 'solver': ['liblinear']}],\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIB8ahcwwtqx",
        "outputId": "9dd33ffd-d83a-49fd-eb55-0d1580c31696"
      },
      "source": [
        "print(cross_val_score(modellr.best_estimator_,dat_resaXredtr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[-0.11972749 -0.08546646 -0.10488305 -0.09775704 -0.06650636 -0.08399093\n",
            " -0.09625531 -0.06434564 -0.09255371 -0.12906877 -0.08476259 -0.07149416\n",
            " -0.10916995 -0.08908533 -0.0827901  -0.08336196 -0.14155235 -0.09703427\n",
            " -0.07066989 -0.09668945 -0.1095076  -0.11439934 -0.08014763 -0.07381169\n",
            " -0.10518836 -0.09196028 -0.10182233 -0.06644554 -0.10648979 -0.09388462\n",
            " -0.10605213 -0.11071252 -0.0721894  -0.10583477 -0.07589814 -0.10006122\n",
            " -0.10398035 -0.10838224 -0.06738437 -0.09785822 -0.07380114 -0.09826964\n",
            " -0.08053537 -0.0990442  -0.11010522 -0.09187085 -0.10091754 -0.09368849\n",
            " -0.06017367 -0.11392047]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7PUcUOzwtqx",
        "outputId": "c0b83b28-782d-447e-e1ae-af75829df68b"
      },
      "source": [
        "modellr.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.09361280731082346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUoTL0DTwtqx",
        "outputId": "58391d02-bc7a-44d0-a06e-55bbfcca7eab"
      },
      "source": [
        "log_loss(dat_resaYts, modellr.predict_proba(dat_resaXredts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09787322181050712"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCOUvHi3wtqy"
      },
      "source": [
        "modellr_best = modellr.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxlBLdPywtqy",
        "outputId": "cb1bcada-3fd4-4b63-b4f4-77649e7a172a"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modellr_best.predict(dat_resaXredtr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.97      0.98      0.98       750\n",
            "        breast adenocarcinoma       0.98      0.99      0.99       750\n",
            "                breast cancer       0.99      0.98      0.99       750\n",
            "chronic lymphocytic leukaemia       1.00      1.00      1.00       750\n",
            "diffuse large B-cell lymphoma       1.00      1.00      1.00       750\n",
            "             multiple myeloma       1.00      0.99      1.00       750\n",
            "                       normal       0.95      0.95      0.95       750\n",
            "\n",
            "                     accuracy                           0.99      5250\n",
            "                    macro avg       0.99      0.99      0.99      5250\n",
            "                 weighted avg       0.99      0.99      0.99      5250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WN07KS_wtqy",
        "outputId": "ed671919-2328-49bc-f4cd-fe39a438f30c"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modellr_best.predict(dat_resaXredts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.94      0.98      0.96       250\n",
            "        breast adenocarcinoma       0.96      0.98      0.97       250\n",
            "                breast cancer       0.97      0.96      0.97       250\n",
            "chronic lymphocytic leukaemia       1.00      0.99      0.99       250\n",
            "diffuse large B-cell lymphoma       0.99      1.00      1.00       250\n",
            "             multiple myeloma       1.00      0.98      0.99       250\n",
            "                       normal       0.94      0.89      0.91       250\n",
            "\n",
            "                     accuracy                           0.97      1750\n",
            "                    macro avg       0.97      0.97      0.97      1750\n",
            "                 weighted avg       0.97      0.97      0.97      1750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x4sO-sXwtqy"
      },
      "source": [
        "##### random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDV62tXbwtqy"
      },
      "source": [
        "my_param_grid = {'bootstrap': [True, False], \n",
        "                 'n_estimators': [10, 50], \n",
        "                 'min_samples_leaf': [20, 40, 60],\n",
        "                 'min_weight_fraction_leaf': [0.01, 0.02, 0.05],\n",
        "                 'criterion': ['gini', 'entropy'], \n",
        "                 'min_impurity_decrease': [1e-5, 1e-6, 1e-7]\n",
        "                 }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVhIw8A1wtqy"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9gCa-A8wtqy"
      },
      "source": [
        "modelrf = GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1,warm_start=True), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUHufa1Xwtqy",
        "outputId": "7bf7f0ff-f405-4bcd-a784-a6a686dbc419"
      },
      "source": [
        "modelrf.fit(dat_resaXredtr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             estimator=RandomForestClassifier(n_jobs=-1, warm_start=True),\n",
              "             param_grid={'bootstrap': [True, False],\n",
              "                         'criterion': ['gini', 'entropy'],\n",
              "                         'min_impurity_decrease': [1e-05, 1e-06, 1e-07],\n",
              "                         'min_samples_leaf': [20, 40, 60],\n",
              "                         'min_weight_fraction_leaf': [0.01, 0.02, 0.05],\n",
              "                         'n_estimators': [10, 50]},\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQxtkX5vwtqy",
        "outputId": "5a0869ec-cfe7-4f4e-8f53-fe7a48700b98"
      },
      "source": [
        "print(cross_val_score(modelrf.best_estimator_,dat_resaXredtr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.22396889 -0.18489852 -0.19580283 -0.19864324 -0.17491384 -0.17838128\n",
            " -0.20080615 -0.20356631 -0.19836231 -0.19961989 -0.18154517 -0.19128325\n",
            " -0.19667208 -0.22336114 -0.18246418 -0.18842871 -0.22194402 -0.19048851\n",
            " -0.19461739 -0.17917701 -0.22559335 -0.18668574 -0.19134814 -0.18115874\n",
            " -0.2065714  -0.22904971 -0.20404313 -0.17538853 -0.19396961 -0.18015368\n",
            " -0.20886185 -0.22032265 -0.17990128 -0.21139622 -0.19457138 -0.18498042\n",
            " -0.22155837 -0.19536142 -0.18543958 -0.20088633 -0.18257245 -0.19395394\n",
            " -0.22946513 -0.19845712 -0.19384661 -0.21846905 -0.19751691 -0.19973792\n",
            " -0.17486103 -0.19810438]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j3Z_DZqwtqy",
        "outputId": "68b1c78d-fe2b-477a-8f73-b645b2aa79ed"
      },
      "source": [
        "-modelrf.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19498978488249707"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAJlwc9Jwtqy",
        "outputId": "d59dd5ff-cd15-4066-b6e8-2878ec4b2563"
      },
      "source": [
        "log_loss(dat_resaYts, modelrf.predict_proba(dat_resaXredts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1996500076703686"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMXLufUNwtqy"
      },
      "source": [
        "modelrf_best = modelrf.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD4Qe6wpwtqz",
        "outputId": "3ddebc7a-922f-4499-b660-d2687efd8569"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modelrf_best.predict(dat_resaXredtr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.92      0.98      0.95       750\n",
            "        breast adenocarcinoma       0.97      0.99      0.98       750\n",
            "                breast cancer       0.98      0.95      0.97       750\n",
            "chronic lymphocytic leukaemia       0.99      0.99      0.99       750\n",
            "diffuse large B-cell lymphoma       0.99      0.99      0.99       750\n",
            "             multiple myeloma       1.00      0.95      0.98       750\n",
            "                       normal       0.88      0.88      0.88       750\n",
            "\n",
            "                     accuracy                           0.96      5250\n",
            "                    macro avg       0.96      0.96      0.96      5250\n",
            "                 weighted avg       0.96      0.96      0.96      5250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-QnRW-Cwtqz",
        "outputId": "05e1960b-e05a-474b-9a0f-78043444ac8b"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modelrf_best.predict(dat_resaXredts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.88      0.98      0.93       250\n",
            "        breast adenocarcinoma       0.95      0.99      0.97       250\n",
            "                breast cancer       0.98      0.96      0.97       250\n",
            "chronic lymphocytic leukaemia       1.00      0.98      0.99       250\n",
            "diffuse large B-cell lymphoma       0.99      1.00      1.00       250\n",
            "             multiple myeloma       1.00      0.96      0.98       250\n",
            "                       normal       0.92      0.84      0.88       250\n",
            "\n",
            "                     accuracy                           0.96      1750\n",
            "                    macro avg       0.96      0.96      0.96      1750\n",
            "                 weighted avg       0.96      0.96      0.96      1750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLVPwX39wtqz"
      },
      "source": [
        "##### gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfcQe6Cawtqz"
      },
      "source": [
        "my_param_grid = {\n",
        "    \"loss\":[\"deviance\"],\n",
        "    \"learning_rate\": [0.2],\n",
        "    \"min_samples_split\": np.linspace(0.1, 0.5, 1),\n",
        "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 1),\n",
        "    \"max_depth\":[3,8],\n",
        "    \"max_features\":[\"log2\",\"sqrt\"],\n",
        "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
        "    \"subsample\":[0.8],\n",
        "    \"n_estimators\":[30]\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvQTeo94wtqz"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEZ-k6Wfwtqz"
      },
      "source": [
        "modelxgb = GridSearchCV(estimator=GradientBoostingClassifier(warm_start=True), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INdcTlGKwtqz",
        "outputId": "c98e1f10-84ff-4b6f-8391-505dc4aa0435"
      },
      "source": [
        "modelxgb.fit(dat_resaXredtr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             estimator=GradientBoostingClassifier(warm_start=True),\n",
              "             param_grid={'criterion': ['friedman_mse', 'mae'],\n",
              "                         'learning_rate': [0.2], 'loss': ['deviance'],\n",
              "                         'max_depth': [3, 8], 'max_features': ['log2', 'sqrt'],\n",
              "                         'min_samples_leaf': array([0.1]),\n",
              "                         'min_samples_split': array([0.1]),\n",
              "                         'n_estimators': [30], 'subsample': [0.8]},\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPfNrvRawtqz",
        "outputId": "ff654a15-2d67-42be-f0d2-6e5564fbc908"
      },
      "source": [
        "print(cross_val_score(modelxgb.best_estimator_,dat_resaXredtr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.11685797 -0.11050104 -0.12453454 -0.12759776 -0.09458868 -0.11108613\n",
            " -0.13276904 -0.09506596 -0.11003752 -0.12788491 -0.10629277 -0.11179169\n",
            " -0.12424922 -0.11913674 -0.11161468 -0.11285759 -0.12799295 -0.09547163\n",
            " -0.11156186 -0.12331951 -0.12076038 -0.11609131 -0.10983003 -0.10199849\n",
            " -0.12398577 -0.12067248 -0.11922933 -0.08885314 -0.11822573 -0.11839651\n",
            " -0.13307419 -0.12664119 -0.10435301 -0.1024045  -0.10526315 -0.11531212\n",
            " -0.11787526 -0.12054093 -0.0949237  -0.12265823 -0.0966568  -0.11914167\n",
            " -0.13289769 -0.10617436 -0.11778718 -0.11869869 -0.11820168 -0.11769315\n",
            " -0.09289165 -0.12313448]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-nviNiqwtqz",
        "outputId": "0fbe4c8c-b893-4905-b68c-d95c8408052f"
      },
      "source": [
        "-modelxgb.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11358585203119352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkDvsZtpwtqz",
        "outputId": "aeed7b33-324f-444a-ca87-74376f3fd8f2"
      },
      "source": [
        "log_loss(dat_resaYts, modelxgb.predict_proba(dat_resaXredts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11842292778209897"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H07xjG1Ewtqz"
      },
      "source": [
        "modelxgb_best = modelxgb.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bbiM9sAwtqz",
        "outputId": "ca996b4e-5c3c-4e87-ca91-d93ffcddebdb"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modelxgb_best.predict(dat_resaXredtr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.97      0.98      0.98       750\n",
            "        breast adenocarcinoma       0.97      0.99      0.98       750\n",
            "                breast cancer       0.98      0.96      0.97       750\n",
            "chronic lymphocytic leukaemia       0.99      1.00      0.99       750\n",
            "diffuse large B-cell lymphoma       1.00      0.99      1.00       750\n",
            "             multiple myeloma       1.00      0.99      1.00       750\n",
            "                       normal       0.94      0.94      0.94       750\n",
            "\n",
            "                     accuracy                           0.98      5250\n",
            "                    macro avg       0.98      0.98      0.98      5250\n",
            "                 weighted avg       0.98      0.98      0.98      5250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTbCaugvwtqz",
        "outputId": "f00d63df-9a4b-4076-c24f-c37067a79671"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modelxgb_best.predict(dat_resaXredts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.95      0.97      0.96       250\n",
            "        breast adenocarcinoma       0.96      0.98      0.97       250\n",
            "                breast cancer       0.99      0.96      0.98       250\n",
            "chronic lymphocytic leukaemia       1.00      0.98      0.99       250\n",
            "diffuse large B-cell lymphoma       0.99      1.00      1.00       250\n",
            "             multiple myeloma       1.00      0.98      0.99       250\n",
            "                       normal       0.91      0.92      0.92       250\n",
            "\n",
            "                     accuracy                           0.97      1750\n",
            "                    macro avg       0.97      0.97      0.97      1750\n",
            "                 weighted avg       0.97      0.97      0.97      1750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiAYZ0XRwtqz"
      },
      "source": [
        "##### fully connected ffnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XY1IfRvLYAT",
        "outputId": "eee0a660-b8bc-4208-a4af-1614d1e941aa"
      },
      "source": [
        "dat_resaYtr"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['chronic lymphocytic leukaemia', 'breast cancer', 'breast cancer',\n",
              "       ..., 'multiple myeloma', 'multiple myeloma', 'multiple myeloma'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7ZYq6yULYAU"
      },
      "source": [
        "num_classes=7\n",
        "dat_resaYtrx=pd.DataFrame(dat_resaYtr)\n",
        "dat_resaYtrx[0] = pd.Categorical(dat_resaYtrx[0])\n",
        "dat_resaYtrx['code'] = dat_resaYtrx[0].cat.codes\n",
        "dat_resaYtrx_=np.array(dat_resaYtrx['code']\n",
        "                       )\n",
        "dat_resaYtsx=pd.DataFrame(dat_resaYts)\n",
        "dat_resaYtsx[0] = pd.Categorical(dat_resaYtsx[0])\n",
        "dat_resaYtsx['code'] = dat_resaYtsx[0].cat.codes\n",
        "dat_resaYtsx_=np.array(dat_resaYtsx['code'])"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVylF_DFLYAU",
        "outputId": "c319ec85-1a3f-4454-eb5b-e887afc73dd9"
      },
      "source": [
        "dat_resaYtrcl = to_categorical(dat_resaYtrx_, num_classes)\n",
        "dat_resaYtscl = to_categorical(dat_resaYtsx_, num_classes)\n",
        "dat_resaYtrcl"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfLfCY45LYAU"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(30, activation='relu', input_shape=(60,)))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfr4g8K8LYAV"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7odlyzPLYAV",
        "outputId": "2a39a285-0df0-41f2-e74b-d7c83adc203e"
      },
      "source": [
        "history_ = model.fit(dat_resaXredtr, dat_resaYtrcl, \n",
        "                            batch_size=2000, epochs=500, verbose=1, validation_split=0.2)\n",
        " \n",
        "score_ = model.evaluate(dat_resaXredts,dat_resaYtscl , verbose=0)\n",
        "print('\\nScore: ', score_)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 2.0050 - accuracy: 0.1707 - val_loss: 2.0840 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.9526 - accuracy: 0.1774 - val_loss: 2.0264 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.9115 - accuracy: 0.1826 - val_loss: 1.9841 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.8782 - accuracy: 0.1871 - val_loss: 1.9472 - val_accuracy: 9.5238e-04\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.8473 - accuracy: 0.1955 - val_loss: 1.9136 - val_accuracy: 0.0048\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.8197 - accuracy: 0.2233 - val_loss: 1.8807 - val_accuracy: 0.0048\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.7899 - accuracy: 0.2440 - val_loss: 1.8488 - val_accuracy: 0.0105\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.7655 - accuracy: 0.2614 - val_loss: 1.8190 - val_accuracy: 0.0343\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.7379 - accuracy: 0.2776 - val_loss: 1.7900 - val_accuracy: 0.0895\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.7046 - accuracy: 0.2943 - val_loss: 1.7593 - val_accuracy: 0.1886\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.6761 - accuracy: 0.3252 - val_loss: 1.7267 - val_accuracy: 0.2771\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.6455 - accuracy: 0.3488 - val_loss: 1.6922 - val_accuracy: 0.3457\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.6095 - accuracy: 0.3879 - val_loss: 1.6555 - val_accuracy: 0.3943\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.5737 - accuracy: 0.4198 - val_loss: 1.6136 - val_accuracy: 0.5181\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.5371 - accuracy: 0.4833 - val_loss: 1.5648 - val_accuracy: 0.5790\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.4952 - accuracy: 0.5324 - val_loss: 1.5100 - val_accuracy: 0.6010\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.4609 - accuracy: 0.5510 - val_loss: 1.4491 - val_accuracy: 0.6257\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.4137 - accuracy: 0.5786 - val_loss: 1.3777 - val_accuracy: 0.6705\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.3742 - accuracy: 0.5955 - val_loss: 1.3008 - val_accuracy: 0.7352\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.3226 - accuracy: 0.6250 - val_loss: 1.2256 - val_accuracy: 0.8200\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.2854 - accuracy: 0.6448 - val_loss: 1.1573 - val_accuracy: 0.8771\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.2375 - accuracy: 0.6686 - val_loss: 1.0998 - val_accuracy: 0.9219\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.1903 - accuracy: 0.6952 - val_loss: 1.0457 - val_accuracy: 0.9371\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.1550 - accuracy: 0.7174 - val_loss: 0.9926 - val_accuracy: 0.9476\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.1210 - accuracy: 0.7210 - val_loss: 0.9421 - val_accuracy: 0.9514\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.0788 - accuracy: 0.7340 - val_loss: 0.8983 - val_accuracy: 0.9524\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 1.0359 - accuracy: 0.7490 - val_loss: 0.8568 - val_accuracy: 0.9533\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.0022 - accuracy: 0.7569 - val_loss: 0.8133 - val_accuracy: 0.9543\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.9658 - accuracy: 0.7700 - val_loss: 0.7711 - val_accuracy: 0.9543\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.9330 - accuracy: 0.7714 - val_loss: 0.7265 - val_accuracy: 0.9571\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.8998 - accuracy: 0.7769 - val_loss: 0.6779 - val_accuracy: 0.9667\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.8732 - accuracy: 0.7843 - val_loss: 0.6334 - val_accuracy: 0.9762\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.8370 - accuracy: 0.7912 - val_loss: 0.5921 - val_accuracy: 0.9762\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.8126 - accuracy: 0.7940 - val_loss: 0.5550 - val_accuracy: 0.9762\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.7864 - accuracy: 0.7981 - val_loss: 0.5226 - val_accuracy: 0.9762\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.7577 - accuracy: 0.8033 - val_loss: 0.4940 - val_accuracy: 0.9762\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.7362 - accuracy: 0.8040 - val_loss: 0.4654 - val_accuracy: 0.9762\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.7002 - accuracy: 0.8245 - val_loss: 0.4387 - val_accuracy: 0.9762\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.6859 - accuracy: 0.8155 - val_loss: 0.4126 - val_accuracy: 0.9762\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.6602 - accuracy: 0.8212 - val_loss: 0.3845 - val_accuracy: 0.9762\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.6323 - accuracy: 0.8336 - val_loss: 0.3602 - val_accuracy: 0.9762\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.6220 - accuracy: 0.8236 - val_loss: 0.3368 - val_accuracy: 0.9762\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.6031 - accuracy: 0.8307 - val_loss: 0.3130 - val_accuracy: 0.9762\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5767 - accuracy: 0.8336 - val_loss: 0.2938 - val_accuracy: 0.9762\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5576 - accuracy: 0.8398 - val_loss: 0.2779 - val_accuracy: 0.9762\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5392 - accuracy: 0.8443 - val_loss: 0.2614 - val_accuracy: 0.9762\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5308 - accuracy: 0.8443 - val_loss: 0.2453 - val_accuracy: 0.9762\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5138 - accuracy: 0.8445 - val_loss: 0.2316 - val_accuracy: 0.9762\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4959 - accuracy: 0.8533 - val_loss: 0.2180 - val_accuracy: 0.9762\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4904 - accuracy: 0.8579 - val_loss: 0.2069 - val_accuracy: 0.9762\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4788 - accuracy: 0.8545 - val_loss: 0.2010 - val_accuracy: 0.9762\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4563 - accuracy: 0.8648 - val_loss: 0.1960 - val_accuracy: 0.9762\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4568 - accuracy: 0.8605 - val_loss: 0.1900 - val_accuracy: 0.9762\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4424 - accuracy: 0.8674 - val_loss: 0.1833 - val_accuracy: 0.9762\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4328 - accuracy: 0.8705 - val_loss: 0.1762 - val_accuracy: 0.9762\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4258 - accuracy: 0.8688 - val_loss: 0.1695 - val_accuracy: 0.9762\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4182 - accuracy: 0.8729 - val_loss: 0.1632 - val_accuracy: 0.9762\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4130 - accuracy: 0.8686 - val_loss: 0.1593 - val_accuracy: 0.9762\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4040 - accuracy: 0.8776 - val_loss: 0.1559 - val_accuracy: 0.9771\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3989 - accuracy: 0.8779 - val_loss: 0.1504 - val_accuracy: 0.9771\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3917 - accuracy: 0.8831 - val_loss: 0.1469 - val_accuracy: 0.9771\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3861 - accuracy: 0.8771 - val_loss: 0.1454 - val_accuracy: 0.9771\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3857 - accuracy: 0.8838 - val_loss: 0.1438 - val_accuracy: 0.9771\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3714 - accuracy: 0.8829 - val_loss: 0.1426 - val_accuracy: 0.9771\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3712 - accuracy: 0.8821 - val_loss: 0.1397 - val_accuracy: 0.9771\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3620 - accuracy: 0.8869 - val_loss: 0.1327 - val_accuracy: 0.9771\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3576 - accuracy: 0.8886 - val_loss: 0.1249 - val_accuracy: 0.9771\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3524 - accuracy: 0.8914 - val_loss: 0.1208 - val_accuracy: 0.9771\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3439 - accuracy: 0.8912 - val_loss: 0.1208 - val_accuracy: 0.9771\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3438 - accuracy: 0.8924 - val_loss: 0.1243 - val_accuracy: 0.9762\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.3429 - accuracy: 0.8933 - val_loss: 0.1258 - val_accuracy: 0.9762\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3370 - accuracy: 0.8890 - val_loss: 0.1233 - val_accuracy: 0.9762\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3395 - accuracy: 0.8886 - val_loss: 0.1205 - val_accuracy: 0.9762\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3256 - accuracy: 0.8986 - val_loss: 0.1202 - val_accuracy: 0.9762\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3328 - accuracy: 0.8964 - val_loss: 0.1207 - val_accuracy: 0.9762\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.3248 - accuracy: 0.8969 - val_loss: 0.1197 - val_accuracy: 0.9762\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.3211 - accuracy: 0.9021 - val_loss: 0.1188 - val_accuracy: 0.9762\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3187 - accuracy: 0.9012 - val_loss: 0.1188 - val_accuracy: 0.9762\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3186 - accuracy: 0.8981 - val_loss: 0.1159 - val_accuracy: 0.9771\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3130 - accuracy: 0.8988 - val_loss: 0.1132 - val_accuracy: 0.9771\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3062 - accuracy: 0.9081 - val_loss: 0.1117 - val_accuracy: 0.9762\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3115 - accuracy: 0.9017 - val_loss: 0.1106 - val_accuracy: 0.9762\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3034 - accuracy: 0.9007 - val_loss: 0.1111 - val_accuracy: 0.9762\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3013 - accuracy: 0.9069 - val_loss: 0.1112 - val_accuracy: 0.9771\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2920 - accuracy: 0.9110 - val_loss: 0.1106 - val_accuracy: 0.9771\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2997 - accuracy: 0.9090 - val_loss: 0.1085 - val_accuracy: 0.9771\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2980 - accuracy: 0.9055 - val_loss: 0.1045 - val_accuracy: 0.9771\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2979 - accuracy: 0.9055 - val_loss: 0.1007 - val_accuracy: 0.9771\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2890 - accuracy: 0.9117 - val_loss: 0.0990 - val_accuracy: 0.9771\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2895 - accuracy: 0.9110 - val_loss: 0.1000 - val_accuracy: 0.9771\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2895 - accuracy: 0.9074 - val_loss: 0.1024 - val_accuracy: 0.9771\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2813 - accuracy: 0.9121 - val_loss: 0.1047 - val_accuracy: 0.9762\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2808 - accuracy: 0.9090 - val_loss: 0.1051 - val_accuracy: 0.9762\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2826 - accuracy: 0.9112 - val_loss: 0.1031 - val_accuracy: 0.9771\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2815 - accuracy: 0.9102 - val_loss: 0.0999 - val_accuracy: 0.9771\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2749 - accuracy: 0.9157 - val_loss: 0.0984 - val_accuracy: 0.9771\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2746 - accuracy: 0.9145 - val_loss: 0.0988 - val_accuracy: 0.9771\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2788 - accuracy: 0.9143 - val_loss: 0.1006 - val_accuracy: 0.9771\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2737 - accuracy: 0.9160 - val_loss: 0.1019 - val_accuracy: 0.9771\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2708 - accuracy: 0.9148 - val_loss: 0.1002 - val_accuracy: 0.9771\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2712 - accuracy: 0.9174 - val_loss: 0.0956 - val_accuracy: 0.9771\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2649 - accuracy: 0.9181 - val_loss: 0.0907 - val_accuracy: 0.9771\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.2659 - accuracy: 0.9198 - val_loss: 0.0874 - val_accuracy: 0.9771\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2654 - accuracy: 0.9126 - val_loss: 0.0863 - val_accuracy: 0.9771\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2595 - accuracy: 0.9167 - val_loss: 0.0875 - val_accuracy: 0.9771\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2579 - accuracy: 0.9183 - val_loss: 0.0909 - val_accuracy: 0.9771\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2563 - accuracy: 0.9205 - val_loss: 0.0952 - val_accuracy: 0.9771\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2553 - accuracy: 0.9226 - val_loss: 0.0980 - val_accuracy: 0.9771\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2581 - accuracy: 0.9167 - val_loss: 0.0984 - val_accuracy: 0.9771\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2577 - accuracy: 0.9200 - val_loss: 0.0959 - val_accuracy: 0.9771\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2484 - accuracy: 0.9260 - val_loss: 0.0924 - val_accuracy: 0.9771\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2456 - accuracy: 0.9229 - val_loss: 0.0900 - val_accuracy: 0.9771\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2522 - accuracy: 0.9212 - val_loss: 0.0875 - val_accuracy: 0.9771\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2448 - accuracy: 0.9219 - val_loss: 0.0846 - val_accuracy: 0.9771\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2509 - accuracy: 0.9217 - val_loss: 0.0828 - val_accuracy: 0.9771\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2494 - accuracy: 0.9238 - val_loss: 0.0818 - val_accuracy: 0.9771\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2433 - accuracy: 0.9217 - val_loss: 0.0815 - val_accuracy: 0.9771\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2401 - accuracy: 0.9231 - val_loss: 0.0815 - val_accuracy: 0.9771\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2364 - accuracy: 0.9262 - val_loss: 0.0820 - val_accuracy: 0.9771\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2434 - accuracy: 0.9262 - val_loss: 0.0831 - val_accuracy: 0.9771\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2426 - accuracy: 0.9243 - val_loss: 0.0827 - val_accuracy: 0.9771\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2404 - accuracy: 0.9236 - val_loss: 0.0824 - val_accuracy: 0.9771\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2432 - accuracy: 0.9276 - val_loss: 0.0820 - val_accuracy: 0.9771\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2376 - accuracy: 0.9271 - val_loss: 0.0827 - val_accuracy: 0.9771\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2378 - accuracy: 0.9219 - val_loss: 0.0829 - val_accuracy: 0.9771\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2322 - accuracy: 0.9274 - val_loss: 0.0821 - val_accuracy: 0.9771\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2268 - accuracy: 0.9326 - val_loss: 0.0815 - val_accuracy: 0.9771\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2327 - accuracy: 0.9293 - val_loss: 0.0801 - val_accuracy: 0.9771\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2298 - accuracy: 0.9326 - val_loss: 0.0799 - val_accuracy: 0.9771\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2278 - accuracy: 0.9324 - val_loss: 0.0811 - val_accuracy: 0.9771\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2288 - accuracy: 0.9290 - val_loss: 0.0833 - val_accuracy: 0.9771\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2264 - accuracy: 0.9312 - val_loss: 0.0841 - val_accuracy: 0.9771\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2336 - accuracy: 0.9250 - val_loss: 0.0826 - val_accuracy: 0.9771\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2330 - accuracy: 0.9255 - val_loss: 0.0808 - val_accuracy: 0.9771\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2257 - accuracy: 0.9298 - val_loss: 0.0798 - val_accuracy: 0.9771\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2265 - accuracy: 0.9295 - val_loss: 0.0786 - val_accuracy: 0.9771\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2234 - accuracy: 0.9317 - val_loss: 0.0760 - val_accuracy: 0.9771\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2265 - accuracy: 0.9331 - val_loss: 0.0737 - val_accuracy: 0.9771\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2261 - accuracy: 0.9298 - val_loss: 0.0749 - val_accuracy: 0.9771\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2210 - accuracy: 0.9345 - val_loss: 0.0777 - val_accuracy: 0.9771\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2189 - accuracy: 0.9305 - val_loss: 0.0802 - val_accuracy: 0.9771\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2278 - accuracy: 0.9276 - val_loss: 0.0813 - val_accuracy: 0.9771\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2125 - accuracy: 0.9381 - val_loss: 0.0794 - val_accuracy: 0.9771\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2201 - accuracy: 0.9345 - val_loss: 0.0763 - val_accuracy: 0.9771\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2164 - accuracy: 0.9321 - val_loss: 0.0741 - val_accuracy: 0.9771\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2157 - accuracy: 0.9348 - val_loss: 0.0728 - val_accuracy: 0.9771\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2133 - accuracy: 0.9348 - val_loss: 0.0731 - val_accuracy: 0.9771\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2172 - accuracy: 0.9348 - val_loss: 0.0745 - val_accuracy: 0.9771\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2194 - accuracy: 0.9331 - val_loss: 0.0768 - val_accuracy: 0.9771\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2181 - accuracy: 0.9343 - val_loss: 0.0781 - val_accuracy: 0.9771\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2109 - accuracy: 0.9343 - val_loss: 0.0775 - val_accuracy: 0.9771\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2123 - accuracy: 0.9350 - val_loss: 0.0753 - val_accuracy: 0.9771\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2118 - accuracy: 0.9348 - val_loss: 0.0722 - val_accuracy: 0.9771\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2115 - accuracy: 0.9340 - val_loss: 0.0701 - val_accuracy: 0.9771\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2105 - accuracy: 0.9348 - val_loss: 0.0700 - val_accuracy: 0.9771\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2085 - accuracy: 0.9379 - val_loss: 0.0712 - val_accuracy: 0.9771\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2059 - accuracy: 0.9383 - val_loss: 0.0723 - val_accuracy: 0.9771\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1987 - accuracy: 0.9400 - val_loss: 0.0737 - val_accuracy: 0.9771\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2091 - accuracy: 0.9336 - val_loss: 0.0758 - val_accuracy: 0.9771\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2100 - accuracy: 0.9364 - val_loss: 0.0776 - val_accuracy: 0.9771\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2109 - accuracy: 0.9324 - val_loss: 0.0773 - val_accuracy: 0.9771\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2014 - accuracy: 0.9419 - val_loss: 0.0760 - val_accuracy: 0.9771\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2052 - accuracy: 0.9369 - val_loss: 0.0746 - val_accuracy: 0.9771\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1990 - accuracy: 0.9395 - val_loss: 0.0734 - val_accuracy: 0.9771\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2023 - accuracy: 0.9379 - val_loss: 0.0712 - val_accuracy: 0.9771\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2026 - accuracy: 0.9395 - val_loss: 0.0690 - val_accuracy: 0.9771\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1989 - accuracy: 0.9414 - val_loss: 0.0688 - val_accuracy: 0.9771\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1987 - accuracy: 0.9436 - val_loss: 0.0696 - val_accuracy: 0.9771\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2016 - accuracy: 0.9407 - val_loss: 0.0717 - val_accuracy: 0.9771\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2023 - accuracy: 0.9383 - val_loss: 0.0721 - val_accuracy: 0.9771\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2027 - accuracy: 0.9410 - val_loss: 0.0703 - val_accuracy: 0.9771\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1954 - accuracy: 0.9445 - val_loss: 0.0688 - val_accuracy: 0.9781\n",
            "Epoch 173/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1884 - accuracy: 0.9407 - val_loss: 0.0695 - val_accuracy: 0.9781\n",
            "Epoch 174/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1977 - accuracy: 0.9386 - val_loss: 0.0719 - val_accuracy: 0.9771\n",
            "Epoch 175/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1962 - accuracy: 0.9402 - val_loss: 0.0728 - val_accuracy: 0.9771\n",
            "Epoch 176/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1978 - accuracy: 0.9414 - val_loss: 0.0719 - val_accuracy: 0.9771\n",
            "Epoch 177/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1886 - accuracy: 0.9431 - val_loss: 0.0703 - val_accuracy: 0.9771\n",
            "Epoch 178/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1896 - accuracy: 0.9424 - val_loss: 0.0702 - val_accuracy: 0.9771\n",
            "Epoch 179/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1946 - accuracy: 0.9452 - val_loss: 0.0709 - val_accuracy: 0.9771\n",
            "Epoch 180/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1948 - accuracy: 0.9405 - val_loss: 0.0707 - val_accuracy: 0.9771\n",
            "Epoch 181/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1908 - accuracy: 0.9421 - val_loss: 0.0697 - val_accuracy: 0.9771\n",
            "Epoch 182/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1893 - accuracy: 0.9464 - val_loss: 0.0700 - val_accuracy: 0.9771\n",
            "Epoch 183/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1918 - accuracy: 0.9443 - val_loss: 0.0705 - val_accuracy: 0.9771\n",
            "Epoch 184/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1884 - accuracy: 0.9440 - val_loss: 0.0706 - val_accuracy: 0.9771\n",
            "Epoch 185/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1873 - accuracy: 0.9450 - val_loss: 0.0714 - val_accuracy: 0.9771\n",
            "Epoch 186/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1822 - accuracy: 0.9452 - val_loss: 0.0737 - val_accuracy: 0.9771\n",
            "Epoch 187/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1919 - accuracy: 0.9400 - val_loss: 0.0747 - val_accuracy: 0.9771\n",
            "Epoch 188/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1857 - accuracy: 0.9448 - val_loss: 0.0739 - val_accuracy: 0.9771\n",
            "Epoch 189/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1794 - accuracy: 0.9450 - val_loss: 0.0731 - val_accuracy: 0.9771\n",
            "Epoch 190/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1819 - accuracy: 0.9476 - val_loss: 0.0707 - val_accuracy: 0.9771\n",
            "Epoch 191/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1881 - accuracy: 0.9440 - val_loss: 0.0682 - val_accuracy: 0.9771\n",
            "Epoch 192/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1892 - accuracy: 0.9407 - val_loss: 0.0677 - val_accuracy: 0.9781\n",
            "Epoch 193/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1875 - accuracy: 0.9433 - val_loss: 0.0681 - val_accuracy: 0.9781\n",
            "Epoch 194/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1854 - accuracy: 0.9398 - val_loss: 0.0688 - val_accuracy: 0.9781\n",
            "Epoch 195/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1859 - accuracy: 0.9455 - val_loss: 0.0703 - val_accuracy: 0.9781\n",
            "Epoch 196/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1846 - accuracy: 0.9462 - val_loss: 0.0698 - val_accuracy: 0.9781\n",
            "Epoch 197/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1857 - accuracy: 0.9455 - val_loss: 0.0683 - val_accuracy: 0.9781\n",
            "Epoch 198/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1777 - accuracy: 0.9486 - val_loss: 0.0670 - val_accuracy: 0.9781\n",
            "Epoch 199/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1802 - accuracy: 0.9483 - val_loss: 0.0665 - val_accuracy: 0.9781\n",
            "Epoch 200/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1789 - accuracy: 0.9469 - val_loss: 0.0675 - val_accuracy: 0.9771\n",
            "Epoch 201/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1811 - accuracy: 0.9448 - val_loss: 0.0687 - val_accuracy: 0.9771\n",
            "Epoch 202/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1880 - accuracy: 0.9393 - val_loss: 0.0700 - val_accuracy: 0.9771\n",
            "Epoch 203/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1819 - accuracy: 0.9448 - val_loss: 0.0714 - val_accuracy: 0.9771\n",
            "Epoch 204/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1848 - accuracy: 0.9476 - val_loss: 0.0717 - val_accuracy: 0.9771\n",
            "Epoch 205/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1771 - accuracy: 0.9490 - val_loss: 0.0696 - val_accuracy: 0.9781\n",
            "Epoch 206/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1790 - accuracy: 0.9488 - val_loss: 0.0671 - val_accuracy: 0.9781\n",
            "Epoch 207/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1722 - accuracy: 0.9479 - val_loss: 0.0644 - val_accuracy: 0.9781\n",
            "Epoch 208/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1778 - accuracy: 0.9486 - val_loss: 0.0639 - val_accuracy: 0.9781\n",
            "Epoch 209/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1755 - accuracy: 0.9474 - val_loss: 0.0645 - val_accuracy: 0.9781\n",
            "Epoch 210/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1735 - accuracy: 0.9495 - val_loss: 0.0656 - val_accuracy: 0.9781\n",
            "Epoch 211/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1796 - accuracy: 0.9455 - val_loss: 0.0666 - val_accuracy: 0.9781\n",
            "Epoch 212/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1724 - accuracy: 0.9486 - val_loss: 0.0683 - val_accuracy: 0.9781\n",
            "Epoch 213/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1774 - accuracy: 0.9467 - val_loss: 0.0690 - val_accuracy: 0.9781\n",
            "Epoch 214/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1792 - accuracy: 0.9483 - val_loss: 0.0674 - val_accuracy: 0.9781\n",
            "Epoch 215/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1757 - accuracy: 0.9457 - val_loss: 0.0667 - val_accuracy: 0.9781\n",
            "Epoch 216/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1771 - accuracy: 0.9460 - val_loss: 0.0674 - val_accuracy: 0.9781\n",
            "Epoch 217/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1689 - accuracy: 0.9483 - val_loss: 0.0685 - val_accuracy: 0.9781\n",
            "Epoch 218/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1776 - accuracy: 0.9443 - val_loss: 0.0692 - val_accuracy: 0.9781\n",
            "Epoch 219/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1752 - accuracy: 0.9452 - val_loss: 0.0697 - val_accuracy: 0.9781\n",
            "Epoch 220/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1722 - accuracy: 0.9474 - val_loss: 0.0679 - val_accuracy: 0.9781\n",
            "Epoch 221/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1725 - accuracy: 0.9481 - val_loss: 0.0642 - val_accuracy: 0.9781\n",
            "Epoch 222/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1699 - accuracy: 0.9498 - val_loss: 0.0622 - val_accuracy: 0.9781\n",
            "Epoch 223/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1652 - accuracy: 0.9521 - val_loss: 0.0618 - val_accuracy: 0.9790\n",
            "Epoch 224/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1704 - accuracy: 0.9548 - val_loss: 0.0624 - val_accuracy: 0.9790\n",
            "Epoch 225/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1688 - accuracy: 0.9510 - val_loss: 0.0622 - val_accuracy: 0.9790\n",
            "Epoch 226/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1707 - accuracy: 0.9488 - val_loss: 0.0616 - val_accuracy: 0.9781\n",
            "Epoch 227/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1683 - accuracy: 0.9502 - val_loss: 0.0627 - val_accuracy: 0.9781\n",
            "Epoch 228/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1682 - accuracy: 0.9519 - val_loss: 0.0637 - val_accuracy: 0.9781\n",
            "Epoch 229/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1683 - accuracy: 0.9507 - val_loss: 0.0641 - val_accuracy: 0.9781\n",
            "Epoch 230/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1652 - accuracy: 0.9529 - val_loss: 0.0643 - val_accuracy: 0.9781\n",
            "Epoch 231/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1700 - accuracy: 0.9495 - val_loss: 0.0647 - val_accuracy: 0.9781\n",
            "Epoch 232/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1672 - accuracy: 0.9479 - val_loss: 0.0643 - val_accuracy: 0.9781\n",
            "Epoch 233/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1679 - accuracy: 0.9517 - val_loss: 0.0631 - val_accuracy: 0.9781\n",
            "Epoch 234/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1667 - accuracy: 0.9521 - val_loss: 0.0623 - val_accuracy: 0.9790\n",
            "Epoch 235/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1674 - accuracy: 0.9500 - val_loss: 0.0620 - val_accuracy: 0.9790\n",
            "Epoch 236/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1597 - accuracy: 0.9519 - val_loss: 0.0621 - val_accuracy: 0.9790\n",
            "Epoch 237/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1610 - accuracy: 0.9510 - val_loss: 0.0622 - val_accuracy: 0.9781\n",
            "Epoch 238/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1610 - accuracy: 0.9533 - val_loss: 0.0619 - val_accuracy: 0.9781\n",
            "Epoch 239/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1627 - accuracy: 0.9536 - val_loss: 0.0612 - val_accuracy: 0.9781\n",
            "Epoch 240/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1607 - accuracy: 0.9524 - val_loss: 0.0620 - val_accuracy: 0.9781\n",
            "Epoch 241/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1651 - accuracy: 0.9495 - val_loss: 0.0638 - val_accuracy: 0.9781\n",
            "Epoch 242/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1617 - accuracy: 0.9519 - val_loss: 0.0640 - val_accuracy: 0.9781\n",
            "Epoch 243/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1655 - accuracy: 0.9493 - val_loss: 0.0641 - val_accuracy: 0.9781\n",
            "Epoch 244/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1640 - accuracy: 0.9469 - val_loss: 0.0642 - val_accuracy: 0.9781\n",
            "Epoch 245/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1600 - accuracy: 0.9481 - val_loss: 0.0641 - val_accuracy: 0.9781\n",
            "Epoch 246/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1576 - accuracy: 0.9519 - val_loss: 0.0649 - val_accuracy: 0.9790\n",
            "Epoch 247/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1618 - accuracy: 0.9507 - val_loss: 0.0658 - val_accuracy: 0.9790\n",
            "Epoch 248/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1574 - accuracy: 0.9531 - val_loss: 0.0646 - val_accuracy: 0.9790\n",
            "Epoch 249/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1625 - accuracy: 0.9495 - val_loss: 0.0621 - val_accuracy: 0.9790\n",
            "Epoch 250/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1573 - accuracy: 0.9531 - val_loss: 0.0616 - val_accuracy: 0.9790\n",
            "Epoch 251/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1584 - accuracy: 0.9538 - val_loss: 0.0633 - val_accuracy: 0.9781\n",
            "Epoch 252/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1539 - accuracy: 0.9557 - val_loss: 0.0649 - val_accuracy: 0.9781\n",
            "Epoch 253/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1565 - accuracy: 0.9533 - val_loss: 0.0650 - val_accuracy: 0.9781\n",
            "Epoch 254/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1588 - accuracy: 0.9555 - val_loss: 0.0633 - val_accuracy: 0.9781\n",
            "Epoch 255/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1589 - accuracy: 0.9540 - val_loss: 0.0617 - val_accuracy: 0.9790\n",
            "Epoch 256/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1577 - accuracy: 0.9536 - val_loss: 0.0610 - val_accuracy: 0.9790\n",
            "Epoch 257/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1532 - accuracy: 0.9536 - val_loss: 0.0613 - val_accuracy: 0.9790\n",
            "Epoch 258/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1622 - accuracy: 0.9514 - val_loss: 0.0617 - val_accuracy: 0.9790\n",
            "Epoch 259/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1547 - accuracy: 0.9538 - val_loss: 0.0627 - val_accuracy: 0.9790\n",
            "Epoch 260/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1557 - accuracy: 0.9552 - val_loss: 0.0637 - val_accuracy: 0.9781\n",
            "Epoch 261/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1519 - accuracy: 0.9517 - val_loss: 0.0643 - val_accuracy: 0.9781\n",
            "Epoch 262/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1527 - accuracy: 0.9536 - val_loss: 0.0642 - val_accuracy: 0.9781\n",
            "Epoch 263/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1550 - accuracy: 0.9526 - val_loss: 0.0651 - val_accuracy: 0.9781\n",
            "Epoch 264/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1568 - accuracy: 0.9540 - val_loss: 0.0659 - val_accuracy: 0.9781\n",
            "Epoch 265/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1582 - accuracy: 0.9548 - val_loss: 0.0658 - val_accuracy: 0.9790\n",
            "Epoch 266/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1577 - accuracy: 0.9550 - val_loss: 0.0646 - val_accuracy: 0.9790\n",
            "Epoch 267/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1523 - accuracy: 0.9550 - val_loss: 0.0631 - val_accuracy: 0.9800\n",
            "Epoch 268/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1555 - accuracy: 0.9540 - val_loss: 0.0612 - val_accuracy: 0.9800\n",
            "Epoch 269/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1532 - accuracy: 0.9545 - val_loss: 0.0601 - val_accuracy: 0.9790\n",
            "Epoch 270/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1534 - accuracy: 0.9545 - val_loss: 0.0598 - val_accuracy: 0.9790\n",
            "Epoch 271/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1499 - accuracy: 0.9560 - val_loss: 0.0606 - val_accuracy: 0.9800\n",
            "Epoch 272/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1499 - accuracy: 0.9552 - val_loss: 0.0614 - val_accuracy: 0.9790\n",
            "Epoch 273/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1568 - accuracy: 0.9524 - val_loss: 0.0610 - val_accuracy: 0.9790\n",
            "Epoch 274/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1577 - accuracy: 0.9533 - val_loss: 0.0600 - val_accuracy: 0.9790\n",
            "Epoch 275/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1534 - accuracy: 0.9560 - val_loss: 0.0593 - val_accuracy: 0.9790\n",
            "Epoch 276/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1534 - accuracy: 0.9557 - val_loss: 0.0596 - val_accuracy: 0.9790\n",
            "Epoch 277/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1517 - accuracy: 0.9583 - val_loss: 0.0603 - val_accuracy: 0.9800\n",
            "Epoch 278/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1510 - accuracy: 0.9574 - val_loss: 0.0613 - val_accuracy: 0.9800\n",
            "Epoch 279/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1442 - accuracy: 0.9579 - val_loss: 0.0623 - val_accuracy: 0.9790\n",
            "Epoch 280/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1514 - accuracy: 0.9505 - val_loss: 0.0628 - val_accuracy: 0.9781\n",
            "Epoch 281/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1552 - accuracy: 0.9519 - val_loss: 0.0632 - val_accuracy: 0.9781\n",
            "Epoch 282/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1463 - accuracy: 0.9560 - val_loss: 0.0639 - val_accuracy: 0.9781\n",
            "Epoch 283/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1498 - accuracy: 0.9560 - val_loss: 0.0635 - val_accuracy: 0.9781\n",
            "Epoch 284/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1496 - accuracy: 0.9550 - val_loss: 0.0620 - val_accuracy: 0.9790\n",
            "Epoch 285/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1486 - accuracy: 0.9555 - val_loss: 0.0598 - val_accuracy: 0.9790\n",
            "Epoch 286/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1469 - accuracy: 0.9545 - val_loss: 0.0579 - val_accuracy: 0.9800\n",
            "Epoch 287/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1480 - accuracy: 0.9567 - val_loss: 0.0574 - val_accuracy: 0.9800\n",
            "Epoch 288/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1455 - accuracy: 0.9562 - val_loss: 0.0587 - val_accuracy: 0.9800\n",
            "Epoch 289/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1466 - accuracy: 0.9533 - val_loss: 0.0601 - val_accuracy: 0.9800\n",
            "Epoch 290/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1472 - accuracy: 0.9579 - val_loss: 0.0606 - val_accuracy: 0.9800\n",
            "Epoch 291/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1494 - accuracy: 0.9526 - val_loss: 0.0608 - val_accuracy: 0.9800\n",
            "Epoch 292/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1472 - accuracy: 0.9564 - val_loss: 0.0612 - val_accuracy: 0.9790\n",
            "Epoch 293/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1407 - accuracy: 0.9583 - val_loss: 0.0606 - val_accuracy: 0.9781\n",
            "Epoch 294/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1484 - accuracy: 0.9576 - val_loss: 0.0605 - val_accuracy: 0.9781\n",
            "Epoch 295/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1470 - accuracy: 0.9574 - val_loss: 0.0622 - val_accuracy: 0.9781\n",
            "Epoch 296/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1463 - accuracy: 0.9569 - val_loss: 0.0633 - val_accuracy: 0.9781\n",
            "Epoch 297/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1465 - accuracy: 0.9543 - val_loss: 0.0630 - val_accuracy: 0.9781\n",
            "Epoch 298/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1435 - accuracy: 0.9586 - val_loss: 0.0623 - val_accuracy: 0.9790\n",
            "Epoch 299/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1465 - accuracy: 0.9576 - val_loss: 0.0621 - val_accuracy: 0.9790\n",
            "Epoch 300/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1451 - accuracy: 0.9557 - val_loss: 0.0615 - val_accuracy: 0.9800\n",
            "Epoch 301/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1422 - accuracy: 0.9576 - val_loss: 0.0606 - val_accuracy: 0.9800\n",
            "Epoch 302/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1413 - accuracy: 0.9605 - val_loss: 0.0596 - val_accuracy: 0.9810\n",
            "Epoch 303/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1415 - accuracy: 0.9605 - val_loss: 0.0586 - val_accuracy: 0.9810\n",
            "Epoch 304/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1405 - accuracy: 0.9586 - val_loss: 0.0575 - val_accuracy: 0.9800\n",
            "Epoch 305/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1420 - accuracy: 0.9600 - val_loss: 0.0564 - val_accuracy: 0.9800\n",
            "Epoch 306/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1437 - accuracy: 0.9555 - val_loss: 0.0564 - val_accuracy: 0.9800\n",
            "Epoch 307/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1393 - accuracy: 0.9571 - val_loss: 0.0569 - val_accuracy: 0.9800\n",
            "Epoch 308/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1438 - accuracy: 0.9555 - val_loss: 0.0579 - val_accuracy: 0.9800\n",
            "Epoch 309/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1367 - accuracy: 0.9612 - val_loss: 0.0587 - val_accuracy: 0.9800\n",
            "Epoch 310/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1371 - accuracy: 0.9571 - val_loss: 0.0586 - val_accuracy: 0.9800\n",
            "Epoch 311/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1395 - accuracy: 0.9595 - val_loss: 0.0573 - val_accuracy: 0.9800\n",
            "Epoch 312/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1400 - accuracy: 0.9564 - val_loss: 0.0555 - val_accuracy: 0.9800\n",
            "Epoch 313/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1431 - accuracy: 0.9564 - val_loss: 0.0546 - val_accuracy: 0.9800\n",
            "Epoch 314/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1421 - accuracy: 0.9579 - val_loss: 0.0548 - val_accuracy: 0.9800\n",
            "Epoch 315/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1377 - accuracy: 0.9612 - val_loss: 0.0552 - val_accuracy: 0.9800\n",
            "Epoch 316/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1317 - accuracy: 0.9602 - val_loss: 0.0553 - val_accuracy: 0.9800\n",
            "Epoch 317/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1424 - accuracy: 0.9579 - val_loss: 0.0555 - val_accuracy: 0.9800\n",
            "Epoch 318/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1403 - accuracy: 0.9560 - val_loss: 0.0563 - val_accuracy: 0.9800\n",
            "Epoch 319/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1397 - accuracy: 0.9543 - val_loss: 0.0576 - val_accuracy: 0.9800\n",
            "Epoch 320/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1334 - accuracy: 0.9626 - val_loss: 0.0571 - val_accuracy: 0.9800\n",
            "Epoch 321/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1386 - accuracy: 0.9560 - val_loss: 0.0556 - val_accuracy: 0.9800\n",
            "Epoch 322/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1413 - accuracy: 0.9583 - val_loss: 0.0548 - val_accuracy: 0.9800\n",
            "Epoch 323/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1352 - accuracy: 0.9614 - val_loss: 0.0539 - val_accuracy: 0.9810\n",
            "Epoch 324/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1432 - accuracy: 0.9571 - val_loss: 0.0530 - val_accuracy: 0.9810\n",
            "Epoch 325/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1322 - accuracy: 0.9583 - val_loss: 0.0529 - val_accuracy: 0.9810\n",
            "Epoch 326/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1364 - accuracy: 0.9593 - val_loss: 0.0534 - val_accuracy: 0.9810\n",
            "Epoch 327/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1320 - accuracy: 0.9631 - val_loss: 0.0553 - val_accuracy: 0.9810\n",
            "Epoch 328/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1410 - accuracy: 0.9595 - val_loss: 0.0576 - val_accuracy: 0.9810\n",
            "Epoch 329/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1366 - accuracy: 0.9607 - val_loss: 0.0584 - val_accuracy: 0.9810\n",
            "Epoch 330/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1376 - accuracy: 0.9588 - val_loss: 0.0574 - val_accuracy: 0.9810\n",
            "Epoch 331/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1356 - accuracy: 0.9629 - val_loss: 0.0557 - val_accuracy: 0.9810\n",
            "Epoch 332/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1356 - accuracy: 0.9598 - val_loss: 0.0542 - val_accuracy: 0.9810\n",
            "Epoch 333/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1356 - accuracy: 0.9598 - val_loss: 0.0537 - val_accuracy: 0.9810\n",
            "Epoch 334/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1360 - accuracy: 0.9619 - val_loss: 0.0536 - val_accuracy: 0.9810\n",
            "Epoch 335/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1307 - accuracy: 0.9633 - val_loss: 0.0536 - val_accuracy: 0.9810\n",
            "Epoch 336/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1365 - accuracy: 0.9602 - val_loss: 0.0543 - val_accuracy: 0.9810\n",
            "Epoch 337/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1359 - accuracy: 0.9619 - val_loss: 0.0541 - val_accuracy: 0.9810\n",
            "Epoch 338/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1352 - accuracy: 0.9598 - val_loss: 0.0535 - val_accuracy: 0.9810\n",
            "Epoch 339/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1313 - accuracy: 0.9583 - val_loss: 0.0535 - val_accuracy: 0.9810\n",
            "Epoch 340/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1320 - accuracy: 0.9612 - val_loss: 0.0536 - val_accuracy: 0.9810\n",
            "Epoch 341/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1344 - accuracy: 0.9610 - val_loss: 0.0542 - val_accuracy: 0.9810\n",
            "Epoch 342/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1302 - accuracy: 0.9605 - val_loss: 0.0551 - val_accuracy: 0.9810\n",
            "Epoch 343/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1277 - accuracy: 0.9617 - val_loss: 0.0562 - val_accuracy: 0.9810\n",
            "Epoch 344/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1319 - accuracy: 0.9633 - val_loss: 0.0572 - val_accuracy: 0.9810\n",
            "Epoch 345/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1315 - accuracy: 0.9610 - val_loss: 0.0576 - val_accuracy: 0.9810\n",
            "Epoch 346/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1332 - accuracy: 0.9633 - val_loss: 0.0578 - val_accuracy: 0.9810\n",
            "Epoch 347/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1299 - accuracy: 0.9605 - val_loss: 0.0581 - val_accuracy: 0.9810\n",
            "Epoch 348/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1347 - accuracy: 0.9605 - val_loss: 0.0583 - val_accuracy: 0.9810\n",
            "Epoch 349/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1303 - accuracy: 0.9600 - val_loss: 0.0584 - val_accuracy: 0.9800\n",
            "Epoch 350/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1356 - accuracy: 0.9581 - val_loss: 0.0572 - val_accuracy: 0.9800\n",
            "Epoch 351/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1286 - accuracy: 0.9593 - val_loss: 0.0556 - val_accuracy: 0.9810\n",
            "Epoch 352/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1297 - accuracy: 0.9607 - val_loss: 0.0542 - val_accuracy: 0.9810\n",
            "Epoch 353/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1292 - accuracy: 0.9629 - val_loss: 0.0529 - val_accuracy: 0.9819\n",
            "Epoch 354/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1288 - accuracy: 0.9612 - val_loss: 0.0525 - val_accuracy: 0.9819\n",
            "Epoch 355/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1274 - accuracy: 0.9638 - val_loss: 0.0532 - val_accuracy: 0.9819\n",
            "Epoch 356/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1264 - accuracy: 0.9621 - val_loss: 0.0532 - val_accuracy: 0.9819\n",
            "Epoch 357/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1241 - accuracy: 0.9633 - val_loss: 0.0522 - val_accuracy: 0.9819\n",
            "Epoch 358/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1273 - accuracy: 0.9619 - val_loss: 0.0505 - val_accuracy: 0.9819\n",
            "Epoch 359/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1251 - accuracy: 0.9650 - val_loss: 0.0493 - val_accuracy: 0.9819\n",
            "Epoch 360/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1294 - accuracy: 0.9631 - val_loss: 0.0486 - val_accuracy: 0.9819\n",
            "Epoch 361/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1308 - accuracy: 0.9610 - val_loss: 0.0485 - val_accuracy: 0.9819\n",
            "Epoch 362/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1249 - accuracy: 0.9638 - val_loss: 0.0497 - val_accuracy: 0.9819\n",
            "Epoch 363/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1255 - accuracy: 0.9631 - val_loss: 0.0512 - val_accuracy: 0.9819\n",
            "Epoch 364/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1309 - accuracy: 0.9598 - val_loss: 0.0524 - val_accuracy: 0.9819\n",
            "Epoch 365/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1302 - accuracy: 0.9643 - val_loss: 0.0526 - val_accuracy: 0.9810\n",
            "Epoch 366/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1302 - accuracy: 0.9619 - val_loss: 0.0521 - val_accuracy: 0.9810\n",
            "Epoch 367/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1232 - accuracy: 0.9648 - val_loss: 0.0515 - val_accuracy: 0.9810\n",
            "Epoch 368/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1296 - accuracy: 0.9617 - val_loss: 0.0510 - val_accuracy: 0.9810\n",
            "Epoch 369/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1234 - accuracy: 0.9633 - val_loss: 0.0508 - val_accuracy: 0.9810\n",
            "Epoch 370/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1232 - accuracy: 0.9640 - val_loss: 0.0512 - val_accuracy: 0.9819\n",
            "Epoch 371/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1216 - accuracy: 0.9660 - val_loss: 0.0509 - val_accuracy: 0.9819\n",
            "Epoch 372/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1263 - accuracy: 0.9610 - val_loss: 0.0501 - val_accuracy: 0.9819\n",
            "Epoch 373/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1197 - accuracy: 0.9669 - val_loss: 0.0492 - val_accuracy: 0.9819\n",
            "Epoch 374/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1302 - accuracy: 0.9602 - val_loss: 0.0496 - val_accuracy: 0.9819\n",
            "Epoch 375/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1270 - accuracy: 0.9602 - val_loss: 0.0497 - val_accuracy: 0.9810\n",
            "Epoch 376/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1260 - accuracy: 0.9619 - val_loss: 0.0502 - val_accuracy: 0.9810\n",
            "Epoch 377/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1214 - accuracy: 0.9631 - val_loss: 0.0512 - val_accuracy: 0.9810\n",
            "Epoch 378/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1236 - accuracy: 0.9679 - val_loss: 0.0517 - val_accuracy: 0.9810\n",
            "Epoch 379/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1246 - accuracy: 0.9648 - val_loss: 0.0516 - val_accuracy: 0.9819\n",
            "Epoch 380/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1222 - accuracy: 0.9614 - val_loss: 0.0512 - val_accuracy: 0.9819\n",
            "Epoch 381/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1216 - accuracy: 0.9643 - val_loss: 0.0513 - val_accuracy: 0.9819\n",
            "Epoch 382/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1226 - accuracy: 0.9626 - val_loss: 0.0514 - val_accuracy: 0.9819\n",
            "Epoch 383/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1242 - accuracy: 0.9640 - val_loss: 0.0509 - val_accuracy: 0.9810\n",
            "Epoch 384/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1233 - accuracy: 0.9626 - val_loss: 0.0506 - val_accuracy: 0.9810\n",
            "Epoch 385/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1236 - accuracy: 0.9626 - val_loss: 0.0510 - val_accuracy: 0.9810\n",
            "Epoch 386/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1222 - accuracy: 0.9645 - val_loss: 0.0517 - val_accuracy: 0.9810\n",
            "Epoch 387/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1221 - accuracy: 0.9633 - val_loss: 0.0516 - val_accuracy: 0.9810\n",
            "Epoch 388/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1247 - accuracy: 0.9624 - val_loss: 0.0507 - val_accuracy: 0.9810\n",
            "Epoch 389/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1196 - accuracy: 0.9650 - val_loss: 0.0498 - val_accuracy: 0.9810\n",
            "Epoch 390/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1225 - accuracy: 0.9650 - val_loss: 0.0494 - val_accuracy: 0.9819\n",
            "Epoch 391/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1217 - accuracy: 0.9643 - val_loss: 0.0502 - val_accuracy: 0.9810\n",
            "Epoch 392/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1206 - accuracy: 0.9643 - val_loss: 0.0509 - val_accuracy: 0.9810\n",
            "Epoch 393/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1211 - accuracy: 0.9633 - val_loss: 0.0511 - val_accuracy: 0.9810\n",
            "Epoch 394/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1222 - accuracy: 0.9657 - val_loss: 0.0518 - val_accuracy: 0.9810\n",
            "Epoch 395/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1194 - accuracy: 0.9657 - val_loss: 0.0521 - val_accuracy: 0.9810\n",
            "Epoch 396/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1215 - accuracy: 0.9660 - val_loss: 0.0516 - val_accuracy: 0.9810\n",
            "Epoch 397/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1288 - accuracy: 0.9595 - val_loss: 0.0505 - val_accuracy: 0.9819\n",
            "Epoch 398/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1168 - accuracy: 0.9638 - val_loss: 0.0487 - val_accuracy: 0.9829\n",
            "Epoch 399/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1209 - accuracy: 0.9652 - val_loss: 0.0477 - val_accuracy: 0.9829\n",
            "Epoch 400/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1175 - accuracy: 0.9640 - val_loss: 0.0476 - val_accuracy: 0.9829\n",
            "Epoch 401/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1200 - accuracy: 0.9667 - val_loss: 0.0485 - val_accuracy: 0.9829\n",
            "Epoch 402/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1184 - accuracy: 0.9640 - val_loss: 0.0502 - val_accuracy: 0.9819\n",
            "Epoch 403/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1233 - accuracy: 0.9652 - val_loss: 0.0513 - val_accuracy: 0.9819\n",
            "Epoch 404/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1241 - accuracy: 0.9648 - val_loss: 0.0520 - val_accuracy: 0.9819\n",
            "Epoch 405/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1175 - accuracy: 0.9664 - val_loss: 0.0527 - val_accuracy: 0.9819\n",
            "Epoch 406/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1221 - accuracy: 0.9607 - val_loss: 0.0529 - val_accuracy: 0.9819\n",
            "Epoch 407/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1187 - accuracy: 0.9650 - val_loss: 0.0522 - val_accuracy: 0.9819\n",
            "Epoch 408/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1177 - accuracy: 0.9662 - val_loss: 0.0515 - val_accuracy: 0.9819\n",
            "Epoch 409/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1160 - accuracy: 0.9657 - val_loss: 0.0513 - val_accuracy: 0.9819\n",
            "Epoch 410/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1178 - accuracy: 0.9645 - val_loss: 0.0509 - val_accuracy: 0.9819\n",
            "Epoch 411/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1207 - accuracy: 0.9621 - val_loss: 0.0505 - val_accuracy: 0.9819\n",
            "Epoch 412/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1229 - accuracy: 0.9645 - val_loss: 0.0510 - val_accuracy: 0.9819\n",
            "Epoch 413/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1165 - accuracy: 0.9631 - val_loss: 0.0523 - val_accuracy: 0.9819\n",
            "Epoch 414/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1240 - accuracy: 0.9636 - val_loss: 0.0524 - val_accuracy: 0.9819\n",
            "Epoch 415/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1181 - accuracy: 0.9629 - val_loss: 0.0515 - val_accuracy: 0.9819\n",
            "Epoch 416/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1181 - accuracy: 0.9669 - val_loss: 0.0495 - val_accuracy: 0.9829\n",
            "Epoch 417/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1174 - accuracy: 0.9657 - val_loss: 0.0476 - val_accuracy: 0.9848\n",
            "Epoch 418/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1154 - accuracy: 0.9669 - val_loss: 0.0460 - val_accuracy: 0.9857\n",
            "Epoch 419/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1160 - accuracy: 0.9648 - val_loss: 0.0451 - val_accuracy: 0.9857\n",
            "Epoch 420/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1162 - accuracy: 0.9662 - val_loss: 0.0448 - val_accuracy: 0.9867\n",
            "Epoch 421/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1141 - accuracy: 0.9662 - val_loss: 0.0453 - val_accuracy: 0.9867\n",
            "Epoch 422/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1095 - accuracy: 0.9681 - val_loss: 0.0466 - val_accuracy: 0.9848\n",
            "Epoch 423/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1127 - accuracy: 0.9660 - val_loss: 0.0481 - val_accuracy: 0.9848\n",
            "Epoch 424/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1180 - accuracy: 0.9681 - val_loss: 0.0484 - val_accuracy: 0.9848\n",
            "Epoch 425/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1149 - accuracy: 0.9657 - val_loss: 0.0482 - val_accuracy: 0.9838\n",
            "Epoch 426/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1174 - accuracy: 0.9652 - val_loss: 0.0479 - val_accuracy: 0.9838\n",
            "Epoch 427/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1175 - accuracy: 0.9669 - val_loss: 0.0473 - val_accuracy: 0.9838\n",
            "Epoch 428/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1168 - accuracy: 0.9643 - val_loss: 0.0470 - val_accuracy: 0.9848\n",
            "Epoch 429/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1175 - accuracy: 0.9660 - val_loss: 0.0473 - val_accuracy: 0.9838\n",
            "Epoch 430/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1148 - accuracy: 0.9664 - val_loss: 0.0477 - val_accuracy: 0.9838\n",
            "Epoch 431/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1162 - accuracy: 0.9671 - val_loss: 0.0483 - val_accuracy: 0.9829\n",
            "Epoch 432/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1119 - accuracy: 0.9688 - val_loss: 0.0482 - val_accuracy: 0.9838\n",
            "Epoch 433/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1120 - accuracy: 0.9676 - val_loss: 0.0478 - val_accuracy: 0.9838\n",
            "Epoch 434/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1151 - accuracy: 0.9655 - val_loss: 0.0478 - val_accuracy: 0.9838\n",
            "Epoch 435/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1118 - accuracy: 0.9698 - val_loss: 0.0480 - val_accuracy: 0.9838\n",
            "Epoch 436/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1105 - accuracy: 0.9683 - val_loss: 0.0473 - val_accuracy: 0.9848\n",
            "Epoch 437/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1123 - accuracy: 0.9667 - val_loss: 0.0454 - val_accuracy: 0.9857\n",
            "Epoch 438/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1135 - accuracy: 0.9660 - val_loss: 0.0440 - val_accuracy: 0.9867\n",
            "Epoch 439/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1099 - accuracy: 0.9667 - val_loss: 0.0438 - val_accuracy: 0.9867\n",
            "Epoch 440/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1136 - accuracy: 0.9650 - val_loss: 0.0442 - val_accuracy: 0.9857\n",
            "Epoch 441/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1099 - accuracy: 0.9674 - val_loss: 0.0449 - val_accuracy: 0.9848\n",
            "Epoch 442/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1098 - accuracy: 0.9686 - val_loss: 0.0467 - val_accuracy: 0.9848\n",
            "Epoch 443/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1152 - accuracy: 0.9686 - val_loss: 0.0477 - val_accuracy: 0.9838\n",
            "Epoch 444/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1082 - accuracy: 0.9688 - val_loss: 0.0473 - val_accuracy: 0.9838\n",
            "Epoch 445/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1151 - accuracy: 0.9660 - val_loss: 0.0467 - val_accuracy: 0.9848\n",
            "Epoch 446/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1092 - accuracy: 0.9693 - val_loss: 0.0460 - val_accuracy: 0.9848\n",
            "Epoch 447/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1135 - accuracy: 0.9645 - val_loss: 0.0456 - val_accuracy: 0.9848\n",
            "Epoch 448/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1137 - accuracy: 0.9693 - val_loss: 0.0447 - val_accuracy: 0.9857\n",
            "Epoch 449/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1150 - accuracy: 0.9660 - val_loss: 0.0441 - val_accuracy: 0.9857\n",
            "Epoch 450/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1105 - accuracy: 0.9681 - val_loss: 0.0435 - val_accuracy: 0.9876\n",
            "Epoch 451/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1090 - accuracy: 0.9693 - val_loss: 0.0431 - val_accuracy: 0.9857\n",
            "Epoch 452/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1123 - accuracy: 0.9671 - val_loss: 0.0438 - val_accuracy: 0.9848\n",
            "Epoch 453/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1121 - accuracy: 0.9650 - val_loss: 0.0454 - val_accuracy: 0.9848\n",
            "Epoch 454/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1114 - accuracy: 0.9674 - val_loss: 0.0477 - val_accuracy: 0.9848\n",
            "Epoch 455/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1122 - accuracy: 0.9688 - val_loss: 0.0487 - val_accuracy: 0.9848\n",
            "Epoch 456/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1117 - accuracy: 0.9681 - val_loss: 0.0475 - val_accuracy: 0.9848\n",
            "Epoch 457/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1103 - accuracy: 0.9679 - val_loss: 0.0469 - val_accuracy: 0.9848\n",
            "Epoch 458/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1088 - accuracy: 0.9695 - val_loss: 0.0462 - val_accuracy: 0.9848\n",
            "Epoch 459/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1109 - accuracy: 0.9721 - val_loss: 0.0438 - val_accuracy: 0.9886\n",
            "Epoch 460/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1125 - accuracy: 0.9667 - val_loss: 0.0423 - val_accuracy: 0.9905\n",
            "Epoch 461/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1100 - accuracy: 0.9683 - val_loss: 0.0420 - val_accuracy: 0.9886\n",
            "Epoch 462/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1097 - accuracy: 0.9695 - val_loss: 0.0421 - val_accuracy: 0.9886\n",
            "Epoch 463/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1068 - accuracy: 0.9719 - val_loss: 0.0422 - val_accuracy: 0.9876\n",
            "Epoch 464/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1077 - accuracy: 0.9693 - val_loss: 0.0422 - val_accuracy: 0.9857\n",
            "Epoch 465/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1086 - accuracy: 0.9679 - val_loss: 0.0426 - val_accuracy: 0.9848\n",
            "Epoch 466/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1046 - accuracy: 0.9700 - val_loss: 0.0435 - val_accuracy: 0.9848\n",
            "Epoch 467/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1058 - accuracy: 0.9683 - val_loss: 0.0448 - val_accuracy: 0.9848\n",
            "Epoch 468/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1104 - accuracy: 0.9695 - val_loss: 0.0457 - val_accuracy: 0.9848\n",
            "Epoch 469/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1088 - accuracy: 0.9705 - val_loss: 0.0464 - val_accuracy: 0.9848\n",
            "Epoch 470/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1058 - accuracy: 0.9719 - val_loss: 0.0467 - val_accuracy: 0.9848\n",
            "Epoch 471/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1086 - accuracy: 0.9676 - val_loss: 0.0466 - val_accuracy: 0.9848\n",
            "Epoch 472/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1098 - accuracy: 0.9683 - val_loss: 0.0461 - val_accuracy: 0.9848\n",
            "Epoch 473/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1104 - accuracy: 0.9683 - val_loss: 0.0453 - val_accuracy: 0.9848\n",
            "Epoch 474/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1086 - accuracy: 0.9679 - val_loss: 0.0452 - val_accuracy: 0.9848\n",
            "Epoch 475/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1101 - accuracy: 0.9657 - val_loss: 0.0455 - val_accuracy: 0.9848\n",
            "Epoch 476/500\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1064 - accuracy: 0.9681 - val_loss: 0.0455 - val_accuracy: 0.9848\n",
            "Epoch 477/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1079 - accuracy: 0.9681 - val_loss: 0.0447 - val_accuracy: 0.9848\n",
            "Epoch 478/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1090 - accuracy: 0.9683 - val_loss: 0.0434 - val_accuracy: 0.9848\n",
            "Epoch 479/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1058 - accuracy: 0.9714 - val_loss: 0.0432 - val_accuracy: 0.9848\n",
            "Epoch 480/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1079 - accuracy: 0.9683 - val_loss: 0.0450 - val_accuracy: 0.9848\n",
            "Epoch 481/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1059 - accuracy: 0.9702 - val_loss: 0.0462 - val_accuracy: 0.9848\n",
            "Epoch 482/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1080 - accuracy: 0.9681 - val_loss: 0.0458 - val_accuracy: 0.9848\n",
            "Epoch 483/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1067 - accuracy: 0.9695 - val_loss: 0.0449 - val_accuracy: 0.9848\n",
            "Epoch 484/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1026 - accuracy: 0.9702 - val_loss: 0.0441 - val_accuracy: 0.9848\n",
            "Epoch 485/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1097 - accuracy: 0.9662 - val_loss: 0.0431 - val_accuracy: 0.9848\n",
            "Epoch 486/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1133 - accuracy: 0.9648 - val_loss: 0.0424 - val_accuracy: 0.9867\n",
            "Epoch 487/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1094 - accuracy: 0.9681 - val_loss: 0.0419 - val_accuracy: 0.9876\n",
            "Epoch 488/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1030 - accuracy: 0.9693 - val_loss: 0.0414 - val_accuracy: 0.9867\n",
            "Epoch 489/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1034 - accuracy: 0.9707 - val_loss: 0.0413 - val_accuracy: 0.9867\n",
            "Epoch 490/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1073 - accuracy: 0.9693 - val_loss: 0.0410 - val_accuracy: 0.9867\n",
            "Epoch 491/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1035 - accuracy: 0.9667 - val_loss: 0.0403 - val_accuracy: 0.9876\n",
            "Epoch 492/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1071 - accuracy: 0.9688 - val_loss: 0.0407 - val_accuracy: 0.9867\n",
            "Epoch 493/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1059 - accuracy: 0.9671 - val_loss: 0.0423 - val_accuracy: 0.9848\n",
            "Epoch 494/500\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1073 - accuracy: 0.9693 - val_loss: 0.0435 - val_accuracy: 0.9848\n",
            "Epoch 495/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1074 - accuracy: 0.9705 - val_loss: 0.0436 - val_accuracy: 0.9848\n",
            "Epoch 496/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1044 - accuracy: 0.9688 - val_loss: 0.0428 - val_accuracy: 0.9848\n",
            "Epoch 497/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1013 - accuracy: 0.9712 - val_loss: 0.0417 - val_accuracy: 0.9848\n",
            "Epoch 498/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1058 - accuracy: 0.9686 - val_loss: 0.0412 - val_accuracy: 0.9848\n",
            "Epoch 499/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1082 - accuracy: 0.9676 - val_loss: 0.0407 - val_accuracy: 0.9848\n",
            "Epoch 500/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1054 - accuracy: 0.9683 - val_loss: 0.0403 - val_accuracy: 0.9848\n",
            "\n",
            "Score:  [0.09690103679895401, 0.9702857136726379]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5LGnpkLLYAV",
        "outputId": "ed425554-8586-42cb-c51e-13c66044250d"
      },
      "source": [
        "np.round(model.predict(dat_resaXredts))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "z2VYCpsVLYAV",
        "outputId": "0f53f519-b57e-4319-c360-7441e8db22c0"
      },
      "source": [
        "ffnn_res=pd.DataFrame(dat_resaXredts)\n",
        "ffnn_res['Disease']=dat_resaYts\n",
        "ffnn_res=pd.concat([ffnn_res,pd.DataFrame(np.round(model.predict(dat_resaXredts))).astype(int)],axis=1)\n",
        "ffnn_res"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>Disease</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.15</td>\n",
              "      <td>5.41e-01</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.16</td>\n",
              "      <td>breast cancer</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.35</td>\n",
              "      <td>4.39e-02</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.45</td>\n",
              "      <td>diffuse large B-cell lymphoma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.04</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.85</td>\n",
              "      <td>3.74e-02</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.70</td>\n",
              "      <td>chronic lymphocytic leukaemia</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>5.55e-03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.08</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.03</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2.99e-01</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>breast cancer</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1745</th>\n",
              "      <td>0.03</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.86e-02</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.09</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1746</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.08</td>\n",
              "      <td>7.05e-02</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.13</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1747</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.17</td>\n",
              "      <td>3.95e-02</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.10</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.06</td>\n",
              "      <td>3.56e-02</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.10</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1749</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.08</td>\n",
              "      <td>6.43e-02</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.10</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1750 rows × 68 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0     1     2     3     4     5     6  ...  0  1  2  3  4  5  6\n",
              "0     0.07  0.77  0.71  0.60  0.65  0.80  0.62  ...  0  0  1  0  0  0  0\n",
              "1     0.02  0.49  0.38  0.11  0.17  0.03  0.74  ...  0  0  0  0  1  0  0\n",
              "2     0.04  0.28  0.34  0.11  0.08  0.08  0.76  ...  0  0  0  1  0  0  0\n",
              "3     0.03  0.81  0.69  0.20  0.28  0.02  0.23  ...  0  0  0  0  0  0  1\n",
              "4     0.03  0.76  0.73  0.44  0.54  0.80  0.30  ...  0  0  1  0  0  0  0\n",
              "...    ...   ...   ...   ...   ...   ...   ...  ... .. .. .. .. .. .. ..\n",
              "1745  0.03  0.36  0.42  0.87  0.81  0.07  0.40  ...  0  0  0  0  0  1  0\n",
              "1746  0.06  0.19  0.37  0.92  0.88  0.15  0.44  ...  0  0  0  0  0  1  0\n",
              "1747  0.05  0.16  0.19  0.53  0.57  0.08  0.44  ...  0  0  0  0  0  1  0\n",
              "1748  0.10  0.12  0.12  0.92  0.92  0.03  0.29  ...  0  0  0  0  0  1  0\n",
              "1749  0.05  0.04  0.11  0.80  0.79  0.08  0.37  ...  0  0  0  0  0  1  0\n",
              "\n",
              "[1750 rows x 68 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpp2RyXlwtqz"
      },
      "source": [
        "#### autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR02hqz8wtqz"
      },
      "source": [
        "##### logistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ar9xDr8wtq0"
      },
      "source": [
        "my_param_grid = [\n",
        "    {'solver': ['newton-cg', 'lbfgs', 'saga'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l2'], 'max_iter': [200]},\n",
        "    {'solver': ['liblinear'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l1', 'l2'], 'max_iter': [200]}\n",
        "]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mcs421O9wtq0"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqHR7yBvwtq0"
      },
      "source": [
        "modellr = GridSearchCV(estimator=LogisticRegression(n_jobs=-1), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wVhhYt7wtq0",
        "outputId": "32ef8328-a2d6-461a-b7c6-7a77c86f3934"
      },
      "source": [
        "modellr.fit(dat_resaXautotr, dat_resaYtr)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1539: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=-1, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid=[{'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l2'],\n",
              "                          'solver': ['newton-cg', 'lbfgs', 'saga']},\n",
              "                         {'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l1', 'l2'], 'solver': ['liblinear']}],\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_log_loss', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCq78FHxwtq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3daba1f7-6b6a-4a7b-c329-d41dd71e364a"
      },
      "source": [
        "print(cross_val_score(modellr.best_estimator_,dat_resaXautotr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.12655681 -0.0976451  -0.11601071 -0.13161541 -0.09104267 -0.12286903\n",
            " -0.12011317 -0.08687225 -0.10044519 -0.15000925 -0.10099857 -0.09825351\n",
            " -0.11944168 -0.1113333  -0.10815304 -0.11052717 -0.15832723 -0.11419243\n",
            " -0.07713415 -0.10115212 -0.11654007 -0.09975397 -0.11651705 -0.10720035\n",
            " -0.13763145 -0.12456123 -0.09769013 -0.10396089 -0.12062673 -0.08953588\n",
            " -0.1175234  -0.0981525  -0.09576474 -0.11312211 -0.11765891 -0.12055402\n",
            " -0.09379423 -0.09703678 -0.11539929 -0.1264573  -0.10392973 -0.09348683\n",
            " -0.1248389  -0.10894806 -0.14189531 -0.11467477 -0.10416916 -0.094422\n",
            " -0.0768164  -0.15000994]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0ctOrqxwtq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa48155-3910-4119-8dab-8e910b9f59d1"
      },
      "source": [
        "modellr.best_score_"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.11129512389618655"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVP8b8_Jwtq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e8f5e0-205c-4cae-dc6f-758b3c1336f6"
      },
      "source": [
        "log_loss(dat_resaYts, modellr.predict_proba(dat_resaXautots))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08167016133726283"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtfSZYTOwtq0"
      },
      "source": [
        "modellr_best = modellr.best_estimator_"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKy4q5vIwtq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0221c5cb-e797-4045-b8b0-f55b1a8d9189"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modellr_best.predict(dat_resaXautotr)))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.97      0.98      0.98       750\n",
            "        breast adenocarcinoma       0.98      0.99      0.99       750\n",
            "                breast cancer       0.98      0.97      0.98       750\n",
            "chronic lymphocytic leukaemia       0.99      0.99      0.99       750\n",
            "diffuse large B-cell lymphoma       0.99      0.99      0.99       750\n",
            "             multiple myeloma       0.97      0.97      0.97       750\n",
            "                       normal       0.93      0.93      0.93       750\n",
            "\n",
            "                     accuracy                           0.97      5250\n",
            "                    macro avg       0.97      0.97      0.97      5250\n",
            "                 weighted avg       0.97      0.97      0.97      5250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeLwf_45wtq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e041732-7bf9-4aac-f311-0f8138cc18f1"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modellr_best.predict(dat_resaXautots)))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               precision    recall  f1-score   support\n",
            "\n",
            "      acute myeloid leukaemia       0.97      0.98      0.97       250\n",
            "        breast adenocarcinoma       0.99      0.98      0.98       250\n",
            "                breast cancer       0.99      0.97      0.98       250\n",
            "chronic lymphocytic leukaemia       0.99      1.00      0.99       250\n",
            "diffuse large B-cell lymphoma       0.99      1.00      0.99       250\n",
            "             multiple myeloma       0.95      1.00      0.97       250\n",
            "                       normal       0.96      0.92      0.94       250\n",
            "\n",
            "                     accuracy                           0.98      1750\n",
            "                    macro avg       0.98      0.98      0.98      1750\n",
            "                 weighted avg       0.98      0.98      0.98      1750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s9SA4zDGEde"
      },
      "source": [
        "##### fully connected ffnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_Lff2jTGEdf",
        "outputId": "fc940c6b-6b19-4feb-9f2b-acb2e818b596"
      },
      "source": [
        "dat_resaYtr"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['chronic lymphocytic leukaemia', 'breast cancer', 'breast cancer',\n",
              "       ..., 'multiple myeloma', 'multiple myeloma', 'multiple myeloma'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKCmJi2vGEdg"
      },
      "source": [
        "num_classes=7\n",
        "dat_resaYtrx=pd.DataFrame(dat_resaYtr)\n",
        "dat_resaYtrx[0] = pd.Categorical(dat_resaYtrx[0])\n",
        "dat_resaYtrx['code'] = dat_resaYtrx[0].cat.codes\n",
        "dat_resaYtrx_=np.array(dat_resaYtrx['code']\n",
        "                       )\n",
        "dat_resaYtsx=pd.DataFrame(dat_resaYts)\n",
        "dat_resaYtsx[0] = pd.Categorical(dat_resaYtsx[0])\n",
        "dat_resaYtsx['code'] = dat_resaYtsx[0].cat.codes\n",
        "dat_resaYtsx_=np.array(dat_resaYtsx['code'])"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA05m2wvGEdh",
        "outputId": "5c063c61-b119-42fe-c010-cc97aa51d51b"
      },
      "source": [
        "dat_resaYtrcl = to_categorical(dat_resaYtrx_, num_classes)\n",
        "dat_resaYtscl = to_categorical(dat_resaYtsx_, num_classes)\n",
        "dat_resaYtrcl"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9HmKcfJGEdh"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(30, activation='relu', input_shape=(60,)))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brp9hqRiGEdh"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI5HF3VBGEdh",
        "outputId": "69faa785-0e09-4732-880d-b94908c2d42f"
      },
      "source": [
        "history_ = model.fit(dat_resaXautotr, dat_resaYtrcl, \n",
        "                            batch_size=2000, epochs=500, verbose=1, validation_split=0.2)\n",
        " \n",
        "score_ = model.evaluate(dat_resaXautots,dat_resaYtscl , verbose=0)\n",
        "print('\\nScore: ', score_)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 12.7225 - accuracy: 0.1164 - val_loss: 9.0511 - val_accuracy: 0.0581\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 10.6075 - accuracy: 0.1164 - val_loss: 6.4916 - val_accuracy: 0.0524\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 8.8175 - accuracy: 0.1350 - val_loss: 4.3959 - val_accuracy: 0.0552\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 7.5322 - accuracy: 0.1548 - val_loss: 3.0137 - val_accuracy: 0.1638\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 6.4939 - accuracy: 0.1836 - val_loss: 2.3171 - val_accuracy: 0.3229\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 5.7383 - accuracy: 0.2181 - val_loss: 1.9479 - val_accuracy: 0.4133\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 5.1224 - accuracy: 0.2483 - val_loss: 1.7235 - val_accuracy: 0.4943\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 4.5929 - accuracy: 0.2786 - val_loss: 1.5886 - val_accuracy: 0.5505\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 4.0621 - accuracy: 0.2971 - val_loss: 1.5218 - val_accuracy: 0.5648\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 3.6522 - accuracy: 0.3226 - val_loss: 1.4836 - val_accuracy: 0.5533\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 3.2105 - accuracy: 0.3536 - val_loss: 1.4714 - val_accuracy: 0.5410\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 2.9802 - accuracy: 0.3607 - val_loss: 1.4707 - val_accuracy: 0.5152\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 2.6612 - accuracy: 0.3993 - val_loss: 1.4622 - val_accuracy: 0.5000\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 2.5147 - accuracy: 0.3938 - val_loss: 1.4403 - val_accuracy: 0.5019\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 2.2704 - accuracy: 0.4295 - val_loss: 1.3879 - val_accuracy: 0.5162\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 2.1029 - accuracy: 0.4362 - val_loss: 1.3492 - val_accuracy: 0.5324\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 2.0288 - accuracy: 0.4462 - val_loss: 1.3141 - val_accuracy: 0.5429\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.9135 - accuracy: 0.4640 - val_loss: 1.3032 - val_accuracy: 0.5410\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.8241 - accuracy: 0.4810 - val_loss: 1.3054 - val_accuracy: 0.5314\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 1.7656 - accuracy: 0.4810 - val_loss: 1.3127 - val_accuracy: 0.5114\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.7036 - accuracy: 0.4914 - val_loss: 1.3145 - val_accuracy: 0.5076\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.6122 - accuracy: 0.5010 - val_loss: 1.3156 - val_accuracy: 0.5000\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.5804 - accuracy: 0.5136 - val_loss: 1.3057 - val_accuracy: 0.5057\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 1.5172 - accuracy: 0.5214 - val_loss: 1.2892 - val_accuracy: 0.5105\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.5065 - accuracy: 0.5348 - val_loss: 1.2600 - val_accuracy: 0.5248\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.4533 - accuracy: 0.5402 - val_loss: 1.2287 - val_accuracy: 0.5286\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 1.3985 - accuracy: 0.5490 - val_loss: 1.2065 - val_accuracy: 0.5314\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 1.3533 - accuracy: 0.5726 - val_loss: 1.1842 - val_accuracy: 0.5352\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 1.3098 - accuracy: 0.5805 - val_loss: 1.1693 - val_accuracy: 0.5276\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 1.2784 - accuracy: 0.5902 - val_loss: 1.1544 - val_accuracy: 0.5343\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.2465 - accuracy: 0.6010 - val_loss: 1.1298 - val_accuracy: 0.5352\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.2054 - accuracy: 0.6079 - val_loss: 1.1017 - val_accuracy: 0.5343\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.1997 - accuracy: 0.6110 - val_loss: 1.0680 - val_accuracy: 0.5429\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.1453 - accuracy: 0.6300 - val_loss: 1.0319 - val_accuracy: 0.5543\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.1120 - accuracy: 0.6331 - val_loss: 0.9952 - val_accuracy: 0.5771\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.0934 - accuracy: 0.6412 - val_loss: 0.9670 - val_accuracy: 0.5905\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.0695 - accuracy: 0.6479 - val_loss: 0.9411 - val_accuracy: 0.6057\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 1.0843 - accuracy: 0.6407 - val_loss: 0.9198 - val_accuracy: 0.6248\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.0248 - accuracy: 0.6652 - val_loss: 0.9018 - val_accuracy: 0.6381\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 1.0161 - accuracy: 0.6764 - val_loss: 0.8880 - val_accuracy: 0.6467\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.0011 - accuracy: 0.6631 - val_loss: 0.8678 - val_accuracy: 0.6543\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.9803 - accuracy: 0.6752 - val_loss: 0.8338 - val_accuracy: 0.6914\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.9532 - accuracy: 0.6960 - val_loss: 0.7986 - val_accuracy: 0.7229\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.9394 - accuracy: 0.6952 - val_loss: 0.7675 - val_accuracy: 0.7448\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.9114 - accuracy: 0.7095 - val_loss: 0.7417 - val_accuracy: 0.7619\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.8584 - accuracy: 0.7210 - val_loss: 0.7236 - val_accuracy: 0.7676\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.8514 - accuracy: 0.7214 - val_loss: 0.7107 - val_accuracy: 0.7705\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.8330 - accuracy: 0.7293 - val_loss: 0.7019 - val_accuracy: 0.7705\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.8149 - accuracy: 0.7374 - val_loss: 0.6857 - val_accuracy: 0.7829\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.8182 - accuracy: 0.7407 - val_loss: 0.6626 - val_accuracy: 0.7943\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.8127 - accuracy: 0.7457 - val_loss: 0.6382 - val_accuracy: 0.8038\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.8083 - accuracy: 0.7417 - val_loss: 0.6225 - val_accuracy: 0.8076\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.7853 - accuracy: 0.7479 - val_loss: 0.6050 - val_accuracy: 0.8114\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.7571 - accuracy: 0.7586 - val_loss: 0.5948 - val_accuracy: 0.8114\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.7292 - accuracy: 0.7600 - val_loss: 0.5798 - val_accuracy: 0.8124\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.7204 - accuracy: 0.7671 - val_loss: 0.5580 - val_accuracy: 0.8257\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.7155 - accuracy: 0.7774 - val_loss: 0.5402 - val_accuracy: 0.8352\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.6830 - accuracy: 0.7845 - val_loss: 0.5257 - val_accuracy: 0.8410\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.6665 - accuracy: 0.7883 - val_loss: 0.5057 - val_accuracy: 0.8552\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.6920 - accuracy: 0.7836 - val_loss: 0.4864 - val_accuracy: 0.8600\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.6588 - accuracy: 0.7821 - val_loss: 0.4739 - val_accuracy: 0.8648\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.6612 - accuracy: 0.7905 - val_loss: 0.4685 - val_accuracy: 0.8695\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.6458 - accuracy: 0.7848 - val_loss: 0.4645 - val_accuracy: 0.8695\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.6553 - accuracy: 0.7883 - val_loss: 0.4626 - val_accuracy: 0.8695\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.6341 - accuracy: 0.7957 - val_loss: 0.4547 - val_accuracy: 0.8686\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.6232 - accuracy: 0.7967 - val_loss: 0.4449 - val_accuracy: 0.8733\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.6129 - accuracy: 0.8007 - val_loss: 0.4332 - val_accuracy: 0.8752\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5961 - accuracy: 0.8090 - val_loss: 0.4163 - val_accuracy: 0.8800\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5917 - accuracy: 0.8019 - val_loss: 0.3989 - val_accuracy: 0.8876\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5846 - accuracy: 0.8117 - val_loss: 0.3877 - val_accuracy: 0.8914\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5506 - accuracy: 0.8160 - val_loss: 0.3800 - val_accuracy: 0.8924\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.5796 - accuracy: 0.8117 - val_loss: 0.3782 - val_accuracy: 0.8933\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5557 - accuracy: 0.8190 - val_loss: 0.3813 - val_accuracy: 0.8952\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.5616 - accuracy: 0.8164 - val_loss: 0.3747 - val_accuracy: 0.8971\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.5605 - accuracy: 0.8086 - val_loss: 0.3635 - val_accuracy: 0.9019\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.5427 - accuracy: 0.8210 - val_loss: 0.3549 - val_accuracy: 0.9010\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5208 - accuracy: 0.8260 - val_loss: 0.3503 - val_accuracy: 0.9019\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5369 - accuracy: 0.8271 - val_loss: 0.3425 - val_accuracy: 0.9029\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.5205 - accuracy: 0.8338 - val_loss: 0.3323 - val_accuracy: 0.9086\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.5245 - accuracy: 0.8221 - val_loss: 0.3322 - val_accuracy: 0.9057\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.5102 - accuracy: 0.8345 - val_loss: 0.3400 - val_accuracy: 0.9048\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.5015 - accuracy: 0.8357 - val_loss: 0.3361 - val_accuracy: 0.9067\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5031 - accuracy: 0.8336 - val_loss: 0.3183 - val_accuracy: 0.9133\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4714 - accuracy: 0.8436 - val_loss: 0.3039 - val_accuracy: 0.9190\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.5058 - accuracy: 0.8338 - val_loss: 0.2969 - val_accuracy: 0.9219\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4903 - accuracy: 0.8412 - val_loss: 0.2876 - val_accuracy: 0.9257\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4666 - accuracy: 0.8469 - val_loss: 0.2805 - val_accuracy: 0.9276\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.4954 - accuracy: 0.8405 - val_loss: 0.2783 - val_accuracy: 0.9314\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4846 - accuracy: 0.8433 - val_loss: 0.2780 - val_accuracy: 0.9324\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4508 - accuracy: 0.8517 - val_loss: 0.2738 - val_accuracy: 0.9343\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4757 - accuracy: 0.8407 - val_loss: 0.2643 - val_accuracy: 0.9410\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4443 - accuracy: 0.8576 - val_loss: 0.2522 - val_accuracy: 0.9438\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4384 - accuracy: 0.8605 - val_loss: 0.2437 - val_accuracy: 0.9486\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4441 - accuracy: 0.8607 - val_loss: 0.2442 - val_accuracy: 0.9457\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4512 - accuracy: 0.8479 - val_loss: 0.2443 - val_accuracy: 0.9448\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4336 - accuracy: 0.8605 - val_loss: 0.2400 - val_accuracy: 0.9476\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4160 - accuracy: 0.8645 - val_loss: 0.2352 - val_accuracy: 0.9486\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4314 - accuracy: 0.8621 - val_loss: 0.2324 - val_accuracy: 0.9495\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4294 - accuracy: 0.8626 - val_loss: 0.2323 - val_accuracy: 0.9495\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4294 - accuracy: 0.8586 - val_loss: 0.2322 - val_accuracy: 0.9505\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4130 - accuracy: 0.8710 - val_loss: 0.2300 - val_accuracy: 0.9505\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4063 - accuracy: 0.8693 - val_loss: 0.2261 - val_accuracy: 0.9514\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3961 - accuracy: 0.8738 - val_loss: 0.2236 - val_accuracy: 0.9495\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4091 - accuracy: 0.8662 - val_loss: 0.2226 - val_accuracy: 0.9505\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4217 - accuracy: 0.8669 - val_loss: 0.2216 - val_accuracy: 0.9495\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4182 - accuracy: 0.8660 - val_loss: 0.2229 - val_accuracy: 0.9505\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3932 - accuracy: 0.8740 - val_loss: 0.2288 - val_accuracy: 0.9495\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3908 - accuracy: 0.8781 - val_loss: 0.2377 - val_accuracy: 0.9476\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3833 - accuracy: 0.8776 - val_loss: 0.2390 - val_accuracy: 0.9476\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.3831 - accuracy: 0.8781 - val_loss: 0.2255 - val_accuracy: 0.9514\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3788 - accuracy: 0.8757 - val_loss: 0.2093 - val_accuracy: 0.9571\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3838 - accuracy: 0.8807 - val_loss: 0.2013 - val_accuracy: 0.9600\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3561 - accuracy: 0.8819 - val_loss: 0.1984 - val_accuracy: 0.9600\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3631 - accuracy: 0.8860 - val_loss: 0.1968 - val_accuracy: 0.9610\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3617 - accuracy: 0.8807 - val_loss: 0.1964 - val_accuracy: 0.9610\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3704 - accuracy: 0.8860 - val_loss: 0.1963 - val_accuracy: 0.9581\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3696 - accuracy: 0.8833 - val_loss: 0.1932 - val_accuracy: 0.9571\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3420 - accuracy: 0.8867 - val_loss: 0.1886 - val_accuracy: 0.9571\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3542 - accuracy: 0.8852 - val_loss: 0.1828 - val_accuracy: 0.9600\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3657 - accuracy: 0.8829 - val_loss: 0.1776 - val_accuracy: 0.9610\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3566 - accuracy: 0.8810 - val_loss: 0.1754 - val_accuracy: 0.9610\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3537 - accuracy: 0.8881 - val_loss: 0.1775 - val_accuracy: 0.9571\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3391 - accuracy: 0.8917 - val_loss: 0.1819 - val_accuracy: 0.9562\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3496 - accuracy: 0.8874 - val_loss: 0.1846 - val_accuracy: 0.9543\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3340 - accuracy: 0.8948 - val_loss: 0.1788 - val_accuracy: 0.9571\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3505 - accuracy: 0.8910 - val_loss: 0.1711 - val_accuracy: 0.9600\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3335 - accuracy: 0.8981 - val_loss: 0.1683 - val_accuracy: 0.9600\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3400 - accuracy: 0.8938 - val_loss: 0.1698 - val_accuracy: 0.9600\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3514 - accuracy: 0.8929 - val_loss: 0.1707 - val_accuracy: 0.9600\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3430 - accuracy: 0.8862 - val_loss: 0.1702 - val_accuracy: 0.9629\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3285 - accuracy: 0.8940 - val_loss: 0.1674 - val_accuracy: 0.9638\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.3148 - accuracy: 0.9021 - val_loss: 0.1649 - val_accuracy: 0.9657\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3130 - accuracy: 0.9033 - val_loss: 0.1627 - val_accuracy: 0.9657\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3219 - accuracy: 0.8964 - val_loss: 0.1624 - val_accuracy: 0.9657\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3054 - accuracy: 0.9067 - val_loss: 0.1711 - val_accuracy: 0.9638\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3101 - accuracy: 0.9045 - val_loss: 0.1832 - val_accuracy: 0.9581\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3240 - accuracy: 0.8995 - val_loss: 0.1888 - val_accuracy: 0.9533\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3140 - accuracy: 0.9040 - val_loss: 0.1840 - val_accuracy: 0.9562\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2964 - accuracy: 0.9000 - val_loss: 0.1716 - val_accuracy: 0.9581\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.3190 - accuracy: 0.8931 - val_loss: 0.1571 - val_accuracy: 0.9629\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3032 - accuracy: 0.9055 - val_loss: 0.1468 - val_accuracy: 0.9657\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3195 - accuracy: 0.8983 - val_loss: 0.1415 - val_accuracy: 0.9667\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2996 - accuracy: 0.9076 - val_loss: 0.1429 - val_accuracy: 0.9657\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2990 - accuracy: 0.9062 - val_loss: 0.1466 - val_accuracy: 0.9648\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3038 - accuracy: 0.8952 - val_loss: 0.1509 - val_accuracy: 0.9638\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2868 - accuracy: 0.9114 - val_loss: 0.1541 - val_accuracy: 0.9629\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2925 - accuracy: 0.9038 - val_loss: 0.1531 - val_accuracy: 0.9638\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2925 - accuracy: 0.9081 - val_loss: 0.1505 - val_accuracy: 0.9648\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2941 - accuracy: 0.9043 - val_loss: 0.1470 - val_accuracy: 0.9648\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2862 - accuracy: 0.9093 - val_loss: 0.1423 - val_accuracy: 0.9648\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2719 - accuracy: 0.9131 - val_loss: 0.1372 - val_accuracy: 0.9657\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2819 - accuracy: 0.9083 - val_loss: 0.1345 - val_accuracy: 0.9686\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2812 - accuracy: 0.9114 - val_loss: 0.1351 - val_accuracy: 0.9686\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2812 - accuracy: 0.9095 - val_loss: 0.1374 - val_accuracy: 0.9657\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2755 - accuracy: 0.9155 - val_loss: 0.1399 - val_accuracy: 0.9657\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2805 - accuracy: 0.9088 - val_loss: 0.1399 - val_accuracy: 0.9648\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2748 - accuracy: 0.9124 - val_loss: 0.1383 - val_accuracy: 0.9648\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2753 - accuracy: 0.9121 - val_loss: 0.1356 - val_accuracy: 0.9648\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2711 - accuracy: 0.9143 - val_loss: 0.1297 - val_accuracy: 0.9686\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2735 - accuracy: 0.9133 - val_loss: 0.1253 - val_accuracy: 0.9714\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2686 - accuracy: 0.9138 - val_loss: 0.1240 - val_accuracy: 0.9733\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2678 - accuracy: 0.9145 - val_loss: 0.1255 - val_accuracy: 0.9714\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2669 - accuracy: 0.9148 - val_loss: 0.1257 - val_accuracy: 0.9714\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2718 - accuracy: 0.9112 - val_loss: 0.1259 - val_accuracy: 0.9714\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2664 - accuracy: 0.9150 - val_loss: 0.1238 - val_accuracy: 0.9724\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2700 - accuracy: 0.9169 - val_loss: 0.1199 - val_accuracy: 0.9724\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2580 - accuracy: 0.9174 - val_loss: 0.1158 - val_accuracy: 0.9743\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2676 - accuracy: 0.9114 - val_loss: 0.1130 - val_accuracy: 0.9743\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2663 - accuracy: 0.9136 - val_loss: 0.1134 - val_accuracy: 0.9743\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2548 - accuracy: 0.9233 - val_loss: 0.1173 - val_accuracy: 0.9733\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2547 - accuracy: 0.9136 - val_loss: 0.1196 - val_accuracy: 0.9714\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2608 - accuracy: 0.9145 - val_loss: 0.1204 - val_accuracy: 0.9705\n",
            "Epoch 173/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2669 - accuracy: 0.9160 - val_loss: 0.1197 - val_accuracy: 0.9695\n",
            "Epoch 174/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2512 - accuracy: 0.9221 - val_loss: 0.1189 - val_accuracy: 0.9676\n",
            "Epoch 175/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2433 - accuracy: 0.9198 - val_loss: 0.1175 - val_accuracy: 0.9686\n",
            "Epoch 176/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2468 - accuracy: 0.9243 - val_loss: 0.1133 - val_accuracy: 0.9705\n",
            "Epoch 177/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2551 - accuracy: 0.9181 - val_loss: 0.1097 - val_accuracy: 0.9714\n",
            "Epoch 178/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2478 - accuracy: 0.9200 - val_loss: 0.1066 - val_accuracy: 0.9714\n",
            "Epoch 179/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2454 - accuracy: 0.9221 - val_loss: 0.1048 - val_accuracy: 0.9714\n",
            "Epoch 180/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2508 - accuracy: 0.9181 - val_loss: 0.1053 - val_accuracy: 0.9724\n",
            "Epoch 181/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2584 - accuracy: 0.9138 - val_loss: 0.1089 - val_accuracy: 0.9714\n",
            "Epoch 182/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2474 - accuracy: 0.9217 - val_loss: 0.1123 - val_accuracy: 0.9724\n",
            "Epoch 183/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2425 - accuracy: 0.9240 - val_loss: 0.1104 - val_accuracy: 0.9724\n",
            "Epoch 184/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2416 - accuracy: 0.9262 - val_loss: 0.1070 - val_accuracy: 0.9724\n",
            "Epoch 185/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2490 - accuracy: 0.9188 - val_loss: 0.1057 - val_accuracy: 0.9724\n",
            "Epoch 186/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2517 - accuracy: 0.9174 - val_loss: 0.1034 - val_accuracy: 0.9724\n",
            "Epoch 187/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2372 - accuracy: 0.9210 - val_loss: 0.1000 - val_accuracy: 0.9724\n",
            "Epoch 188/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2473 - accuracy: 0.9198 - val_loss: 0.0964 - val_accuracy: 0.9724\n",
            "Epoch 189/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2318 - accuracy: 0.9260 - val_loss: 0.0969 - val_accuracy: 0.9724\n",
            "Epoch 190/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2274 - accuracy: 0.9248 - val_loss: 0.0986 - val_accuracy: 0.9724\n",
            "Epoch 191/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2366 - accuracy: 0.9231 - val_loss: 0.1023 - val_accuracy: 0.9724\n",
            "Epoch 192/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2400 - accuracy: 0.9214 - val_loss: 0.1057 - val_accuracy: 0.9724\n",
            "Epoch 193/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2421 - accuracy: 0.9229 - val_loss: 0.1086 - val_accuracy: 0.9714\n",
            "Epoch 194/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2331 - accuracy: 0.9248 - val_loss: 0.1110 - val_accuracy: 0.9714\n",
            "Epoch 195/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2422 - accuracy: 0.9257 - val_loss: 0.1127 - val_accuracy: 0.9714\n",
            "Epoch 196/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2360 - accuracy: 0.9236 - val_loss: 0.1119 - val_accuracy: 0.9724\n",
            "Epoch 197/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2264 - accuracy: 0.9248 - val_loss: 0.1097 - val_accuracy: 0.9714\n",
            "Epoch 198/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2246 - accuracy: 0.9286 - val_loss: 0.1091 - val_accuracy: 0.9724\n",
            "Epoch 199/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2338 - accuracy: 0.9255 - val_loss: 0.1070 - val_accuracy: 0.9724\n",
            "Epoch 200/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2380 - accuracy: 0.9221 - val_loss: 0.1039 - val_accuracy: 0.9724\n",
            "Epoch 201/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2224 - accuracy: 0.9264 - val_loss: 0.1007 - val_accuracy: 0.9733\n",
            "Epoch 202/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2388 - accuracy: 0.9245 - val_loss: 0.0983 - val_accuracy: 0.9743\n",
            "Epoch 203/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2215 - accuracy: 0.9305 - val_loss: 0.0977 - val_accuracy: 0.9752\n",
            "Epoch 204/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2157 - accuracy: 0.9343 - val_loss: 0.0957 - val_accuracy: 0.9762\n",
            "Epoch 205/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2231 - accuracy: 0.9250 - val_loss: 0.0937 - val_accuracy: 0.9752\n",
            "Epoch 206/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2280 - accuracy: 0.9252 - val_loss: 0.0908 - val_accuracy: 0.9743\n",
            "Epoch 207/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2246 - accuracy: 0.9305 - val_loss: 0.0884 - val_accuracy: 0.9743\n",
            "Epoch 208/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2202 - accuracy: 0.9252 - val_loss: 0.0875 - val_accuracy: 0.9733\n",
            "Epoch 209/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2168 - accuracy: 0.9310 - val_loss: 0.0857 - val_accuracy: 0.9733\n",
            "Epoch 210/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2293 - accuracy: 0.9310 - val_loss: 0.0840 - val_accuracy: 0.9743\n",
            "Epoch 211/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2174 - accuracy: 0.9293 - val_loss: 0.0855 - val_accuracy: 0.9743\n",
            "Epoch 212/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2227 - accuracy: 0.9293 - val_loss: 0.0893 - val_accuracy: 0.9743\n",
            "Epoch 213/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2164 - accuracy: 0.9324 - val_loss: 0.0910 - val_accuracy: 0.9743\n",
            "Epoch 214/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2095 - accuracy: 0.9364 - val_loss: 0.0914 - val_accuracy: 0.9743\n",
            "Epoch 215/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2167 - accuracy: 0.9262 - val_loss: 0.0929 - val_accuracy: 0.9743\n",
            "Epoch 216/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2135 - accuracy: 0.9295 - val_loss: 0.0928 - val_accuracy: 0.9733\n",
            "Epoch 217/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2125 - accuracy: 0.9321 - val_loss: 0.0894 - val_accuracy: 0.9733\n",
            "Epoch 218/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1998 - accuracy: 0.9357 - val_loss: 0.0835 - val_accuracy: 0.9743\n",
            "Epoch 219/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2132 - accuracy: 0.9319 - val_loss: 0.0800 - val_accuracy: 0.9752\n",
            "Epoch 220/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2199 - accuracy: 0.9324 - val_loss: 0.0785 - val_accuracy: 0.9752\n",
            "Epoch 221/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2071 - accuracy: 0.9343 - val_loss: 0.0797 - val_accuracy: 0.9752\n",
            "Epoch 222/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2075 - accuracy: 0.9340 - val_loss: 0.0819 - val_accuracy: 0.9743\n",
            "Epoch 223/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2172 - accuracy: 0.9314 - val_loss: 0.0854 - val_accuracy: 0.9743\n",
            "Epoch 224/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2165 - accuracy: 0.9317 - val_loss: 0.0886 - val_accuracy: 0.9733\n",
            "Epoch 225/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2151 - accuracy: 0.9300 - val_loss: 0.0919 - val_accuracy: 0.9724\n",
            "Epoch 226/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2084 - accuracy: 0.9326 - val_loss: 0.0948 - val_accuracy: 0.9714\n",
            "Epoch 227/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.2060 - accuracy: 0.9329 - val_loss: 0.0917 - val_accuracy: 0.9714\n",
            "Epoch 228/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1991 - accuracy: 0.9357 - val_loss: 0.0869 - val_accuracy: 0.9724\n",
            "Epoch 229/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2118 - accuracy: 0.9305 - val_loss: 0.0836 - val_accuracy: 0.9733\n",
            "Epoch 230/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2038 - accuracy: 0.9345 - val_loss: 0.0809 - val_accuracy: 0.9743\n",
            "Epoch 231/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2071 - accuracy: 0.9355 - val_loss: 0.0815 - val_accuracy: 0.9752\n",
            "Epoch 232/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1977 - accuracy: 0.9352 - val_loss: 0.0839 - val_accuracy: 0.9752\n",
            "Epoch 233/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2043 - accuracy: 0.9379 - val_loss: 0.0858 - val_accuracy: 0.9743\n",
            "Epoch 234/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2063 - accuracy: 0.9374 - val_loss: 0.0855 - val_accuracy: 0.9743\n",
            "Epoch 235/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1960 - accuracy: 0.9390 - val_loss: 0.0825 - val_accuracy: 0.9743\n",
            "Epoch 236/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1980 - accuracy: 0.9371 - val_loss: 0.0805 - val_accuracy: 0.9752\n",
            "Epoch 237/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1869 - accuracy: 0.9405 - val_loss: 0.0785 - val_accuracy: 0.9752\n",
            "Epoch 238/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2038 - accuracy: 0.9338 - val_loss: 0.0758 - val_accuracy: 0.9752\n",
            "Epoch 239/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1982 - accuracy: 0.9360 - val_loss: 0.0741 - val_accuracy: 0.9762\n",
            "Epoch 240/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1931 - accuracy: 0.9419 - val_loss: 0.0733 - val_accuracy: 0.9771\n",
            "Epoch 241/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1966 - accuracy: 0.9376 - val_loss: 0.0740 - val_accuracy: 0.9771\n",
            "Epoch 242/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2057 - accuracy: 0.9369 - val_loss: 0.0739 - val_accuracy: 0.9771\n",
            "Epoch 243/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1873 - accuracy: 0.9386 - val_loss: 0.0744 - val_accuracy: 0.9771\n",
            "Epoch 244/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1896 - accuracy: 0.9400 - val_loss: 0.0747 - val_accuracy: 0.9762\n",
            "Epoch 245/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1918 - accuracy: 0.9407 - val_loss: 0.0750 - val_accuracy: 0.9762\n",
            "Epoch 246/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1922 - accuracy: 0.9371 - val_loss: 0.0754 - val_accuracy: 0.9762\n",
            "Epoch 247/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1995 - accuracy: 0.9355 - val_loss: 0.0744 - val_accuracy: 0.9762\n",
            "Epoch 248/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1922 - accuracy: 0.9383 - val_loss: 0.0727 - val_accuracy: 0.9771\n",
            "Epoch 249/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1953 - accuracy: 0.9362 - val_loss: 0.0685 - val_accuracy: 0.9771\n",
            "Epoch 250/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1964 - accuracy: 0.9398 - val_loss: 0.0639 - val_accuracy: 0.9790\n",
            "Epoch 251/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1946 - accuracy: 0.9383 - val_loss: 0.0617 - val_accuracy: 0.9829\n",
            "Epoch 252/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1889 - accuracy: 0.9410 - val_loss: 0.0628 - val_accuracy: 0.9829\n",
            "Epoch 253/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1845 - accuracy: 0.9436 - val_loss: 0.0664 - val_accuracy: 0.9800\n",
            "Epoch 254/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1897 - accuracy: 0.9390 - val_loss: 0.0714 - val_accuracy: 0.9781\n",
            "Epoch 255/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1872 - accuracy: 0.9386 - val_loss: 0.0762 - val_accuracy: 0.9781\n",
            "Epoch 256/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1984 - accuracy: 0.9355 - val_loss: 0.0763 - val_accuracy: 0.9781\n",
            "Epoch 257/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1899 - accuracy: 0.9390 - val_loss: 0.0748 - val_accuracy: 0.9800\n",
            "Epoch 258/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1799 - accuracy: 0.9421 - val_loss: 0.0713 - val_accuracy: 0.9781\n",
            "Epoch 259/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1817 - accuracy: 0.9402 - val_loss: 0.0681 - val_accuracy: 0.9800\n",
            "Epoch 260/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1738 - accuracy: 0.9440 - val_loss: 0.0659 - val_accuracy: 0.9800\n",
            "Epoch 261/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1858 - accuracy: 0.9393 - val_loss: 0.0647 - val_accuracy: 0.9800\n",
            "Epoch 262/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1806 - accuracy: 0.9400 - val_loss: 0.0645 - val_accuracy: 0.9810\n",
            "Epoch 263/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1765 - accuracy: 0.9419 - val_loss: 0.0648 - val_accuracy: 0.9810\n",
            "Epoch 264/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1830 - accuracy: 0.9421 - val_loss: 0.0666 - val_accuracy: 0.9781\n",
            "Epoch 265/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1828 - accuracy: 0.9433 - val_loss: 0.0672 - val_accuracy: 0.9781\n",
            "Epoch 266/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1837 - accuracy: 0.9417 - val_loss: 0.0668 - val_accuracy: 0.9781\n",
            "Epoch 267/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1740 - accuracy: 0.9455 - val_loss: 0.0651 - val_accuracy: 0.9790\n",
            "Epoch 268/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1798 - accuracy: 0.9421 - val_loss: 0.0647 - val_accuracy: 0.9790\n",
            "Epoch 269/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1798 - accuracy: 0.9460 - val_loss: 0.0651 - val_accuracy: 0.9781\n",
            "Epoch 270/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1746 - accuracy: 0.9460 - val_loss: 0.0642 - val_accuracy: 0.9790\n",
            "Epoch 271/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1768 - accuracy: 0.9462 - val_loss: 0.0620 - val_accuracy: 0.9810\n",
            "Epoch 272/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1738 - accuracy: 0.9443 - val_loss: 0.0597 - val_accuracy: 0.9848\n",
            "Epoch 273/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1807 - accuracy: 0.9457 - val_loss: 0.0579 - val_accuracy: 0.9857\n",
            "Epoch 274/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1843 - accuracy: 0.9412 - val_loss: 0.0593 - val_accuracy: 0.9857\n",
            "Epoch 275/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1701 - accuracy: 0.9448 - val_loss: 0.0617 - val_accuracy: 0.9848\n",
            "Epoch 276/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1832 - accuracy: 0.9440 - val_loss: 0.0633 - val_accuracy: 0.9829\n",
            "Epoch 277/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1729 - accuracy: 0.9476 - val_loss: 0.0649 - val_accuracy: 0.9829\n",
            "Epoch 278/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1705 - accuracy: 0.9448 - val_loss: 0.0658 - val_accuracy: 0.9829\n",
            "Epoch 279/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1804 - accuracy: 0.9445 - val_loss: 0.0664 - val_accuracy: 0.9819\n",
            "Epoch 280/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1636 - accuracy: 0.9498 - val_loss: 0.0654 - val_accuracy: 0.9829\n",
            "Epoch 281/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1714 - accuracy: 0.9493 - val_loss: 0.0646 - val_accuracy: 0.9838\n",
            "Epoch 282/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1689 - accuracy: 0.9431 - val_loss: 0.0630 - val_accuracy: 0.9838\n",
            "Epoch 283/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1733 - accuracy: 0.9426 - val_loss: 0.0613 - val_accuracy: 0.9838\n",
            "Epoch 284/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1696 - accuracy: 0.9464 - val_loss: 0.0605 - val_accuracy: 0.9829\n",
            "Epoch 285/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1731 - accuracy: 0.9450 - val_loss: 0.0603 - val_accuracy: 0.9829\n",
            "Epoch 286/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1713 - accuracy: 0.9443 - val_loss: 0.0614 - val_accuracy: 0.9838\n",
            "Epoch 287/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1746 - accuracy: 0.9440 - val_loss: 0.0639 - val_accuracy: 0.9829\n",
            "Epoch 288/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1744 - accuracy: 0.9460 - val_loss: 0.0674 - val_accuracy: 0.9810\n",
            "Epoch 289/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1785 - accuracy: 0.9402 - val_loss: 0.0683 - val_accuracy: 0.9810\n",
            "Epoch 290/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1637 - accuracy: 0.9455 - val_loss: 0.0666 - val_accuracy: 0.9819\n",
            "Epoch 291/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1554 - accuracy: 0.9548 - val_loss: 0.0635 - val_accuracy: 0.9829\n",
            "Epoch 292/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1680 - accuracy: 0.9445 - val_loss: 0.0610 - val_accuracy: 0.9829\n",
            "Epoch 293/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1532 - accuracy: 0.9495 - val_loss: 0.0601 - val_accuracy: 0.9829\n",
            "Epoch 294/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1572 - accuracy: 0.9495 - val_loss: 0.0591 - val_accuracy: 0.9829\n",
            "Epoch 295/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1698 - accuracy: 0.9440 - val_loss: 0.0583 - val_accuracy: 0.9838\n",
            "Epoch 296/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1704 - accuracy: 0.9431 - val_loss: 0.0589 - val_accuracy: 0.9838\n",
            "Epoch 297/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1658 - accuracy: 0.9455 - val_loss: 0.0582 - val_accuracy: 0.9838\n",
            "Epoch 298/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1652 - accuracy: 0.9467 - val_loss: 0.0570 - val_accuracy: 0.9838\n",
            "Epoch 299/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1787 - accuracy: 0.9440 - val_loss: 0.0567 - val_accuracy: 0.9848\n",
            "Epoch 300/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1550 - accuracy: 0.9514 - val_loss: 0.0567 - val_accuracy: 0.9848\n",
            "Epoch 301/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1648 - accuracy: 0.9493 - val_loss: 0.0566 - val_accuracy: 0.9848\n",
            "Epoch 302/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1674 - accuracy: 0.9440 - val_loss: 0.0575 - val_accuracy: 0.9848\n",
            "Epoch 303/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1620 - accuracy: 0.9493 - val_loss: 0.0594 - val_accuracy: 0.9829\n",
            "Epoch 304/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1692 - accuracy: 0.9452 - val_loss: 0.0611 - val_accuracy: 0.9829\n",
            "Epoch 305/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1669 - accuracy: 0.9438 - val_loss: 0.0639 - val_accuracy: 0.9838\n",
            "Epoch 306/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1615 - accuracy: 0.9493 - val_loss: 0.0631 - val_accuracy: 0.9829\n",
            "Epoch 307/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1718 - accuracy: 0.9457 - val_loss: 0.0611 - val_accuracy: 0.9838\n",
            "Epoch 308/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1566 - accuracy: 0.9490 - val_loss: 0.0601 - val_accuracy: 0.9838\n",
            "Epoch 309/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1546 - accuracy: 0.9533 - val_loss: 0.0585 - val_accuracy: 0.9838\n",
            "Epoch 310/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1561 - accuracy: 0.9490 - val_loss: 0.0575 - val_accuracy: 0.9848\n",
            "Epoch 311/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1592 - accuracy: 0.9490 - val_loss: 0.0567 - val_accuracy: 0.9848\n",
            "Epoch 312/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1600 - accuracy: 0.9490 - val_loss: 0.0565 - val_accuracy: 0.9848\n",
            "Epoch 313/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1663 - accuracy: 0.9495 - val_loss: 0.0567 - val_accuracy: 0.9848\n",
            "Epoch 314/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1529 - accuracy: 0.9495 - val_loss: 0.0568 - val_accuracy: 0.9848\n",
            "Epoch 315/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1654 - accuracy: 0.9452 - val_loss: 0.0575 - val_accuracy: 0.9838\n",
            "Epoch 316/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1551 - accuracy: 0.9507 - val_loss: 0.0583 - val_accuracy: 0.9838\n",
            "Epoch 317/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1613 - accuracy: 0.9495 - val_loss: 0.0578 - val_accuracy: 0.9848\n",
            "Epoch 318/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1528 - accuracy: 0.9519 - val_loss: 0.0564 - val_accuracy: 0.9848\n",
            "Epoch 319/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1622 - accuracy: 0.9464 - val_loss: 0.0555 - val_accuracy: 0.9848\n",
            "Epoch 320/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1526 - accuracy: 0.9521 - val_loss: 0.0547 - val_accuracy: 0.9838\n",
            "Epoch 321/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1606 - accuracy: 0.9479 - val_loss: 0.0541 - val_accuracy: 0.9848\n",
            "Epoch 322/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1604 - accuracy: 0.9500 - val_loss: 0.0540 - val_accuracy: 0.9848\n",
            "Epoch 323/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1496 - accuracy: 0.9519 - val_loss: 0.0544 - val_accuracy: 0.9857\n",
            "Epoch 324/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1639 - accuracy: 0.9483 - val_loss: 0.0560 - val_accuracy: 0.9857\n",
            "Epoch 325/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1494 - accuracy: 0.9540 - val_loss: 0.0578 - val_accuracy: 0.9838\n",
            "Epoch 326/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1586 - accuracy: 0.9486 - val_loss: 0.0587 - val_accuracy: 0.9829\n",
            "Epoch 327/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1561 - accuracy: 0.9500 - val_loss: 0.0584 - val_accuracy: 0.9838\n",
            "Epoch 328/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1496 - accuracy: 0.9507 - val_loss: 0.0584 - val_accuracy: 0.9848\n",
            "Epoch 329/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1516 - accuracy: 0.9505 - val_loss: 0.0574 - val_accuracy: 0.9848\n",
            "Epoch 330/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1530 - accuracy: 0.9505 - val_loss: 0.0551 - val_accuracy: 0.9848\n",
            "Epoch 331/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1496 - accuracy: 0.9507 - val_loss: 0.0525 - val_accuracy: 0.9848\n",
            "Epoch 332/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1490 - accuracy: 0.9531 - val_loss: 0.0517 - val_accuracy: 0.9848\n",
            "Epoch 333/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1572 - accuracy: 0.9502 - val_loss: 0.0519 - val_accuracy: 0.9848\n",
            "Epoch 334/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1556 - accuracy: 0.9510 - val_loss: 0.0514 - val_accuracy: 0.9848\n",
            "Epoch 335/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1504 - accuracy: 0.9540 - val_loss: 0.0523 - val_accuracy: 0.9848\n",
            "Epoch 336/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1521 - accuracy: 0.9490 - val_loss: 0.0530 - val_accuracy: 0.9848\n",
            "Epoch 337/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1515 - accuracy: 0.9552 - val_loss: 0.0522 - val_accuracy: 0.9848\n",
            "Epoch 338/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1553 - accuracy: 0.9486 - val_loss: 0.0510 - val_accuracy: 0.9848\n",
            "Epoch 339/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1464 - accuracy: 0.9543 - val_loss: 0.0498 - val_accuracy: 0.9848\n",
            "Epoch 340/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1480 - accuracy: 0.9514 - val_loss: 0.0498 - val_accuracy: 0.9857\n",
            "Epoch 341/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1486 - accuracy: 0.9495 - val_loss: 0.0508 - val_accuracy: 0.9857\n",
            "Epoch 342/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1498 - accuracy: 0.9510 - val_loss: 0.0514 - val_accuracy: 0.9857\n",
            "Epoch 343/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1461 - accuracy: 0.9533 - val_loss: 0.0498 - val_accuracy: 0.9867\n",
            "Epoch 344/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1482 - accuracy: 0.9493 - val_loss: 0.0486 - val_accuracy: 0.9867\n",
            "Epoch 345/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1477 - accuracy: 0.9531 - val_loss: 0.0487 - val_accuracy: 0.9867\n",
            "Epoch 346/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1494 - accuracy: 0.9512 - val_loss: 0.0510 - val_accuracy: 0.9867\n",
            "Epoch 347/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1465 - accuracy: 0.9538 - val_loss: 0.0540 - val_accuracy: 0.9857\n",
            "Epoch 348/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1535 - accuracy: 0.9519 - val_loss: 0.0561 - val_accuracy: 0.9857\n",
            "Epoch 349/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1531 - accuracy: 0.9533 - val_loss: 0.0559 - val_accuracy: 0.9848\n",
            "Epoch 350/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1434 - accuracy: 0.9517 - val_loss: 0.0530 - val_accuracy: 0.9848\n",
            "Epoch 351/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1477 - accuracy: 0.9531 - val_loss: 0.0508 - val_accuracy: 0.9848\n",
            "Epoch 352/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1490 - accuracy: 0.9533 - val_loss: 0.0495 - val_accuracy: 0.9848\n",
            "Epoch 353/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1502 - accuracy: 0.9514 - val_loss: 0.0484 - val_accuracy: 0.9848\n",
            "Epoch 354/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1496 - accuracy: 0.9555 - val_loss: 0.0466 - val_accuracy: 0.9857\n",
            "Epoch 355/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1567 - accuracy: 0.9524 - val_loss: 0.0454 - val_accuracy: 0.9867\n",
            "Epoch 356/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1612 - accuracy: 0.9481 - val_loss: 0.0461 - val_accuracy: 0.9867\n",
            "Epoch 357/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1442 - accuracy: 0.9552 - val_loss: 0.0475 - val_accuracy: 0.9867\n",
            "Epoch 358/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1536 - accuracy: 0.9533 - val_loss: 0.0480 - val_accuracy: 0.9867\n",
            "Epoch 359/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1468 - accuracy: 0.9536 - val_loss: 0.0476 - val_accuracy: 0.9867\n",
            "Epoch 360/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1542 - accuracy: 0.9526 - val_loss: 0.0479 - val_accuracy: 0.9867\n",
            "Epoch 361/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1396 - accuracy: 0.9564 - val_loss: 0.0492 - val_accuracy: 0.9867\n",
            "Epoch 362/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1483 - accuracy: 0.9502 - val_loss: 0.0501 - val_accuracy: 0.9867\n",
            "Epoch 363/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1440 - accuracy: 0.9512 - val_loss: 0.0518 - val_accuracy: 0.9867\n",
            "Epoch 364/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1366 - accuracy: 0.9555 - val_loss: 0.0520 - val_accuracy: 0.9867\n",
            "Epoch 365/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1356 - accuracy: 0.9560 - val_loss: 0.0507 - val_accuracy: 0.9867\n",
            "Epoch 366/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1424 - accuracy: 0.9500 - val_loss: 0.0489 - val_accuracy: 0.9867\n",
            "Epoch 367/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1374 - accuracy: 0.9581 - val_loss: 0.0467 - val_accuracy: 0.9867\n",
            "Epoch 368/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1501 - accuracy: 0.9498 - val_loss: 0.0460 - val_accuracy: 0.9876\n",
            "Epoch 369/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1378 - accuracy: 0.9564 - val_loss: 0.0454 - val_accuracy: 0.9876\n",
            "Epoch 370/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1390 - accuracy: 0.9593 - val_loss: 0.0449 - val_accuracy: 0.9876\n",
            "Epoch 371/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1351 - accuracy: 0.9590 - val_loss: 0.0451 - val_accuracy: 0.9876\n",
            "Epoch 372/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1326 - accuracy: 0.9569 - val_loss: 0.0460 - val_accuracy: 0.9867\n",
            "Epoch 373/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1358 - accuracy: 0.9550 - val_loss: 0.0475 - val_accuracy: 0.9867\n",
            "Epoch 374/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1456 - accuracy: 0.9545 - val_loss: 0.0495 - val_accuracy: 0.9857\n",
            "Epoch 375/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1405 - accuracy: 0.9557 - val_loss: 0.0501 - val_accuracy: 0.9857\n",
            "Epoch 376/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1462 - accuracy: 0.9507 - val_loss: 0.0509 - val_accuracy: 0.9857\n",
            "Epoch 377/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1404 - accuracy: 0.9560 - val_loss: 0.0513 - val_accuracy: 0.9857\n",
            "Epoch 378/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1498 - accuracy: 0.9533 - val_loss: 0.0499 - val_accuracy: 0.9867\n",
            "Epoch 379/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1396 - accuracy: 0.9533 - val_loss: 0.0477 - val_accuracy: 0.9876\n",
            "Epoch 380/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1422 - accuracy: 0.9560 - val_loss: 0.0458 - val_accuracy: 0.9876\n",
            "Epoch 381/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1484 - accuracy: 0.9531 - val_loss: 0.0440 - val_accuracy: 0.9876\n",
            "Epoch 382/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1366 - accuracy: 0.9581 - val_loss: 0.0433 - val_accuracy: 0.9876\n",
            "Epoch 383/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1340 - accuracy: 0.9576 - val_loss: 0.0427 - val_accuracy: 0.9876\n",
            "Epoch 384/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1464 - accuracy: 0.9550 - val_loss: 0.0429 - val_accuracy: 0.9876\n",
            "Epoch 385/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1417 - accuracy: 0.9562 - val_loss: 0.0441 - val_accuracy: 0.9876\n",
            "Epoch 386/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1369 - accuracy: 0.9560 - val_loss: 0.0449 - val_accuracy: 0.9876\n",
            "Epoch 387/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1430 - accuracy: 0.9560 - val_loss: 0.0452 - val_accuracy: 0.9876\n",
            "Epoch 388/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1367 - accuracy: 0.9583 - val_loss: 0.0444 - val_accuracy: 0.9876\n",
            "Epoch 389/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1357 - accuracy: 0.9538 - val_loss: 0.0437 - val_accuracy: 0.9876\n",
            "Epoch 390/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1381 - accuracy: 0.9574 - val_loss: 0.0431 - val_accuracy: 0.9876\n",
            "Epoch 391/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1426 - accuracy: 0.9543 - val_loss: 0.0433 - val_accuracy: 0.9876\n",
            "Epoch 392/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1428 - accuracy: 0.9529 - val_loss: 0.0439 - val_accuracy: 0.9876\n",
            "Epoch 393/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1380 - accuracy: 0.9564 - val_loss: 0.0435 - val_accuracy: 0.9867\n",
            "Epoch 394/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1340 - accuracy: 0.9552 - val_loss: 0.0431 - val_accuracy: 0.9876\n",
            "Epoch 395/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1478 - accuracy: 0.9548 - val_loss: 0.0446 - val_accuracy: 0.9876\n",
            "Epoch 396/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1386 - accuracy: 0.9548 - val_loss: 0.0461 - val_accuracy: 0.9876\n",
            "Epoch 397/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1417 - accuracy: 0.9536 - val_loss: 0.0456 - val_accuracy: 0.9876\n",
            "Epoch 398/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1453 - accuracy: 0.9531 - val_loss: 0.0451 - val_accuracy: 0.9876\n",
            "Epoch 399/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1372 - accuracy: 0.9581 - val_loss: 0.0448 - val_accuracy: 0.9876\n",
            "Epoch 400/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1330 - accuracy: 0.9600 - val_loss: 0.0444 - val_accuracy: 0.9876\n",
            "Epoch 401/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1384 - accuracy: 0.9545 - val_loss: 0.0432 - val_accuracy: 0.9876\n",
            "Epoch 402/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1386 - accuracy: 0.9557 - val_loss: 0.0431 - val_accuracy: 0.9876\n",
            "Epoch 403/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1285 - accuracy: 0.9571 - val_loss: 0.0435 - val_accuracy: 0.9848\n",
            "Epoch 404/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1383 - accuracy: 0.9564 - val_loss: 0.0447 - val_accuracy: 0.9848\n",
            "Epoch 405/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1422 - accuracy: 0.9579 - val_loss: 0.0468 - val_accuracy: 0.9848\n",
            "Epoch 406/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1390 - accuracy: 0.9545 - val_loss: 0.0462 - val_accuracy: 0.9848\n",
            "Epoch 407/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1441 - accuracy: 0.9576 - val_loss: 0.0447 - val_accuracy: 0.9857\n",
            "Epoch 408/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1354 - accuracy: 0.9555 - val_loss: 0.0438 - val_accuracy: 0.9867\n",
            "Epoch 409/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1333 - accuracy: 0.9600 - val_loss: 0.0424 - val_accuracy: 0.9867\n",
            "Epoch 410/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1280 - accuracy: 0.9607 - val_loss: 0.0406 - val_accuracy: 0.9876\n",
            "Epoch 411/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1322 - accuracy: 0.9574 - val_loss: 0.0393 - val_accuracy: 0.9886\n",
            "Epoch 412/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1360 - accuracy: 0.9548 - val_loss: 0.0388 - val_accuracy: 0.9876\n",
            "Epoch 413/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1390 - accuracy: 0.9526 - val_loss: 0.0389 - val_accuracy: 0.9876\n",
            "Epoch 414/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1344 - accuracy: 0.9552 - val_loss: 0.0400 - val_accuracy: 0.9876\n",
            "Epoch 415/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1367 - accuracy: 0.9588 - val_loss: 0.0400 - val_accuracy: 0.9876\n",
            "Epoch 416/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1257 - accuracy: 0.9588 - val_loss: 0.0398 - val_accuracy: 0.9876\n",
            "Epoch 417/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1316 - accuracy: 0.9576 - val_loss: 0.0398 - val_accuracy: 0.9886\n",
            "Epoch 418/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1286 - accuracy: 0.9538 - val_loss: 0.0396 - val_accuracy: 0.9886\n",
            "Epoch 419/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1292 - accuracy: 0.9590 - val_loss: 0.0396 - val_accuracy: 0.9895\n",
            "Epoch 420/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1341 - accuracy: 0.9569 - val_loss: 0.0401 - val_accuracy: 0.9895\n",
            "Epoch 421/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1305 - accuracy: 0.9588 - val_loss: 0.0415 - val_accuracy: 0.9876\n",
            "Epoch 422/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1347 - accuracy: 0.9567 - val_loss: 0.0430 - val_accuracy: 0.9867\n",
            "Epoch 423/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1335 - accuracy: 0.9588 - val_loss: 0.0427 - val_accuracy: 0.9867\n",
            "Epoch 424/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1351 - accuracy: 0.9574 - val_loss: 0.0417 - val_accuracy: 0.9876\n",
            "Epoch 425/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1412 - accuracy: 0.9550 - val_loss: 0.0406 - val_accuracy: 0.9876\n",
            "Epoch 426/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1275 - accuracy: 0.9614 - val_loss: 0.0400 - val_accuracy: 0.9876\n",
            "Epoch 427/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1255 - accuracy: 0.9602 - val_loss: 0.0397 - val_accuracy: 0.9876\n",
            "Epoch 428/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1224 - accuracy: 0.9619 - val_loss: 0.0395 - val_accuracy: 0.9876\n",
            "Epoch 429/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1306 - accuracy: 0.9581 - val_loss: 0.0394 - val_accuracy: 0.9876\n",
            "Epoch 430/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1324 - accuracy: 0.9560 - val_loss: 0.0403 - val_accuracy: 0.9867\n",
            "Epoch 431/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1352 - accuracy: 0.9569 - val_loss: 0.0418 - val_accuracy: 0.9867\n",
            "Epoch 432/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1269 - accuracy: 0.9588 - val_loss: 0.0423 - val_accuracy: 0.9867\n",
            "Epoch 433/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1322 - accuracy: 0.9574 - val_loss: 0.0416 - val_accuracy: 0.9886\n",
            "Epoch 434/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1311 - accuracy: 0.9557 - val_loss: 0.0407 - val_accuracy: 0.9876\n",
            "Epoch 435/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1312 - accuracy: 0.9576 - val_loss: 0.0413 - val_accuracy: 0.9867\n",
            "Epoch 436/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1301 - accuracy: 0.9550 - val_loss: 0.0430 - val_accuracy: 0.9867\n",
            "Epoch 437/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1343 - accuracy: 0.9567 - val_loss: 0.0437 - val_accuracy: 0.9857\n",
            "Epoch 438/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1288 - accuracy: 0.9593 - val_loss: 0.0424 - val_accuracy: 0.9838\n",
            "Epoch 439/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1223 - accuracy: 0.9607 - val_loss: 0.0411 - val_accuracy: 0.9838\n",
            "Epoch 440/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1252 - accuracy: 0.9610 - val_loss: 0.0403 - val_accuracy: 0.9838\n",
            "Epoch 441/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1305 - accuracy: 0.9564 - val_loss: 0.0396 - val_accuracy: 0.9848\n",
            "Epoch 442/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1350 - accuracy: 0.9569 - val_loss: 0.0399 - val_accuracy: 0.9857\n",
            "Epoch 443/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1259 - accuracy: 0.9586 - val_loss: 0.0410 - val_accuracy: 0.9876\n",
            "Epoch 444/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1370 - accuracy: 0.9543 - val_loss: 0.0417 - val_accuracy: 0.9876\n",
            "Epoch 445/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1324 - accuracy: 0.9567 - val_loss: 0.0404 - val_accuracy: 0.9886\n",
            "Epoch 446/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1322 - accuracy: 0.9581 - val_loss: 0.0387 - val_accuracy: 0.9886\n",
            "Epoch 447/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1281 - accuracy: 0.9588 - val_loss: 0.0379 - val_accuracy: 0.9886\n",
            "Epoch 448/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1220 - accuracy: 0.9621 - val_loss: 0.0377 - val_accuracy: 0.9886\n",
            "Epoch 449/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1306 - accuracy: 0.9581 - val_loss: 0.0384 - val_accuracy: 0.9876\n",
            "Epoch 450/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1345 - accuracy: 0.9574 - val_loss: 0.0397 - val_accuracy: 0.9857\n",
            "Epoch 451/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1320 - accuracy: 0.9576 - val_loss: 0.0413 - val_accuracy: 0.9848\n",
            "Epoch 452/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1276 - accuracy: 0.9588 - val_loss: 0.0418 - val_accuracy: 0.9848\n",
            "Epoch 453/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1209 - accuracy: 0.9605 - val_loss: 0.0412 - val_accuracy: 0.9848\n",
            "Epoch 454/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1306 - accuracy: 0.9564 - val_loss: 0.0403 - val_accuracy: 0.9848\n",
            "Epoch 455/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1191 - accuracy: 0.9612 - val_loss: 0.0389 - val_accuracy: 0.9848\n",
            "Epoch 456/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1305 - accuracy: 0.9593 - val_loss: 0.0373 - val_accuracy: 0.9857\n",
            "Epoch 457/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1396 - accuracy: 0.9548 - val_loss: 0.0371 - val_accuracy: 0.9876\n",
            "Epoch 458/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1337 - accuracy: 0.9590 - val_loss: 0.0374 - val_accuracy: 0.9876\n",
            "Epoch 459/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1273 - accuracy: 0.9588 - val_loss: 0.0380 - val_accuracy: 0.9886\n",
            "Epoch 460/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1242 - accuracy: 0.9583 - val_loss: 0.0389 - val_accuracy: 0.9876\n",
            "Epoch 461/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1295 - accuracy: 0.9562 - val_loss: 0.0409 - val_accuracy: 0.9867\n",
            "Epoch 462/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1306 - accuracy: 0.9590 - val_loss: 0.0428 - val_accuracy: 0.9867\n",
            "Epoch 463/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1222 - accuracy: 0.9590 - val_loss: 0.0420 - val_accuracy: 0.9867\n",
            "Epoch 464/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1270 - accuracy: 0.9586 - val_loss: 0.0397 - val_accuracy: 0.9867\n",
            "Epoch 465/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1206 - accuracy: 0.9640 - val_loss: 0.0369 - val_accuracy: 0.9876\n",
            "Epoch 466/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1235 - accuracy: 0.9631 - val_loss: 0.0355 - val_accuracy: 0.9886\n",
            "Epoch 467/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1237 - accuracy: 0.9598 - val_loss: 0.0357 - val_accuracy: 0.9886\n",
            "Epoch 468/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1244 - accuracy: 0.9590 - val_loss: 0.0364 - val_accuracy: 0.9848\n",
            "Epoch 469/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1198 - accuracy: 0.9607 - val_loss: 0.0375 - val_accuracy: 0.9848\n",
            "Epoch 470/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1221 - accuracy: 0.9619 - val_loss: 0.0397 - val_accuracy: 0.9848\n",
            "Epoch 471/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1197 - accuracy: 0.9636 - val_loss: 0.0409 - val_accuracy: 0.9838\n",
            "Epoch 472/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1280 - accuracy: 0.9593 - val_loss: 0.0416 - val_accuracy: 0.9829\n",
            "Epoch 473/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1313 - accuracy: 0.9586 - val_loss: 0.0423 - val_accuracy: 0.9829\n",
            "Epoch 474/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1189 - accuracy: 0.9638 - val_loss: 0.0431 - val_accuracy: 0.9829\n",
            "Epoch 475/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1196 - accuracy: 0.9617 - val_loss: 0.0433 - val_accuracy: 0.9857\n",
            "Epoch 476/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1202 - accuracy: 0.9624 - val_loss: 0.0424 - val_accuracy: 0.9876\n",
            "Epoch 477/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1252 - accuracy: 0.9607 - val_loss: 0.0423 - val_accuracy: 0.9895\n",
            "Epoch 478/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1252 - accuracy: 0.9586 - val_loss: 0.0416 - val_accuracy: 0.9895\n",
            "Epoch 479/500\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1170 - accuracy: 0.9621 - val_loss: 0.0408 - val_accuracy: 0.9895\n",
            "Epoch 480/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1235 - accuracy: 0.9612 - val_loss: 0.0395 - val_accuracy: 0.9895\n",
            "Epoch 481/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1246 - accuracy: 0.9621 - val_loss: 0.0387 - val_accuracy: 0.9895\n",
            "Epoch 482/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1203 - accuracy: 0.9624 - val_loss: 0.0386 - val_accuracy: 0.9895\n",
            "Epoch 483/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1283 - accuracy: 0.9588 - val_loss: 0.0390 - val_accuracy: 0.9886\n",
            "Epoch 484/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1247 - accuracy: 0.9579 - val_loss: 0.0400 - val_accuracy: 0.9886\n",
            "Epoch 485/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1209 - accuracy: 0.9595 - val_loss: 0.0396 - val_accuracy: 0.9886\n",
            "Epoch 486/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1166 - accuracy: 0.9645 - val_loss: 0.0383 - val_accuracy: 0.9886\n",
            "Epoch 487/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1232 - accuracy: 0.9636 - val_loss: 0.0374 - val_accuracy: 0.9886\n",
            "Epoch 488/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1194 - accuracy: 0.9595 - val_loss: 0.0377 - val_accuracy: 0.9886\n",
            "Epoch 489/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1250 - accuracy: 0.9612 - val_loss: 0.0380 - val_accuracy: 0.9876\n",
            "Epoch 490/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1182 - accuracy: 0.9640 - val_loss: 0.0375 - val_accuracy: 0.9876\n",
            "Epoch 491/500\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1236 - accuracy: 0.9617 - val_loss: 0.0374 - val_accuracy: 0.9876\n",
            "Epoch 492/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1173 - accuracy: 0.9605 - val_loss: 0.0384 - val_accuracy: 0.9876\n",
            "Epoch 493/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1144 - accuracy: 0.9617 - val_loss: 0.0394 - val_accuracy: 0.9876\n",
            "Epoch 494/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1170 - accuracy: 0.9640 - val_loss: 0.0386 - val_accuracy: 0.9876\n",
            "Epoch 495/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1201 - accuracy: 0.9588 - val_loss: 0.0377 - val_accuracy: 0.9876\n",
            "Epoch 496/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1190 - accuracy: 0.9624 - val_loss: 0.0368 - val_accuracy: 0.9886\n",
            "Epoch 497/500\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1236 - accuracy: 0.9600 - val_loss: 0.0365 - val_accuracy: 0.9886\n",
            "Epoch 498/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1152 - accuracy: 0.9638 - val_loss: 0.0356 - val_accuracy: 0.9886\n",
            "Epoch 499/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1130 - accuracy: 0.9626 - val_loss: 0.0354 - val_accuracy: 0.9886\n",
            "Epoch 500/500\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1176 - accuracy: 0.9612 - val_loss: 0.0357 - val_accuracy: 0.9886\n",
            "\n",
            "Score:  [0.07078466564416885, 0.973714292049408]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkFO8VHuGEdh",
        "outputId": "fa1dabd5-b3fe-4392-efab-2f5d3023f1fa"
      },
      "source": [
        "np.round(model.predict(dat_resaXautots))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h9Rfu_R9GEdh",
        "outputId": "89cecf8b-7742-4335-bc1f-4e7fc2887dd2"
      },
      "source": [
        "ffnn_res=pd.DataFrame(dat_resaXautots)\n",
        "ffnn_res['Disease']=dat_resaYts\n",
        "ffnn_res=pd.concat([ffnn_res,pd.DataFrame(np.round(model.predict(dat_resaXautots))).astype(int)],axis=1)\n",
        "ffnn_res"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>Disease</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.21</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>22.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.82</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.62</td>\n",
              "      <td>10.29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.56</td>\n",
              "      <td>9.80</td>\n",
              "      <td>16.80</td>\n",
              "      <td>3.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.68</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.26</td>\n",
              "      <td>34.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.95</td>\n",
              "      <td>12.42</td>\n",
              "      <td>5.41</td>\n",
              "      <td>breast cancer</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.55</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.53</td>\n",
              "      <td>12.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.18</td>\n",
              "      <td>11.29</td>\n",
              "      <td>28.76</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.82</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.96</td>\n",
              "      <td>39.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.98</td>\n",
              "      <td>34.77</td>\n",
              "      <td>2.18</td>\n",
              "      <td>diffuse large B-cell lymphoma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.63</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.72</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.76</td>\n",
              "      <td>0.00</td>\n",
              "      <td>59.81</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.94</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.15</td>\n",
              "      <td>20.76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.92</td>\n",
              "      <td>43.97</td>\n",
              "      <td>0.00</td>\n",
              "      <td>chronic lymphocytic leukaemia</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>34.57</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.80</td>\n",
              "      <td>18.61</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.41</td>\n",
              "      <td>4.90</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.30</td>\n",
              "      <td>44.66</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.91</td>\n",
              "      <td>35.68</td>\n",
              "      <td>14.80</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>31.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.18</td>\n",
              "      <td>24.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.41</td>\n",
              "      <td>10.63</td>\n",
              "      <td>29.24</td>\n",
              "      <td>3.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.95</td>\n",
              "      <td>13.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>breast cancer</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1745</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.92</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.77</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.75</td>\n",
              "      <td>32.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.77</td>\n",
              "      <td>15.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.22</td>\n",
              "      <td>0.00</td>\n",
              "      <td>63.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.95</td>\n",
              "      <td>47.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.48</td>\n",
              "      <td>24.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1746</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.92</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.08</td>\n",
              "      <td>35.67</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.71</td>\n",
              "      <td>7.74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.21</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>66.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>38.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.16</td>\n",
              "      <td>26.59</td>\n",
              "      <td>3.18</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1747</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.82</td>\n",
              "      <td>36.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.47</td>\n",
              "      <td>3.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.78</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.78</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.72</td>\n",
              "      <td>36.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.59</td>\n",
              "      <td>49.02</td>\n",
              "      <td>12.03</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.90</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.70</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.08</td>\n",
              "      <td>27.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47.26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47.24</td>\n",
              "      <td>11.70</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49.52</td>\n",
              "      <td>2.16</td>\n",
              "      <td>43.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.78</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>48.91</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.59</td>\n",
              "      <td>37.27</td>\n",
              "      <td>15.95</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1749</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.07</td>\n",
              "      <td>38.66</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.36</td>\n",
              "      <td>6.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>44.13</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.97</td>\n",
              "      <td>24.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.47</td>\n",
              "      <td>22.41</td>\n",
              "      <td>0.45</td>\n",
              "      <td>multiple myeloma</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1750 rows × 68 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0    1    2      3    4      5    6  ...  0  1  2  3  4  5  6\n",
              "0     0.0  0.0  0.0  48.02  0.0  54.00  0.0  ...  0  0  1  0  0  0  0\n",
              "1     0.0  0.0  0.0  22.04  0.0  51.55  0.0  ...  0  0  0  0  1  0  0\n",
              "2     0.0  0.0  0.0   1.02  0.0  13.20  0.0  ...  0  0  0  1  0  0  0\n",
              "3     0.0  0.0  0.0  40.25  0.0  34.98  0.0  ...  0  0  0  0  0  0  0\n",
              "4     0.0  0.0  0.0  37.06  0.0  39.49  0.0  ...  0  0  1  0  0  0  0\n",
              "...   ...  ...  ...    ...  ...    ...  ...  ... .. .. .. .. .. .. ..\n",
              "1745  0.0  0.0  0.0  11.92  0.0  23.33  0.0  ...  0  0  0  0  0  1  0\n",
              "1746  0.0  0.0  0.0   4.26  0.0  17.71  0.0  ...  0  0  0  0  0  1  0\n",
              "1747  0.0  0.0  0.0   0.00  0.0  30.04  0.0  ...  0  0  0  0  0  1  0\n",
              "1748  0.0  0.0  0.0   1.90  0.0  31.31  0.0  ...  0  0  0  0  0  1  0\n",
              "1749  0.0  0.0  0.0   9.44  0.0   6.81  0.0  ...  0  0  0  0  0  1  0\n",
              "\n",
              "[1750 rows x 68 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trC3jauLMJTV"
      },
      "source": [
        "### dataset2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg8e1GJzL68W"
      },
      "source": [
        "#### selectkbest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFop3KfwL68X"
      },
      "source": [
        "##### logistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmo-8rLjL68X"
      },
      "source": [
        "my_param_grid = [\n",
        "    {'solver': ['newton-cg', 'lbfgs', 'saga'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l2'], 'max_iter': [200]},\n",
        "    {'solver': ['liblinear'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l1', 'l2'], 'max_iter': [200]}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdKb5i3RL68X"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOPA2DbQL68Y"
      },
      "source": [
        "modellr = GridSearchCV(estimator=LogisticRegression(n_jobs=-1), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH-dcbhwL68Y",
        "outputId": "19b6fd78-2e31-4655-a806-b331680951b9"
      },
      "source": [
        "modellr.fit(dat_resaXredtr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             estimator=LogisticRegression(n_jobs=-1),\n",
              "             param_grid=[{'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l2'],\n",
              "                          'solver': ['newton-cg', 'lbfgs', 'saga']},\n",
              "                         {'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l1', 'l2'], 'solver': ['liblinear']}],\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcJpFtyvL68Z",
        "outputId": "9dd33ffd-d83a-49fd-eb55-0d1580c31696"
      },
      "source": [
        "print(cross_val_score(modellr.best_estimator_,dat_resaXredtr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.14617329 -0.10654547 -0.10701896 -0.1044449  -0.10095171 -0.12607507\n",
            " -0.11120578 -0.11116194 -0.1197717  -0.11450305 -0.10409454 -0.10523808\n",
            " -0.10293788 -0.1286345  -0.12359535 -0.12638269 -0.10721437 -0.12482472\n",
            " -0.09128577 -0.11978918 -0.10239291 -0.10299215 -0.12986071 -0.1323986\n",
            " -0.11746923 -0.10481024 -0.12848086 -0.10335878 -0.11401957 -0.11172256\n",
            " -0.13358603 -0.09187093 -0.11208597 -0.10695082 -0.12313237 -0.1119684\n",
            " -0.12880841 -0.12129331 -0.09783017 -0.10809226 -0.09734799 -0.10857409\n",
            " -0.12479109 -0.12350218 -0.11543679 -0.10375743 -0.10508829 -0.11656594\n",
            " -0.13352618 -0.1071    ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcv8SjxDL68Z",
        "outputId": "c0b83b28-782d-447e-e1ae-af75829df68b"
      },
      "source": [
        "modellr.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.1140164119294721"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_8GmXFWL68a",
        "outputId": "58391d02-bc7a-44d0-a06e-55bbfcca7eab"
      },
      "source": [
        "log_loss(dat_resaYts, modellr.predict_proba(dat_resaXredts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1439721643675131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifNu_skZL68a"
      },
      "source": [
        "modellr_best = modellr.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SzUOE2dL68a",
        "outputId": "cb1bcada-3fd4-4b63-b4f4-77649e7a172a"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modellr_best.predict(dat_resaXredtr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.98      0.99      0.98      2827\n",
            "   leukaemia       0.97      0.97      0.97      2827\n",
            "    lymphoma       0.98      0.98      0.98      2827\n",
            "      normal       0.96      0.95      0.96      2827\n",
            "\n",
            "    accuracy                           0.97     11308\n",
            "   macro avg       0.97      0.97      0.97     11308\n",
            "weighted avg       0.97      0.97      0.97     11308\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twqIwiwYL68a",
        "outputId": "ed671919-2328-49bc-f4cd-fe39a438f30c"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modellr_best.predict(dat_resaXredts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.95      0.99      0.97       943\n",
            "   leukaemia       0.95      0.97      0.96       943\n",
            "    lymphoma       0.96      0.98      0.97       943\n",
            "      normal       0.97      0.89      0.93       943\n",
            "\n",
            "    accuracy                           0.96      3772\n",
            "   macro avg       0.96      0.96      0.96      3772\n",
            "weighted avg       0.96      0.96      0.96      3772\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE90RLMIL68a"
      },
      "source": [
        "##### random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT0Q2fBuL68a"
      },
      "source": [
        "my_param_grid = {'bootstrap': [True, False], \n",
        "                 'n_estimators': [10, 50], \n",
        "                 'min_samples_leaf': [20, 40, 60],\n",
        "                 'min_weight_fraction_leaf': [0.01, 0.02, 0.05],\n",
        "                 'criterion': ['gini', 'entropy'], \n",
        "                 'min_impurity_decrease': [1e-5, 1e-6, 1e-7]\n",
        "                 }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsByOzRCL68a"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTN5Z4uAL68a"
      },
      "source": [
        "modelrf = GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1,warm_start=True), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_fOEHF0L68a",
        "outputId": "7bf7f0ff-f405-4bcd-a784-a6a686dbc419"
      },
      "source": [
        "modelrf.fit(dat_resaXredtr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             estimator=RandomForestClassifier(n_jobs=-1, warm_start=True),\n",
              "             param_grid={'bootstrap': [True, False],\n",
              "                         'criterion': ['gini', 'entropy'],\n",
              "                         'min_impurity_decrease': [1e-05, 1e-06, 1e-07],\n",
              "                         'min_samples_leaf': [20, 40, 60],\n",
              "                         'min_weight_fraction_leaf': [0.01, 0.02, 0.05],\n",
              "                         'n_estimators': [10, 50]},\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYLSJwoaL68a",
        "outputId": "5a0869ec-cfe7-4f4e-8f53-fe7a48700b98"
      },
      "source": [
        "print(cross_val_score(modelrf.best_estimator_,dat_resaXredtr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.26793647 -0.25251624 -0.25940738 -0.25640126 -0.26042252 -0.26512255\n",
            " -0.26470194 -0.25510965 -0.26010878 -0.26021473 -0.25454657 -0.27536382\n",
            " -0.26355159 -0.26082242 -0.25170511 -0.27250712 -0.26116075 -0.2594405\n",
            " -0.25246968 -0.26071598 -0.25640056 -0.25546072 -0.26433777 -0.26409907\n",
            " -0.27376285 -0.24456461 -0.2816777  -0.25667293 -0.26368459 -0.25089927\n",
            " -0.26683732 -0.25033319 -0.25561378 -0.26221845 -0.27006592 -0.26155326\n",
            " -0.26419556 -0.26731754 -0.24902143 -0.25674785 -0.25103987 -0.24920398\n",
            " -0.27009671 -0.25826128 -0.27377455 -0.26644021 -0.25171801 -0.25360589\n",
            " -0.27417333 -0.26631113]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwuWwg9CL68a",
        "outputId": "68b1c78d-fe2b-477a-8f73-b645b2aa79ed"
      },
      "source": [
        "-modelrf.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26088673693418607"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h17Di43hL68b",
        "outputId": "d59dd5ff-cd15-4066-b6e8-2878ec4b2563"
      },
      "source": [
        "log_loss(dat_resaYts, modelrf.predict_proba(dat_resaXredts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2737405390615318"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEhzZI8LL68b"
      },
      "source": [
        "modelrf_best = modelrf.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7-qR4ojL68b",
        "outputId": "3ddebc7a-922f-4499-b660-d2687efd8569"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modelrf_best.predict(dat_resaXredtr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.99      0.97      0.98      2827\n",
            "   leukaemia       0.91      0.95      0.93      2827\n",
            "    lymphoma       0.96      0.96      0.96      2827\n",
            "      normal       0.93      0.91      0.92      2827\n",
            "\n",
            "    accuracy                           0.95     11308\n",
            "   macro avg       0.95      0.95      0.95     11308\n",
            "weighted avg       0.95      0.95      0.95     11308\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F-gwvQPL68b",
        "outputId": "05e1960b-e05a-474b-9a0f-78043444ac8b"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modelrf_best.predict(dat_resaXredts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.97      0.97      0.97       943\n",
            "   leukaemia       0.89      0.94      0.92       943\n",
            "    lymphoma       0.95      0.97      0.96       943\n",
            "      normal       0.92      0.85      0.88       943\n",
            "\n",
            "    accuracy                           0.93      3772\n",
            "   macro avg       0.93      0.93      0.93      3772\n",
            "weighted avg       0.93      0.93      0.93      3772\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph9TLTksL68b"
      },
      "source": [
        "##### gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd1M5bELL68b"
      },
      "source": [
        "my_param_grid = {\n",
        "    \"loss\":[\"deviance\"],\n",
        "    \"learning_rate\": [0.2],\n",
        "    \"min_samples_split\": np.linspace(0.1, 0.5, 1),\n",
        "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 1),\n",
        "    \"max_depth\":[3,8],\n",
        "    \"max_features\":[\"log2\",\"sqrt\"],\n",
        "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
        "    \"subsample\":[0.8],\n",
        "    \"n_estimators\":[30]\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2WlUJ4AL68b"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-wxU2-vL68b"
      },
      "source": [
        "modelxgb = GridSearchCV(estimator=GradientBoostingClassifier(warm_start=True), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBUb3xgcL68b",
        "outputId": "c98e1f10-84ff-4b6f-8391-505dc4aa0435"
      },
      "source": [
        "modelxgb.fit(dat_resaXredtr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             estimator=GradientBoostingClassifier(warm_start=True),\n",
              "             param_grid={'criterion': ['friedman_mse', 'mae'],\n",
              "                         'learning_rate': [0.2], 'loss': ['deviance'],\n",
              "                         'max_depth': [3, 8], 'max_features': ['log2', 'sqrt'],\n",
              "                         'min_samples_leaf': array([0.1]),\n",
              "                         'min_samples_split': array([0.1]),\n",
              "                         'n_estimators': [30], 'subsample': [0.8]},\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrfgGQNmL68b",
        "outputId": "ff654a15-2d67-42be-f0d2-6e5564fbc908"
      },
      "source": [
        "print(cross_val_score(modelxgb.best_estimator_,dat_resaXredtr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.2033193  -0.18504889 -0.19782628 -0.20161552 -0.20393813 -0.20346237\n",
            " -0.19019388 -0.18978992 -0.1981412  -0.19279822 -0.18405773 -0.21044535\n",
            " -0.19242025 -0.20034338 -0.19154668 -0.2082805  -0.19100681 -0.19274935\n",
            " -0.18682589 -0.19049503 -0.19058187 -0.17924525 -0.20276461 -0.20388554\n",
            " -0.20144507 -0.18944425 -0.21337723 -0.17818946 -0.18618992 -0.19396377\n",
            " -0.20656579 -0.18126434 -0.19577074 -0.19432884 -0.20558769 -0.1925647\n",
            " -0.20018755 -0.2044163  -0.17792045 -0.19368494 -0.18757612 -0.18320381\n",
            " -0.20465876 -0.19415993 -0.20367977 -0.19138515 -0.18834825 -0.19000477\n",
            " -0.2065942  -0.18912238]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X-wmmNgL68b",
        "outputId": "0fbe4c8c-b893-4905-b68c-d95c8408052f"
      },
      "source": [
        "-modelxgb.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1938361420159131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St77yrdiL68b",
        "outputId": "aeed7b33-324f-444a-ca87-74376f3fd8f2"
      },
      "source": [
        "log_loss(dat_resaYts, modelxgb.predict_proba(dat_resaXredts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19557447710319892"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if4PgSsCL68b"
      },
      "source": [
        "modelxgb_best = modelxgb.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAbpqyqlL68b",
        "outputId": "ca996b4e-5c3c-4e87-ca91-d93ffcddebdb"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modelxgb_best.predict(dat_resaXredtr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.98      0.97      0.97      2827\n",
            "   leukaemia       0.94      0.95      0.94      2827\n",
            "    lymphoma       0.96      0.96      0.96      2827\n",
            "      normal       0.93      0.93      0.93      2827\n",
            "\n",
            "    accuracy                           0.95     11308\n",
            "   macro avg       0.95      0.95      0.95     11308\n",
            "weighted avg       0.95      0.95      0.95     11308\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QfYY8TPL68b",
        "outputId": "f00d63df-9a4b-4076-c24f-c37067a79671"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modelxgb_best.predict(dat_resaXredts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.95      0.98      0.96       943\n",
            "   leukaemia       0.94      0.95      0.94       943\n",
            "    lymphoma       0.95      0.97      0.96       943\n",
            "      normal       0.95      0.89      0.92       943\n",
            "\n",
            "    accuracy                           0.95      3772\n",
            "   macro avg       0.95      0.95      0.95      3772\n",
            "weighted avg       0.95      0.95      0.95      3772\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvfVTKKHL68b"
      },
      "source": [
        "##### fully connected ffnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZviMELdL68c",
        "outputId": "356f8914-92d2-4863-c201-8cf256ad3ddb"
      },
      "source": [
        "dat_resaYtr"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['breast', 'normal', 'leukaemia', ..., 'normal', 'normal', 'normal'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srMM0Q6vL68c"
      },
      "source": [
        "num_classes=4\n",
        "dat_resaYtrx=pd.DataFrame(dat_resaYtr)\n",
        "dat_resaYtrx[0] = pd.Categorical(dat_resaYtrx[0])\n",
        "dat_resaYtrx['code'] = dat_resaYtrx[0].cat.codes\n",
        "dat_resaYtrx_=np.array(dat_resaYtrx['code']\n",
        "                       )\n",
        "dat_resaYtsx=pd.DataFrame(dat_resaYts)\n",
        "dat_resaYtsx[0] = pd.Categorical(dat_resaYtsx[0])\n",
        "dat_resaYtsx['code'] = dat_resaYtsx[0].cat.codes\n",
        "dat_resaYtsx_=np.array(dat_resaYtsx['code'])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz5PUj_GL68c",
        "outputId": "6d5220bd-64c8-4d77-ce6b-b92bf4397ebd"
      },
      "source": [
        "dat_resaYtrcl = to_categorical(dat_resaYtrx_, num_classes)\n",
        "dat_resaYtscl = to_categorical(dat_resaYtsx_, num_classes)\n",
        "dat_resaYtrcl"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqIl6xObL68c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(30, activation='relu', input_shape=(100,)))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulJbxcHSL68c"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOXc7govL68c",
        "outputId": "872ecfc9-cb29-4651-e96f-238eb5ee74a9"
      },
      "source": [
        "history_ = model.fit(dat_resaXredtr, dat_resaYtrcl, \n",
        "                            batch_size=8000, epochs=500, verbose=1, validation_split=0.2)\n",
        " \n",
        "score_ = model.evaluate(dat_resaXredts,dat_resaYtscl , verbose=0)\n",
        "print('\\nScore: ', score_)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "2/2 [==============================] - 0s 99ms/step - loss: 1.4594 - accuracy: 0.2580 - val_loss: 1.3262 - val_accuracy: 0.3426\n",
            "Epoch 2/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.3894 - accuracy: 0.3154 - val_loss: 1.3443 - val_accuracy: 0.2993\n",
            "Epoch 3/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.3288 - accuracy: 0.4212 - val_loss: 1.3625 - val_accuracy: 0.2423\n",
            "Epoch 4/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.2699 - accuracy: 0.5043 - val_loss: 1.3833 - val_accuracy: 0.1932\n",
            "Epoch 5/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.2163 - accuracy: 0.5543 - val_loss: 1.4077 - val_accuracy: 0.1348\n",
            "Epoch 6/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1673 - accuracy: 0.5820 - val_loss: 1.4310 - val_accuracy: 0.0968\n",
            "Epoch 7/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1371 - accuracy: 0.5885 - val_loss: 1.4474 - val_accuracy: 0.0787\n",
            "Epoch 8/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1083 - accuracy: 0.5953 - val_loss: 1.4557 - val_accuracy: 0.0637\n",
            "Epoch 9/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0871 - accuracy: 0.6013 - val_loss: 1.4574 - val_accuracy: 0.0575\n",
            "Epoch 10/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.0627 - accuracy: 0.6173 - val_loss: 1.4539 - val_accuracy: 0.0544\n",
            "Epoch 11/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0439 - accuracy: 0.6230 - val_loss: 1.4451 - val_accuracy: 0.0566\n",
            "Epoch 12/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.0222 - accuracy: 0.6355 - val_loss: 1.4343 - val_accuracy: 0.0628\n",
            "Epoch 13/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0091 - accuracy: 0.6351 - val_loss: 1.4237 - val_accuracy: 0.0712\n",
            "Epoch 14/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9906 - accuracy: 0.6439 - val_loss: 1.4149 - val_accuracy: 0.0774\n",
            "Epoch 15/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9767 - accuracy: 0.6485 - val_loss: 1.4090 - val_accuracy: 0.0840\n",
            "Epoch 16/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.9611 - accuracy: 0.6500 - val_loss: 1.4037 - val_accuracy: 0.0920\n",
            "Epoch 17/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9448 - accuracy: 0.6510 - val_loss: 1.3973 - val_accuracy: 0.1012\n",
            "Epoch 18/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.9305 - accuracy: 0.6527 - val_loss: 1.3919 - val_accuracy: 0.1074\n",
            "Epoch 19/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9205 - accuracy: 0.6558 - val_loss: 1.3864 - val_accuracy: 0.1123\n",
            "Epoch 20/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.9059 - accuracy: 0.6572 - val_loss: 1.3799 - val_accuracy: 0.1145\n",
            "Epoch 21/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.8924 - accuracy: 0.6629 - val_loss: 1.3720 - val_accuracy: 0.1163\n",
            "Epoch 22/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.8802 - accuracy: 0.6617 - val_loss: 1.3616 - val_accuracy: 0.1211\n",
            "Epoch 23/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.8707 - accuracy: 0.6612 - val_loss: 1.3499 - val_accuracy: 0.1256\n",
            "Epoch 24/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.8557 - accuracy: 0.6637 - val_loss: 1.3420 - val_accuracy: 0.1282\n",
            "Epoch 25/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.8506 - accuracy: 0.6644 - val_loss: 1.3405 - val_accuracy: 0.1269\n",
            "Epoch 26/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.8398 - accuracy: 0.6680 - val_loss: 1.3407 - val_accuracy: 0.1269\n",
            "Epoch 27/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.8285 - accuracy: 0.6673 - val_loss: 1.3374 - val_accuracy: 0.1282\n",
            "Epoch 28/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.8202 - accuracy: 0.6668 - val_loss: 1.3316 - val_accuracy: 0.1313\n",
            "Epoch 29/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.8082 - accuracy: 0.6707 - val_loss: 1.3233 - val_accuracy: 0.1353\n",
            "Epoch 30/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8019 - accuracy: 0.6729 - val_loss: 1.3147 - val_accuracy: 0.1397\n",
            "Epoch 31/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7931 - accuracy: 0.6740 - val_loss: 1.3070 - val_accuracy: 0.1432\n",
            "Epoch 32/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7898 - accuracy: 0.6743 - val_loss: 1.3029 - val_accuracy: 0.1446\n",
            "Epoch 33/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7801 - accuracy: 0.6758 - val_loss: 1.3025 - val_accuracy: 0.1446\n",
            "Epoch 34/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7726 - accuracy: 0.6775 - val_loss: 1.3034 - val_accuracy: 0.1446\n",
            "Epoch 35/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7641 - accuracy: 0.6772 - val_loss: 1.3021 - val_accuracy: 0.1446\n",
            "Epoch 36/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7570 - accuracy: 0.6805 - val_loss: 1.2976 - val_accuracy: 0.1459\n",
            "Epoch 37/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.7485 - accuracy: 0.6781 - val_loss: 1.2913 - val_accuracy: 0.1485\n",
            "Epoch 38/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7418 - accuracy: 0.6810 - val_loss: 1.2829 - val_accuracy: 0.1508\n",
            "Epoch 39/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7334 - accuracy: 0.6817 - val_loss: 1.2723 - val_accuracy: 0.1578\n",
            "Epoch 40/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7268 - accuracy: 0.6841 - val_loss: 1.2609 - val_accuracy: 0.1618\n",
            "Epoch 41/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7232 - accuracy: 0.6846 - val_loss: 1.2518 - val_accuracy: 0.1653\n",
            "Epoch 42/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.7140 - accuracy: 0.6872 - val_loss: 1.2430 - val_accuracy: 0.1671\n",
            "Epoch 43/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7049 - accuracy: 0.6888 - val_loss: 1.2318 - val_accuracy: 0.1706\n",
            "Epoch 44/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6959 - accuracy: 0.6939 - val_loss: 1.2183 - val_accuracy: 0.1804\n",
            "Epoch 45/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6907 - accuracy: 0.6971 - val_loss: 1.2056 - val_accuracy: 0.1848\n",
            "Epoch 46/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6822 - accuracy: 0.6991 - val_loss: 1.1922 - val_accuracy: 0.1892\n",
            "Epoch 47/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6758 - accuracy: 0.7061 - val_loss: 1.1720 - val_accuracy: 0.2109\n",
            "Epoch 48/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6663 - accuracy: 0.7185 - val_loss: 1.1416 - val_accuracy: 0.2679\n",
            "Epoch 49/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6583 - accuracy: 0.7250 - val_loss: 1.1084 - val_accuracy: 0.3528\n",
            "Epoch 50/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6478 - accuracy: 0.7443 - val_loss: 1.0740 - val_accuracy: 0.4288\n",
            "Epoch 51/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6398 - accuracy: 0.7569 - val_loss: 1.0433 - val_accuracy: 0.4987\n",
            "Epoch 52/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6283 - accuracy: 0.7709 - val_loss: 1.0262 - val_accuracy: 0.5420\n",
            "Epoch 53/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6163 - accuracy: 0.7790 - val_loss: 1.0188 - val_accuracy: 0.5672\n",
            "Epoch 54/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6041 - accuracy: 0.7872 - val_loss: 1.0118 - val_accuracy: 0.5774\n",
            "Epoch 55/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5975 - accuracy: 0.7904 - val_loss: 0.9981 - val_accuracy: 0.5924\n",
            "Epoch 56/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5907 - accuracy: 0.7932 - val_loss: 0.9737 - val_accuracy: 0.6167\n",
            "Epoch 57/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5840 - accuracy: 0.7953 - val_loss: 0.9468 - val_accuracy: 0.6344\n",
            "Epoch 58/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5735 - accuracy: 0.7963 - val_loss: 0.9301 - val_accuracy: 0.6454\n",
            "Epoch 59/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5652 - accuracy: 0.7988 - val_loss: 0.9228 - val_accuracy: 0.6503\n",
            "Epoch 60/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5587 - accuracy: 0.8033 - val_loss: 0.9176 - val_accuracy: 0.6547\n",
            "Epoch 61/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5525 - accuracy: 0.8040 - val_loss: 0.9096 - val_accuracy: 0.6574\n",
            "Epoch 62/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5434 - accuracy: 0.8089 - val_loss: 0.9030 - val_accuracy: 0.6622\n",
            "Epoch 63/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5360 - accuracy: 0.8121 - val_loss: 0.8884 - val_accuracy: 0.6662\n",
            "Epoch 64/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5286 - accuracy: 0.8128 - val_loss: 0.8636 - val_accuracy: 0.6742\n",
            "Epoch 65/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5241 - accuracy: 0.8172 - val_loss: 0.8435 - val_accuracy: 0.6879\n",
            "Epoch 66/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5148 - accuracy: 0.8227 - val_loss: 0.8312 - val_accuracy: 0.6936\n",
            "Epoch 67/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5084 - accuracy: 0.8217 - val_loss: 0.8234 - val_accuracy: 0.6950\n",
            "Epoch 68/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5059 - accuracy: 0.8209 - val_loss: 0.8206 - val_accuracy: 0.6919\n",
            "Epoch 69/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4963 - accuracy: 0.8239 - val_loss: 0.8168 - val_accuracy: 0.6897\n",
            "Epoch 70/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4913 - accuracy: 0.8275 - val_loss: 0.8049 - val_accuracy: 0.6923\n",
            "Epoch 71/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4904 - accuracy: 0.8282 - val_loss: 0.7835 - val_accuracy: 0.6989\n",
            "Epoch 72/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4827 - accuracy: 0.8301 - val_loss: 0.7631 - val_accuracy: 0.7056\n",
            "Epoch 73/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4797 - accuracy: 0.8288 - val_loss: 0.7484 - val_accuracy: 0.7082\n",
            "Epoch 74/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4713 - accuracy: 0.8315 - val_loss: 0.7403 - val_accuracy: 0.7100\n",
            "Epoch 75/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4702 - accuracy: 0.8291 - val_loss: 0.7389 - val_accuracy: 0.7087\n",
            "Epoch 76/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4617 - accuracy: 0.8314 - val_loss: 0.7408 - val_accuracy: 0.7056\n",
            "Epoch 77/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4606 - accuracy: 0.8354 - val_loss: 0.7396 - val_accuracy: 0.7051\n",
            "Epoch 78/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4563 - accuracy: 0.8354 - val_loss: 0.7272 - val_accuracy: 0.7118\n",
            "Epoch 79/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4499 - accuracy: 0.8374 - val_loss: 0.7045 - val_accuracy: 0.7206\n",
            "Epoch 80/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.4423 - accuracy: 0.8416 - val_loss: 0.6797 - val_accuracy: 0.7299\n",
            "Epoch 81/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4387 - accuracy: 0.8450 - val_loss: 0.6680 - val_accuracy: 0.7321\n",
            "Epoch 82/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4349 - accuracy: 0.8417 - val_loss: 0.6711 - val_accuracy: 0.7272\n",
            "Epoch 83/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4331 - accuracy: 0.8366 - val_loss: 0.6756 - val_accuracy: 0.7237\n",
            "Epoch 84/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4293 - accuracy: 0.8398 - val_loss: 0.6717 - val_accuracy: 0.7255\n",
            "Epoch 85/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4219 - accuracy: 0.8450 - val_loss: 0.6630 - val_accuracy: 0.7303\n",
            "Epoch 86/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4197 - accuracy: 0.8431 - val_loss: 0.6539 - val_accuracy: 0.7343\n",
            "Epoch 87/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4182 - accuracy: 0.8405 - val_loss: 0.6444 - val_accuracy: 0.7352\n",
            "Epoch 88/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4091 - accuracy: 0.8480 - val_loss: 0.6379 - val_accuracy: 0.7334\n",
            "Epoch 89/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4077 - accuracy: 0.8480 - val_loss: 0.6312 - val_accuracy: 0.7330\n",
            "Epoch 90/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4047 - accuracy: 0.8495 - val_loss: 0.6189 - val_accuracy: 0.7378\n",
            "Epoch 91/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.4022 - accuracy: 0.8481 - val_loss: 0.6027 - val_accuracy: 0.7502\n",
            "Epoch 92/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3991 - accuracy: 0.8543 - val_loss: 0.5884 - val_accuracy: 0.7573\n",
            "Epoch 93/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3969 - accuracy: 0.8537 - val_loss: 0.5894 - val_accuracy: 0.7551\n",
            "Epoch 94/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3911 - accuracy: 0.8522 - val_loss: 0.6035 - val_accuracy: 0.7431\n",
            "Epoch 95/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3875 - accuracy: 0.8514 - val_loss: 0.6069 - val_accuracy: 0.7370\n",
            "Epoch 96/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3837 - accuracy: 0.8549 - val_loss: 0.5896 - val_accuracy: 0.7493\n",
            "Epoch 97/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3822 - accuracy: 0.8540 - val_loss: 0.5675 - val_accuracy: 0.7577\n",
            "Epoch 98/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3728 - accuracy: 0.8604 - val_loss: 0.5519 - val_accuracy: 0.7613\n",
            "Epoch 99/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3757 - accuracy: 0.8595 - val_loss: 0.5486 - val_accuracy: 0.7604\n",
            "Epoch 100/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3695 - accuracy: 0.8610 - val_loss: 0.5500 - val_accuracy: 0.7591\n",
            "Epoch 101/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3656 - accuracy: 0.8624 - val_loss: 0.5510 - val_accuracy: 0.7577\n",
            "Epoch 102/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3633 - accuracy: 0.8624 - val_loss: 0.5484 - val_accuracy: 0.7617\n",
            "Epoch 103/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3615 - accuracy: 0.8637 - val_loss: 0.5409 - val_accuracy: 0.7644\n",
            "Epoch 104/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3582 - accuracy: 0.8641 - val_loss: 0.5333 - val_accuracy: 0.7653\n",
            "Epoch 105/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3522 - accuracy: 0.8661 - val_loss: 0.5261 - val_accuracy: 0.7697\n",
            "Epoch 106/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3501 - accuracy: 0.8662 - val_loss: 0.5193 - val_accuracy: 0.7723\n",
            "Epoch 107/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3493 - accuracy: 0.8685 - val_loss: 0.5129 - val_accuracy: 0.7745\n",
            "Epoch 108/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3450 - accuracy: 0.8698 - val_loss: 0.5022 - val_accuracy: 0.7838\n",
            "Epoch 109/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3418 - accuracy: 0.8693 - val_loss: 0.4972 - val_accuracy: 0.7891\n",
            "Epoch 110/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3364 - accuracy: 0.8748 - val_loss: 0.5020 - val_accuracy: 0.7940\n",
            "Epoch 111/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3366 - accuracy: 0.8762 - val_loss: 0.5111 - val_accuracy: 0.7958\n",
            "Epoch 112/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3358 - accuracy: 0.8773 - val_loss: 0.5112 - val_accuracy: 0.7958\n",
            "Epoch 113/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3345 - accuracy: 0.8749 - val_loss: 0.4926 - val_accuracy: 0.7993\n",
            "Epoch 114/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3278 - accuracy: 0.8782 - val_loss: 0.4709 - val_accuracy: 0.8059\n",
            "Epoch 115/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3278 - accuracy: 0.8809 - val_loss: 0.4612 - val_accuracy: 0.8086\n",
            "Epoch 116/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3228 - accuracy: 0.8827 - val_loss: 0.4658 - val_accuracy: 0.8077\n",
            "Epoch 117/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3210 - accuracy: 0.8803 - val_loss: 0.4708 - val_accuracy: 0.8108\n",
            "Epoch 118/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3218 - accuracy: 0.8803 - val_loss: 0.4629 - val_accuracy: 0.8236\n",
            "Epoch 119/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3174 - accuracy: 0.8856 - val_loss: 0.4525 - val_accuracy: 0.8338\n",
            "Epoch 120/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3143 - accuracy: 0.8868 - val_loss: 0.4472 - val_accuracy: 0.8364\n",
            "Epoch 121/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3138 - accuracy: 0.8869 - val_loss: 0.4466 - val_accuracy: 0.8355\n",
            "Epoch 122/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3126 - accuracy: 0.8865 - val_loss: 0.4455 - val_accuracy: 0.8355\n",
            "Epoch 123/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3100 - accuracy: 0.8898 - val_loss: 0.4451 - val_accuracy: 0.8338\n",
            "Epoch 124/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3045 - accuracy: 0.8906 - val_loss: 0.4449 - val_accuracy: 0.8373\n",
            "Epoch 125/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3004 - accuracy: 0.8933 - val_loss: 0.4411 - val_accuracy: 0.8382\n",
            "Epoch 126/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3035 - accuracy: 0.8899 - val_loss: 0.4307 - val_accuracy: 0.8510\n",
            "Epoch 127/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3003 - accuracy: 0.8949 - val_loss: 0.4235 - val_accuracy: 0.8550\n",
            "Epoch 128/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3012 - accuracy: 0.8948 - val_loss: 0.4214 - val_accuracy: 0.8546\n",
            "Epoch 129/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2963 - accuracy: 0.8958 - val_loss: 0.4224 - val_accuracy: 0.8523\n",
            "Epoch 130/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2970 - accuracy: 0.8955 - val_loss: 0.4219 - val_accuracy: 0.8470\n",
            "Epoch 131/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2911 - accuracy: 0.8956 - val_loss: 0.4116 - val_accuracy: 0.8523\n",
            "Epoch 132/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2867 - accuracy: 0.9004 - val_loss: 0.4058 - val_accuracy: 0.8576\n",
            "Epoch 133/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2889 - accuracy: 0.8996 - val_loss: 0.4068 - val_accuracy: 0.8585\n",
            "Epoch 134/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2846 - accuracy: 0.8994 - val_loss: 0.4021 - val_accuracy: 0.8616\n",
            "Epoch 135/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2849 - accuracy: 0.9025 - val_loss: 0.3942 - val_accuracy: 0.8647\n",
            "Epoch 136/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2818 - accuracy: 0.9022 - val_loss: 0.3892 - val_accuracy: 0.8669\n",
            "Epoch 137/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2824 - accuracy: 0.9039 - val_loss: 0.3865 - val_accuracy: 0.8660\n",
            "Epoch 138/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2807 - accuracy: 0.9024 - val_loss: 0.3834 - val_accuracy: 0.8638\n",
            "Epoch 139/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2768 - accuracy: 0.9061 - val_loss: 0.3750 - val_accuracy: 0.8665\n",
            "Epoch 140/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2746 - accuracy: 0.9047 - val_loss: 0.3687 - val_accuracy: 0.8678\n",
            "Epoch 141/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2739 - accuracy: 0.9088 - val_loss: 0.3707 - val_accuracy: 0.8683\n",
            "Epoch 142/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2745 - accuracy: 0.9068 - val_loss: 0.3705 - val_accuracy: 0.8683\n",
            "Epoch 143/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2706 - accuracy: 0.9061 - val_loss: 0.3696 - val_accuracy: 0.8705\n",
            "Epoch 144/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2706 - accuracy: 0.9068 - val_loss: 0.3727 - val_accuracy: 0.8700\n",
            "Epoch 145/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2684 - accuracy: 0.9074 - val_loss: 0.3754 - val_accuracy: 0.8660\n",
            "Epoch 146/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2680 - accuracy: 0.9064 - val_loss: 0.3696 - val_accuracy: 0.8709\n",
            "Epoch 147/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2650 - accuracy: 0.9090 - val_loss: 0.3601 - val_accuracy: 0.8767\n",
            "Epoch 148/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2646 - accuracy: 0.9086 - val_loss: 0.3538 - val_accuracy: 0.8775\n",
            "Epoch 149/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2620 - accuracy: 0.9126 - val_loss: 0.3518 - val_accuracy: 0.8789\n",
            "Epoch 150/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2606 - accuracy: 0.9094 - val_loss: 0.3536 - val_accuracy: 0.8749\n",
            "Epoch 151/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2588 - accuracy: 0.9123 - val_loss: 0.3519 - val_accuracy: 0.8731\n",
            "Epoch 152/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2590 - accuracy: 0.9091 - val_loss: 0.3437 - val_accuracy: 0.8749\n",
            "Epoch 153/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2562 - accuracy: 0.9149 - val_loss: 0.3360 - val_accuracy: 0.8793\n",
            "Epoch 154/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2534 - accuracy: 0.9138 - val_loss: 0.3328 - val_accuracy: 0.8851\n",
            "Epoch 155/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2555 - accuracy: 0.9149 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "Epoch 156/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2562 - accuracy: 0.9140 - val_loss: 0.3584 - val_accuracy: 0.8727\n",
            "Epoch 157/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2497 - accuracy: 0.9149 - val_loss: 0.3604 - val_accuracy: 0.8678\n",
            "Epoch 158/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2514 - accuracy: 0.9109 - val_loss: 0.3444 - val_accuracy: 0.8767\n",
            "Epoch 159/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2454 - accuracy: 0.9178 - val_loss: 0.3268 - val_accuracy: 0.8855\n",
            "Epoch 160/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2460 - accuracy: 0.9201 - val_loss: 0.3218 - val_accuracy: 0.8877\n",
            "Epoch 161/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2446 - accuracy: 0.9182 - val_loss: 0.3264 - val_accuracy: 0.8846\n",
            "Epoch 162/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2433 - accuracy: 0.9196 - val_loss: 0.3265 - val_accuracy: 0.8820\n",
            "Epoch 163/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2432 - accuracy: 0.9172 - val_loss: 0.3180 - val_accuracy: 0.8859\n",
            "Epoch 164/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2442 - accuracy: 0.9165 - val_loss: 0.3152 - val_accuracy: 0.8873\n",
            "Epoch 165/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2438 - accuracy: 0.9173 - val_loss: 0.3204 - val_accuracy: 0.8833\n",
            "Epoch 166/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2427 - accuracy: 0.9180 - val_loss: 0.3272 - val_accuracy: 0.8806\n",
            "Epoch 167/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2396 - accuracy: 0.9176 - val_loss: 0.3279 - val_accuracy: 0.8798\n",
            "Epoch 168/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2433 - accuracy: 0.9197 - val_loss: 0.3187 - val_accuracy: 0.8846\n",
            "Epoch 169/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2343 - accuracy: 0.9229 - val_loss: 0.3115 - val_accuracy: 0.8873\n",
            "Epoch 170/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2360 - accuracy: 0.9194 - val_loss: 0.3078 - val_accuracy: 0.8877\n",
            "Epoch 171/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2342 - accuracy: 0.9225 - val_loss: 0.3045 - val_accuracy: 0.8886\n",
            "Epoch 172/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2352 - accuracy: 0.9218 - val_loss: 0.3022 - val_accuracy: 0.8899\n",
            "Epoch 173/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2324 - accuracy: 0.9254 - val_loss: 0.2988 - val_accuracy: 0.8917\n",
            "Epoch 174/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2332 - accuracy: 0.9224 - val_loss: 0.2978 - val_accuracy: 0.8890\n",
            "Epoch 175/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2322 - accuracy: 0.9229 - val_loss: 0.2966 - val_accuracy: 0.8890\n",
            "Epoch 176/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2302 - accuracy: 0.9223 - val_loss: 0.2919 - val_accuracy: 0.8908\n",
            "Epoch 177/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2271 - accuracy: 0.9239 - val_loss: 0.2914 - val_accuracy: 0.8908\n",
            "Epoch 178/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2244 - accuracy: 0.9233 - val_loss: 0.2940 - val_accuracy: 0.8904\n",
            "Epoch 179/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2280 - accuracy: 0.9234 - val_loss: 0.2926 - val_accuracy: 0.8912\n",
            "Epoch 180/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2254 - accuracy: 0.9276 - val_loss: 0.2876 - val_accuracy: 0.8926\n",
            "Epoch 181/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2240 - accuracy: 0.9248 - val_loss: 0.2827 - val_accuracy: 0.8930\n",
            "Epoch 182/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2238 - accuracy: 0.9250 - val_loss: 0.2802 - val_accuracy: 0.8948\n",
            "Epoch 183/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2218 - accuracy: 0.9255 - val_loss: 0.2792 - val_accuracy: 0.8957\n",
            "Epoch 184/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2209 - accuracy: 0.9259 - val_loss: 0.2752 - val_accuracy: 0.8957\n",
            "Epoch 185/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2198 - accuracy: 0.9279 - val_loss: 0.2674 - val_accuracy: 0.8996\n",
            "Epoch 186/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2226 - accuracy: 0.9275 - val_loss: 0.2674 - val_accuracy: 0.8983\n",
            "Epoch 187/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2194 - accuracy: 0.9266 - val_loss: 0.2717 - val_accuracy: 0.8970\n",
            "Epoch 188/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2229 - accuracy: 0.9257 - val_loss: 0.2683 - val_accuracy: 0.8996\n",
            "Epoch 189/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2169 - accuracy: 0.9255 - val_loss: 0.2664 - val_accuracy: 0.9023\n",
            "Epoch 190/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2167 - accuracy: 0.9293 - val_loss: 0.2724 - val_accuracy: 0.8992\n",
            "Epoch 191/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2143 - accuracy: 0.9306 - val_loss: 0.2803 - val_accuracy: 0.8948\n",
            "Epoch 192/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2161 - accuracy: 0.9285 - val_loss: 0.2760 - val_accuracy: 0.8948\n",
            "Epoch 193/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2125 - accuracy: 0.9278 - val_loss: 0.2597 - val_accuracy: 0.9023\n",
            "Epoch 194/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2156 - accuracy: 0.9281 - val_loss: 0.2472 - val_accuracy: 0.9045\n",
            "Epoch 195/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2103 - accuracy: 0.9330 - val_loss: 0.2486 - val_accuracy: 0.9058\n",
            "Epoch 196/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2105 - accuracy: 0.9305 - val_loss: 0.2625 - val_accuracy: 0.9019\n",
            "Epoch 197/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2094 - accuracy: 0.9312 - val_loss: 0.2709 - val_accuracy: 0.8988\n",
            "Epoch 198/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2098 - accuracy: 0.9295 - val_loss: 0.2639 - val_accuracy: 0.9032\n",
            "Epoch 199/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2130 - accuracy: 0.9293 - val_loss: 0.2498 - val_accuracy: 0.9076\n",
            "Epoch 200/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2070 - accuracy: 0.9302 - val_loss: 0.2360 - val_accuracy: 0.9103\n",
            "Epoch 201/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2093 - accuracy: 0.9352 - val_loss: 0.2354 - val_accuracy: 0.9094\n",
            "Epoch 202/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2094 - accuracy: 0.9350 - val_loss: 0.2429 - val_accuracy: 0.9067\n",
            "Epoch 203/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2067 - accuracy: 0.9323 - val_loss: 0.2490 - val_accuracy: 0.9085\n",
            "Epoch 204/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2039 - accuracy: 0.9350 - val_loss: 0.2499 - val_accuracy: 0.9098\n",
            "Epoch 205/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2058 - accuracy: 0.9326 - val_loss: 0.2482 - val_accuracy: 0.9089\n",
            "Epoch 206/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2027 - accuracy: 0.9326 - val_loss: 0.2465 - val_accuracy: 0.9085\n",
            "Epoch 207/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2022 - accuracy: 0.9346 - val_loss: 0.2434 - val_accuracy: 0.9094\n",
            "Epoch 208/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2000 - accuracy: 0.9361 - val_loss: 0.2373 - val_accuracy: 0.9107\n",
            "Epoch 209/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1957 - accuracy: 0.9365 - val_loss: 0.2328 - val_accuracy: 0.9147\n",
            "Epoch 210/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1997 - accuracy: 0.9343 - val_loss: 0.2340 - val_accuracy: 0.9142\n",
            "Epoch 211/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1976 - accuracy: 0.9362 - val_loss: 0.2334 - val_accuracy: 0.9147\n",
            "Epoch 212/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2000 - accuracy: 0.9341 - val_loss: 0.2292 - val_accuracy: 0.9147\n",
            "Epoch 213/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1967 - accuracy: 0.9369 - val_loss: 0.2301 - val_accuracy: 0.9160\n",
            "Epoch 214/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1962 - accuracy: 0.9359 - val_loss: 0.2282 - val_accuracy: 0.9164\n",
            "Epoch 215/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1957 - accuracy: 0.9384 - val_loss: 0.2231 - val_accuracy: 0.9187\n",
            "Epoch 216/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1961 - accuracy: 0.9389 - val_loss: 0.2261 - val_accuracy: 0.9178\n",
            "Epoch 217/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1953 - accuracy: 0.9353 - val_loss: 0.2323 - val_accuracy: 0.9164\n",
            "Epoch 218/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1938 - accuracy: 0.9357 - val_loss: 0.2287 - val_accuracy: 0.9151\n",
            "Epoch 219/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1925 - accuracy: 0.9391 - val_loss: 0.2209 - val_accuracy: 0.9200\n",
            "Epoch 220/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1928 - accuracy: 0.9357 - val_loss: 0.2138 - val_accuracy: 0.9235\n",
            "Epoch 221/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1906 - accuracy: 0.9388 - val_loss: 0.2138 - val_accuracy: 0.9213\n",
            "Epoch 222/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1945 - accuracy: 0.9377 - val_loss: 0.2194 - val_accuracy: 0.9209\n",
            "Epoch 223/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1905 - accuracy: 0.9404 - val_loss: 0.2208 - val_accuracy: 0.9209\n",
            "Epoch 224/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1911 - accuracy: 0.9378 - val_loss: 0.2193 - val_accuracy: 0.9213\n",
            "Epoch 225/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1878 - accuracy: 0.9405 - val_loss: 0.2183 - val_accuracy: 0.9213\n",
            "Epoch 226/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1842 - accuracy: 0.9437 - val_loss: 0.2186 - val_accuracy: 0.9218\n",
            "Epoch 227/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1899 - accuracy: 0.9391 - val_loss: 0.2227 - val_accuracy: 0.9204\n",
            "Epoch 228/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1865 - accuracy: 0.9391 - val_loss: 0.2269 - val_accuracy: 0.9182\n",
            "Epoch 229/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1892 - accuracy: 0.9385 - val_loss: 0.2236 - val_accuracy: 0.9178\n",
            "Epoch 230/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1864 - accuracy: 0.9396 - val_loss: 0.2149 - val_accuracy: 0.9213\n",
            "Epoch 231/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1839 - accuracy: 0.9426 - val_loss: 0.2085 - val_accuracy: 0.9248\n",
            "Epoch 232/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1840 - accuracy: 0.9417 - val_loss: 0.2058 - val_accuracy: 0.9253\n",
            "Epoch 233/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1849 - accuracy: 0.9427 - val_loss: 0.2025 - val_accuracy: 0.9275\n",
            "Epoch 234/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1861 - accuracy: 0.9405 - val_loss: 0.2028 - val_accuracy: 0.9315\n",
            "Epoch 235/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1839 - accuracy: 0.9423 - val_loss: 0.2107 - val_accuracy: 0.9262\n",
            "Epoch 236/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1843 - accuracy: 0.9410 - val_loss: 0.2138 - val_accuracy: 0.9240\n",
            "Epoch 237/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1826 - accuracy: 0.9440 - val_loss: 0.2028 - val_accuracy: 0.9302\n",
            "Epoch 238/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1834 - accuracy: 0.9423 - val_loss: 0.1925 - val_accuracy: 0.9355\n",
            "Epoch 239/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1803 - accuracy: 0.9454 - val_loss: 0.1941 - val_accuracy: 0.9359\n",
            "Epoch 240/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1797 - accuracy: 0.9442 - val_loss: 0.2079 - val_accuracy: 0.9262\n",
            "Epoch 241/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1790 - accuracy: 0.9422 - val_loss: 0.2172 - val_accuracy: 0.9226\n",
            "Epoch 242/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1806 - accuracy: 0.9413 - val_loss: 0.2106 - val_accuracy: 0.9266\n",
            "Epoch 243/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1790 - accuracy: 0.9435 - val_loss: 0.2015 - val_accuracy: 0.9310\n",
            "Epoch 244/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1780 - accuracy: 0.9438 - val_loss: 0.1977 - val_accuracy: 0.9346\n",
            "Epoch 245/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1759 - accuracy: 0.9445 - val_loss: 0.2013 - val_accuracy: 0.9310\n",
            "Epoch 246/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1769 - accuracy: 0.9433 - val_loss: 0.2076 - val_accuracy: 0.9284\n",
            "Epoch 247/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1774 - accuracy: 0.9444 - val_loss: 0.2099 - val_accuracy: 0.9279\n",
            "Epoch 248/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1794 - accuracy: 0.9441 - val_loss: 0.2037 - val_accuracy: 0.9297\n",
            "Epoch 249/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1764 - accuracy: 0.9434 - val_loss: 0.1887 - val_accuracy: 0.9372\n",
            "Epoch 250/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1716 - accuracy: 0.9455 - val_loss: 0.1804 - val_accuracy: 0.9394\n",
            "Epoch 251/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1721 - accuracy: 0.9468 - val_loss: 0.1862 - val_accuracy: 0.9385\n",
            "Epoch 252/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1749 - accuracy: 0.9441 - val_loss: 0.1980 - val_accuracy: 0.9346\n",
            "Epoch 253/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1719 - accuracy: 0.9436 - val_loss: 0.2038 - val_accuracy: 0.9310\n",
            "Epoch 254/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1717 - accuracy: 0.9458 - val_loss: 0.2018 - val_accuracy: 0.9328\n",
            "Epoch 255/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1722 - accuracy: 0.9455 - val_loss: 0.1959 - val_accuracy: 0.9355\n",
            "Epoch 256/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1696 - accuracy: 0.9457 - val_loss: 0.1923 - val_accuracy: 0.9359\n",
            "Epoch 257/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1753 - accuracy: 0.9459 - val_loss: 0.1942 - val_accuracy: 0.9337\n",
            "Epoch 258/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1711 - accuracy: 0.9465 - val_loss: 0.1941 - val_accuracy: 0.9350\n",
            "Epoch 259/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1696 - accuracy: 0.9455 - val_loss: 0.1889 - val_accuracy: 0.9377\n",
            "Epoch 260/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1670 - accuracy: 0.9466 - val_loss: 0.1861 - val_accuracy: 0.9385\n",
            "Epoch 261/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1670 - accuracy: 0.9459 - val_loss: 0.1877 - val_accuracy: 0.9399\n",
            "Epoch 262/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1679 - accuracy: 0.9474 - val_loss: 0.1842 - val_accuracy: 0.9399\n",
            "Epoch 263/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1674 - accuracy: 0.9485 - val_loss: 0.1800 - val_accuracy: 0.9408\n",
            "Epoch 264/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1646 - accuracy: 0.9486 - val_loss: 0.1844 - val_accuracy: 0.9403\n",
            "Epoch 265/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1670 - accuracy: 0.9476 - val_loss: 0.1918 - val_accuracy: 0.9368\n",
            "Epoch 266/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1678 - accuracy: 0.9475 - val_loss: 0.1974 - val_accuracy: 0.9346\n",
            "Epoch 267/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1655 - accuracy: 0.9479 - val_loss: 0.1918 - val_accuracy: 0.9363\n",
            "Epoch 268/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1646 - accuracy: 0.9484 - val_loss: 0.1801 - val_accuracy: 0.9403\n",
            "Epoch 269/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1619 - accuracy: 0.9484 - val_loss: 0.1758 - val_accuracy: 0.9425\n",
            "Epoch 270/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1664 - accuracy: 0.9467 - val_loss: 0.1813 - val_accuracy: 0.9403\n",
            "Epoch 271/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1648 - accuracy: 0.9482 - val_loss: 0.1949 - val_accuracy: 0.9346\n",
            "Epoch 272/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1641 - accuracy: 0.9495 - val_loss: 0.2065 - val_accuracy: 0.9284\n",
            "Epoch 273/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1625 - accuracy: 0.9476 - val_loss: 0.1966 - val_accuracy: 0.9341\n",
            "Epoch 274/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1622 - accuracy: 0.9498 - val_loss: 0.1753 - val_accuracy: 0.9452\n",
            "Epoch 275/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1625 - accuracy: 0.9478 - val_loss: 0.1663 - val_accuracy: 0.9465\n",
            "Epoch 276/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1592 - accuracy: 0.9509 - val_loss: 0.1714 - val_accuracy: 0.9439\n",
            "Epoch 277/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1612 - accuracy: 0.9491 - val_loss: 0.1847 - val_accuracy: 0.9385\n",
            "Epoch 278/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1616 - accuracy: 0.9509 - val_loss: 0.1971 - val_accuracy: 0.9332\n",
            "Epoch 279/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1594 - accuracy: 0.9493 - val_loss: 0.1940 - val_accuracy: 0.9346\n",
            "Epoch 280/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1613 - accuracy: 0.9477 - val_loss: 0.1800 - val_accuracy: 0.9399\n",
            "Epoch 281/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1591 - accuracy: 0.9506 - val_loss: 0.1696 - val_accuracy: 0.9452\n",
            "Epoch 282/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1607 - accuracy: 0.9490 - val_loss: 0.1676 - val_accuracy: 0.9452\n",
            "Epoch 283/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1575 - accuracy: 0.9522 - val_loss: 0.1715 - val_accuracy: 0.9430\n",
            "Epoch 284/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1579 - accuracy: 0.9491 - val_loss: 0.1737 - val_accuracy: 0.9430\n",
            "Epoch 285/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1586 - accuracy: 0.9514 - val_loss: 0.1714 - val_accuracy: 0.9434\n",
            "Epoch 286/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1583 - accuracy: 0.9503 - val_loss: 0.1688 - val_accuracy: 0.9456\n",
            "Epoch 287/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1570 - accuracy: 0.9512 - val_loss: 0.1722 - val_accuracy: 0.9452\n",
            "Epoch 288/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1583 - accuracy: 0.9490 - val_loss: 0.1804 - val_accuracy: 0.9416\n",
            "Epoch 289/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1542 - accuracy: 0.9521 - val_loss: 0.1857 - val_accuracy: 0.9377\n",
            "Epoch 290/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1569 - accuracy: 0.9501 - val_loss: 0.1804 - val_accuracy: 0.9394\n",
            "Epoch 291/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1549 - accuracy: 0.9537 - val_loss: 0.1727 - val_accuracy: 0.9447\n",
            "Epoch 292/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1565 - accuracy: 0.9521 - val_loss: 0.1699 - val_accuracy: 0.9447\n",
            "Epoch 293/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1557 - accuracy: 0.9512 - val_loss: 0.1702 - val_accuracy: 0.9447\n",
            "Epoch 294/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1544 - accuracy: 0.9525 - val_loss: 0.1741 - val_accuracy: 0.9443\n",
            "Epoch 295/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1547 - accuracy: 0.9505 - val_loss: 0.1781 - val_accuracy: 0.9416\n",
            "Epoch 296/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1544 - accuracy: 0.9532 - val_loss: 0.1785 - val_accuracy: 0.9408\n",
            "Epoch 297/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1554 - accuracy: 0.9504 - val_loss: 0.1772 - val_accuracy: 0.9430\n",
            "Epoch 298/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1554 - accuracy: 0.9529 - val_loss: 0.1749 - val_accuracy: 0.9443\n",
            "Epoch 299/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1523 - accuracy: 0.9521 - val_loss: 0.1734 - val_accuracy: 0.9447\n",
            "Epoch 300/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1512 - accuracy: 0.9530 - val_loss: 0.1736 - val_accuracy: 0.9447\n",
            "Epoch 301/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1531 - accuracy: 0.9520 - val_loss: 0.1716 - val_accuracy: 0.9439\n",
            "Epoch 302/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1501 - accuracy: 0.9540 - val_loss: 0.1689 - val_accuracy: 0.9465\n",
            "Epoch 303/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1498 - accuracy: 0.9528 - val_loss: 0.1692 - val_accuracy: 0.9465\n",
            "Epoch 304/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1516 - accuracy: 0.9533 - val_loss: 0.1693 - val_accuracy: 0.9474\n",
            "Epoch 305/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1518 - accuracy: 0.9526 - val_loss: 0.1681 - val_accuracy: 0.9465\n",
            "Epoch 306/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1497 - accuracy: 0.9551 - val_loss: 0.1687 - val_accuracy: 0.9461\n",
            "Epoch 307/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1504 - accuracy: 0.9535 - val_loss: 0.1665 - val_accuracy: 0.9452\n",
            "Epoch 308/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1490 - accuracy: 0.9529 - val_loss: 0.1651 - val_accuracy: 0.9456\n",
            "Epoch 309/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1489 - accuracy: 0.9537 - val_loss: 0.1653 - val_accuracy: 0.9456\n",
            "Epoch 310/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1492 - accuracy: 0.9560 - val_loss: 0.1711 - val_accuracy: 0.9430\n",
            "Epoch 311/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1501 - accuracy: 0.9540 - val_loss: 0.1791 - val_accuracy: 0.9385\n",
            "Epoch 312/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1509 - accuracy: 0.9535 - val_loss: 0.1798 - val_accuracy: 0.9403\n",
            "Epoch 313/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1502 - accuracy: 0.9538 - val_loss: 0.1735 - val_accuracy: 0.9439\n",
            "Epoch 314/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1512 - accuracy: 0.9525 - val_loss: 0.1637 - val_accuracy: 0.9474\n",
            "Epoch 315/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1500 - accuracy: 0.9527 - val_loss: 0.1577 - val_accuracy: 0.9500\n",
            "Epoch 316/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1465 - accuracy: 0.9545 - val_loss: 0.1586 - val_accuracy: 0.9487\n",
            "Epoch 317/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1449 - accuracy: 0.9545 - val_loss: 0.1643 - val_accuracy: 0.9469\n",
            "Epoch 318/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1460 - accuracy: 0.9546 - val_loss: 0.1724 - val_accuracy: 0.9443\n",
            "Epoch 319/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1450 - accuracy: 0.9556 - val_loss: 0.1740 - val_accuracy: 0.9443\n",
            "Epoch 320/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1495 - accuracy: 0.9547 - val_loss: 0.1686 - val_accuracy: 0.9478\n",
            "Epoch 321/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1508 - accuracy: 0.9536 - val_loss: 0.1623 - val_accuracy: 0.9505\n",
            "Epoch 322/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1467 - accuracy: 0.9539 - val_loss: 0.1578 - val_accuracy: 0.9500\n",
            "Epoch 323/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1443 - accuracy: 0.9560 - val_loss: 0.1573 - val_accuracy: 0.9496\n",
            "Epoch 324/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1441 - accuracy: 0.9550 - val_loss: 0.1585 - val_accuracy: 0.9487\n",
            "Epoch 325/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1463 - accuracy: 0.9538 - val_loss: 0.1610 - val_accuracy: 0.9487\n",
            "Epoch 326/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1471 - accuracy: 0.9552 - val_loss: 0.1670 - val_accuracy: 0.9474\n",
            "Epoch 327/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1458 - accuracy: 0.9567 - val_loss: 0.1701 - val_accuracy: 0.9465\n",
            "Epoch 328/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1433 - accuracy: 0.9576 - val_loss: 0.1649 - val_accuracy: 0.9505\n",
            "Epoch 329/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1466 - accuracy: 0.9546 - val_loss: 0.1545 - val_accuracy: 0.9531\n",
            "Epoch 330/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1417 - accuracy: 0.9577 - val_loss: 0.1497 - val_accuracy: 0.9540\n",
            "Epoch 331/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1444 - accuracy: 0.9561 - val_loss: 0.1545 - val_accuracy: 0.9518\n",
            "Epoch 332/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1444 - accuracy: 0.9558 - val_loss: 0.1633 - val_accuracy: 0.9474\n",
            "Epoch 333/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1401 - accuracy: 0.9560 - val_loss: 0.1644 - val_accuracy: 0.9465\n",
            "Epoch 334/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1435 - accuracy: 0.9558 - val_loss: 0.1595 - val_accuracy: 0.9483\n",
            "Epoch 335/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1422 - accuracy: 0.9550 - val_loss: 0.1553 - val_accuracy: 0.9514\n",
            "Epoch 336/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1446 - accuracy: 0.9557 - val_loss: 0.1590 - val_accuracy: 0.9500\n",
            "Epoch 337/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1425 - accuracy: 0.9588 - val_loss: 0.1660 - val_accuracy: 0.9474\n",
            "Epoch 338/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1401 - accuracy: 0.9580 - val_loss: 0.1601 - val_accuracy: 0.9505\n",
            "Epoch 339/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1425 - accuracy: 0.9569 - val_loss: 0.1522 - val_accuracy: 0.9536\n",
            "Epoch 340/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1431 - accuracy: 0.9574 - val_loss: 0.1489 - val_accuracy: 0.9558\n",
            "Epoch 341/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1420 - accuracy: 0.9568 - val_loss: 0.1527 - val_accuracy: 0.9545\n",
            "Epoch 342/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1394 - accuracy: 0.9591 - val_loss: 0.1582 - val_accuracy: 0.9518\n",
            "Epoch 343/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1397 - accuracy: 0.9578 - val_loss: 0.1584 - val_accuracy: 0.9492\n",
            "Epoch 344/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1386 - accuracy: 0.9579 - val_loss: 0.1545 - val_accuracy: 0.9509\n",
            "Epoch 345/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1398 - accuracy: 0.9568 - val_loss: 0.1550 - val_accuracy: 0.9500\n",
            "Epoch 346/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1415 - accuracy: 0.9576 - val_loss: 0.1588 - val_accuracy: 0.9496\n",
            "Epoch 347/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1338 - accuracy: 0.9588 - val_loss: 0.1570 - val_accuracy: 0.9518\n",
            "Epoch 348/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1363 - accuracy: 0.9595 - val_loss: 0.1512 - val_accuracy: 0.9540\n",
            "Epoch 349/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1375 - accuracy: 0.9592 - val_loss: 0.1453 - val_accuracy: 0.9562\n",
            "Epoch 350/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1392 - accuracy: 0.9568 - val_loss: 0.1426 - val_accuracy: 0.9580\n",
            "Epoch 351/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1403 - accuracy: 0.9587 - val_loss: 0.1448 - val_accuracy: 0.9567\n",
            "Epoch 352/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1359 - accuracy: 0.9577 - val_loss: 0.1502 - val_accuracy: 0.9545\n",
            "Epoch 353/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1365 - accuracy: 0.9594 - val_loss: 0.1544 - val_accuracy: 0.9540\n",
            "Epoch 354/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1380 - accuracy: 0.9589 - val_loss: 0.1551 - val_accuracy: 0.9536\n",
            "Epoch 355/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1389 - accuracy: 0.9569 - val_loss: 0.1492 - val_accuracy: 0.9558\n",
            "Epoch 356/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1349 - accuracy: 0.9588 - val_loss: 0.1458 - val_accuracy: 0.9567\n",
            "Epoch 357/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1382 - accuracy: 0.9573 - val_loss: 0.1498 - val_accuracy: 0.9545\n",
            "Epoch 358/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1353 - accuracy: 0.9583 - val_loss: 0.1550 - val_accuracy: 0.9545\n",
            "Epoch 359/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1372 - accuracy: 0.9584 - val_loss: 0.1577 - val_accuracy: 0.9527\n",
            "Epoch 360/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1369 - accuracy: 0.9587 - val_loss: 0.1524 - val_accuracy: 0.9549\n",
            "Epoch 361/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1372 - accuracy: 0.9568 - val_loss: 0.1420 - val_accuracy: 0.9580\n",
            "Epoch 362/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1349 - accuracy: 0.9584 - val_loss: 0.1375 - val_accuracy: 0.9607\n",
            "Epoch 363/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1374 - accuracy: 0.9584 - val_loss: 0.1434 - val_accuracy: 0.9589\n",
            "Epoch 364/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1325 - accuracy: 0.9579 - val_loss: 0.1531 - val_accuracy: 0.9558\n",
            "Epoch 365/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1344 - accuracy: 0.9594 - val_loss: 0.1526 - val_accuracy: 0.9562\n",
            "Epoch 366/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1326 - accuracy: 0.9590 - val_loss: 0.1485 - val_accuracy: 0.9584\n",
            "Epoch 367/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1341 - accuracy: 0.9592 - val_loss: 0.1476 - val_accuracy: 0.9584\n",
            "Epoch 368/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1364 - accuracy: 0.9601 - val_loss: 0.1504 - val_accuracy: 0.9567\n",
            "Epoch 369/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1344 - accuracy: 0.9594 - val_loss: 0.1516 - val_accuracy: 0.9567\n",
            "Epoch 370/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1332 - accuracy: 0.9576 - val_loss: 0.1486 - val_accuracy: 0.9553\n",
            "Epoch 371/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1327 - accuracy: 0.9587 - val_loss: 0.1461 - val_accuracy: 0.9558\n",
            "Epoch 372/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1323 - accuracy: 0.9600 - val_loss: 0.1508 - val_accuracy: 0.9545\n",
            "Epoch 373/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1326 - accuracy: 0.9584 - val_loss: 0.1580 - val_accuracy: 0.9500\n",
            "Epoch 374/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1343 - accuracy: 0.9594 - val_loss: 0.1594 - val_accuracy: 0.9492\n",
            "Epoch 375/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1361 - accuracy: 0.9570 - val_loss: 0.1517 - val_accuracy: 0.9545\n",
            "Epoch 376/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1335 - accuracy: 0.9602 - val_loss: 0.1444 - val_accuracy: 0.9584\n",
            "Epoch 377/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1328 - accuracy: 0.9601 - val_loss: 0.1408 - val_accuracy: 0.9602\n",
            "Epoch 378/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1358 - accuracy: 0.9567 - val_loss: 0.1429 - val_accuracy: 0.9602\n",
            "Epoch 379/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1332 - accuracy: 0.9595 - val_loss: 0.1480 - val_accuracy: 0.9584\n",
            "Epoch 380/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1316 - accuracy: 0.9597 - val_loss: 0.1489 - val_accuracy: 0.9576\n",
            "Epoch 381/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1332 - accuracy: 0.9572 - val_loss: 0.1433 - val_accuracy: 0.9602\n",
            "Epoch 382/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1310 - accuracy: 0.9594 - val_loss: 0.1381 - val_accuracy: 0.9607\n",
            "Epoch 383/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1299 - accuracy: 0.9614 - val_loss: 0.1413 - val_accuracy: 0.9602\n",
            "Epoch 384/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1308 - accuracy: 0.9602 - val_loss: 0.1523 - val_accuracy: 0.9549\n",
            "Epoch 385/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1305 - accuracy: 0.9601 - val_loss: 0.1560 - val_accuracy: 0.9509\n",
            "Epoch 386/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1298 - accuracy: 0.9601 - val_loss: 0.1484 - val_accuracy: 0.9540\n",
            "Epoch 387/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1305 - accuracy: 0.9612 - val_loss: 0.1397 - val_accuracy: 0.9584\n",
            "Epoch 388/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1299 - accuracy: 0.9594 - val_loss: 0.1393 - val_accuracy: 0.9593\n",
            "Epoch 389/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1303 - accuracy: 0.9609 - val_loss: 0.1472 - val_accuracy: 0.9571\n",
            "Epoch 390/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1264 - accuracy: 0.9620 - val_loss: 0.1577 - val_accuracy: 0.9527\n",
            "Epoch 391/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1317 - accuracy: 0.9588 - val_loss: 0.1530 - val_accuracy: 0.9558\n",
            "Epoch 392/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1309 - accuracy: 0.9600 - val_loss: 0.1381 - val_accuracy: 0.9611\n",
            "Epoch 393/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1291 - accuracy: 0.9624 - val_loss: 0.1325 - val_accuracy: 0.9620\n",
            "Epoch 394/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1317 - accuracy: 0.9604 - val_loss: 0.1383 - val_accuracy: 0.9611\n",
            "Epoch 395/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1263 - accuracy: 0.9616 - val_loss: 0.1498 - val_accuracy: 0.9562\n",
            "Epoch 396/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1281 - accuracy: 0.9619 - val_loss: 0.1523 - val_accuracy: 0.9540\n",
            "Epoch 397/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1280 - accuracy: 0.9600 - val_loss: 0.1436 - val_accuracy: 0.9576\n",
            "Epoch 398/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1283 - accuracy: 0.9612 - val_loss: 0.1384 - val_accuracy: 0.9589\n",
            "Epoch 399/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1231 - accuracy: 0.9609 - val_loss: 0.1420 - val_accuracy: 0.9593\n",
            "Epoch 400/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1282 - accuracy: 0.9618 - val_loss: 0.1456 - val_accuracy: 0.9576\n",
            "Epoch 401/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1287 - accuracy: 0.9603 - val_loss: 0.1449 - val_accuracy: 0.9589\n",
            "Epoch 402/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1301 - accuracy: 0.9592 - val_loss: 0.1429 - val_accuracy: 0.9611\n",
            "Epoch 403/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1279 - accuracy: 0.9600 - val_loss: 0.1439 - val_accuracy: 0.9593\n",
            "Epoch 404/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1266 - accuracy: 0.9604 - val_loss: 0.1433 - val_accuracy: 0.9598\n",
            "Epoch 405/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1272 - accuracy: 0.9611 - val_loss: 0.1388 - val_accuracy: 0.9615\n",
            "Epoch 406/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1278 - accuracy: 0.9593 - val_loss: 0.1343 - val_accuracy: 0.9620\n",
            "Epoch 407/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1249 - accuracy: 0.9624 - val_loss: 0.1317 - val_accuracy: 0.9624\n",
            "Epoch 408/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1282 - accuracy: 0.9610 - val_loss: 0.1329 - val_accuracy: 0.9637\n",
            "Epoch 409/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1287 - accuracy: 0.9611 - val_loss: 0.1391 - val_accuracy: 0.9615\n",
            "Epoch 410/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1270 - accuracy: 0.9618 - val_loss: 0.1415 - val_accuracy: 0.9607\n",
            "Epoch 411/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1250 - accuracy: 0.9622 - val_loss: 0.1386 - val_accuracy: 0.9611\n",
            "Epoch 412/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1235 - accuracy: 0.9620 - val_loss: 0.1356 - val_accuracy: 0.9620\n",
            "Epoch 413/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1222 - accuracy: 0.9619 - val_loss: 0.1344 - val_accuracy: 0.9624\n",
            "Epoch 414/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1250 - accuracy: 0.9624 - val_loss: 0.1361 - val_accuracy: 0.9620\n",
            "Epoch 415/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1251 - accuracy: 0.9614 - val_loss: 0.1397 - val_accuracy: 0.9607\n",
            "Epoch 416/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1245 - accuracy: 0.9627 - val_loss: 0.1452 - val_accuracy: 0.9589\n",
            "Epoch 417/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1243 - accuracy: 0.9623 - val_loss: 0.1483 - val_accuracy: 0.9580\n",
            "Epoch 418/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1240 - accuracy: 0.9619 - val_loss: 0.1438 - val_accuracy: 0.9580\n",
            "Epoch 419/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1252 - accuracy: 0.9631 - val_loss: 0.1342 - val_accuracy: 0.9611\n",
            "Epoch 420/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1265 - accuracy: 0.9610 - val_loss: 0.1298 - val_accuracy: 0.9624\n",
            "Epoch 421/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1237 - accuracy: 0.9632 - val_loss: 0.1339 - val_accuracy: 0.9620\n",
            "Epoch 422/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1244 - accuracy: 0.9616 - val_loss: 0.1430 - val_accuracy: 0.9580\n",
            "Epoch 423/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1229 - accuracy: 0.9622 - val_loss: 0.1466 - val_accuracy: 0.9567\n",
            "Epoch 424/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1239 - accuracy: 0.9634 - val_loss: 0.1416 - val_accuracy: 0.9589\n",
            "Epoch 425/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1229 - accuracy: 0.9606 - val_loss: 0.1331 - val_accuracy: 0.9624\n",
            "Epoch 426/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1203 - accuracy: 0.9616 - val_loss: 0.1285 - val_accuracy: 0.9633\n",
            "Epoch 427/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1233 - accuracy: 0.9612 - val_loss: 0.1342 - val_accuracy: 0.9620\n",
            "Epoch 428/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1227 - accuracy: 0.9630 - val_loss: 0.1457 - val_accuracy: 0.9584\n",
            "Epoch 429/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1259 - accuracy: 0.9613 - val_loss: 0.1483 - val_accuracy: 0.9567\n",
            "Epoch 430/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1232 - accuracy: 0.9619 - val_loss: 0.1411 - val_accuracy: 0.9580\n",
            "Epoch 431/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1208 - accuracy: 0.9639 - val_loss: 0.1328 - val_accuracy: 0.9602\n",
            "Epoch 432/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1255 - accuracy: 0.9610 - val_loss: 0.1306 - val_accuracy: 0.9615\n",
            "Epoch 433/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1236 - accuracy: 0.9629 - val_loss: 0.1337 - val_accuracy: 0.9620\n",
            "Epoch 434/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1199 - accuracy: 0.9626 - val_loss: 0.1352 - val_accuracy: 0.9624\n",
            "Epoch 435/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1217 - accuracy: 0.9627 - val_loss: 0.1315 - val_accuracy: 0.9629\n",
            "Epoch 436/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1213 - accuracy: 0.9624 - val_loss: 0.1250 - val_accuracy: 0.9651\n",
            "Epoch 437/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1221 - accuracy: 0.9629 - val_loss: 0.1270 - val_accuracy: 0.9633\n",
            "Epoch 438/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1194 - accuracy: 0.9643 - val_loss: 0.1381 - val_accuracy: 0.9611\n",
            "Epoch 439/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1199 - accuracy: 0.9632 - val_loss: 0.1477 - val_accuracy: 0.9576\n",
            "Epoch 440/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1226 - accuracy: 0.9620 - val_loss: 0.1436 - val_accuracy: 0.9584\n",
            "Epoch 441/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1219 - accuracy: 0.9625 - val_loss: 0.1338 - val_accuracy: 0.9620\n",
            "Epoch 442/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1205 - accuracy: 0.9621 - val_loss: 0.1253 - val_accuracy: 0.9637\n",
            "Epoch 443/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1170 - accuracy: 0.9637 - val_loss: 0.1215 - val_accuracy: 0.9646\n",
            "Epoch 444/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1198 - accuracy: 0.9639 - val_loss: 0.1269 - val_accuracy: 0.9637\n",
            "Epoch 445/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1174 - accuracy: 0.9647 - val_loss: 0.1383 - val_accuracy: 0.9615\n",
            "Epoch 446/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1223 - accuracy: 0.9612 - val_loss: 0.1450 - val_accuracy: 0.9602\n",
            "Epoch 447/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1245 - accuracy: 0.9611 - val_loss: 0.1405 - val_accuracy: 0.9593\n",
            "Epoch 448/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1203 - accuracy: 0.9634 - val_loss: 0.1316 - val_accuracy: 0.9620\n",
            "Epoch 449/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1196 - accuracy: 0.9635 - val_loss: 0.1262 - val_accuracy: 0.9637\n",
            "Epoch 450/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1200 - accuracy: 0.9625 - val_loss: 0.1258 - val_accuracy: 0.9655\n",
            "Epoch 451/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1189 - accuracy: 0.9644 - val_loss: 0.1298 - val_accuracy: 0.9642\n",
            "Epoch 452/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1194 - accuracy: 0.9640 - val_loss: 0.1345 - val_accuracy: 0.9611\n",
            "Epoch 453/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1179 - accuracy: 0.9648 - val_loss: 0.1343 - val_accuracy: 0.9589\n",
            "Epoch 454/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1194 - accuracy: 0.9637 - val_loss: 0.1344 - val_accuracy: 0.9584\n",
            "Epoch 455/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1164 - accuracy: 0.9653 - val_loss: 0.1422 - val_accuracy: 0.9567\n",
            "Epoch 456/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1194 - accuracy: 0.9631 - val_loss: 0.1454 - val_accuracy: 0.9567\n",
            "Epoch 457/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1185 - accuracy: 0.9632 - val_loss: 0.1363 - val_accuracy: 0.9611\n",
            "Epoch 458/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1176 - accuracy: 0.9636 - val_loss: 0.1278 - val_accuracy: 0.9637\n",
            "Epoch 459/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1166 - accuracy: 0.9640 - val_loss: 0.1262 - val_accuracy: 0.9651\n",
            "Epoch 460/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1169 - accuracy: 0.9640 - val_loss: 0.1281 - val_accuracy: 0.9646\n",
            "Epoch 461/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1168 - accuracy: 0.9639 - val_loss: 0.1300 - val_accuracy: 0.9637\n",
            "Epoch 462/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1158 - accuracy: 0.9654 - val_loss: 0.1327 - val_accuracy: 0.9615\n",
            "Epoch 463/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1146 - accuracy: 0.9646 - val_loss: 0.1356 - val_accuracy: 0.9602\n",
            "Epoch 464/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1159 - accuracy: 0.9642 - val_loss: 0.1330 - val_accuracy: 0.9620\n",
            "Epoch 465/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1169 - accuracy: 0.9627 - val_loss: 0.1273 - val_accuracy: 0.9642\n",
            "Epoch 466/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1165 - accuracy: 0.9632 - val_loss: 0.1242 - val_accuracy: 0.9668\n",
            "Epoch 467/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1162 - accuracy: 0.9630 - val_loss: 0.1226 - val_accuracy: 0.9651\n",
            "Epoch 468/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1157 - accuracy: 0.9640 - val_loss: 0.1245 - val_accuracy: 0.9646\n",
            "Epoch 469/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1179 - accuracy: 0.9609 - val_loss: 0.1299 - val_accuracy: 0.9642\n",
            "Epoch 470/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1149 - accuracy: 0.9635 - val_loss: 0.1319 - val_accuracy: 0.9633\n",
            "Epoch 471/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1131 - accuracy: 0.9652 - val_loss: 0.1297 - val_accuracy: 0.9651\n",
            "Epoch 472/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1139 - accuracy: 0.9643 - val_loss: 0.1254 - val_accuracy: 0.9651\n",
            "Epoch 473/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1144 - accuracy: 0.9647 - val_loss: 0.1234 - val_accuracy: 0.9655\n",
            "Epoch 474/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1143 - accuracy: 0.9655 - val_loss: 0.1260 - val_accuracy: 0.9637\n",
            "Epoch 475/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1157 - accuracy: 0.9648 - val_loss: 0.1268 - val_accuracy: 0.9660\n",
            "Epoch 476/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1192 - accuracy: 0.9624 - val_loss: 0.1251 - val_accuracy: 0.9660\n",
            "Epoch 477/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1149 - accuracy: 0.9639 - val_loss: 0.1262 - val_accuracy: 0.9660\n",
            "Epoch 478/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1152 - accuracy: 0.9645 - val_loss: 0.1266 - val_accuracy: 0.9660\n",
            "Epoch 479/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1131 - accuracy: 0.9651 - val_loss: 0.1247 - val_accuracy: 0.9660\n",
            "Epoch 480/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1116 - accuracy: 0.9652 - val_loss: 0.1239 - val_accuracy: 0.9655\n",
            "Epoch 481/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1153 - accuracy: 0.9641 - val_loss: 0.1226 - val_accuracy: 0.9655\n",
            "Epoch 482/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1120 - accuracy: 0.9663 - val_loss: 0.1237 - val_accuracy: 0.9664\n",
            "Epoch 483/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1113 - accuracy: 0.9655 - val_loss: 0.1268 - val_accuracy: 0.9655\n",
            "Epoch 484/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1125 - accuracy: 0.9656 - val_loss: 0.1254 - val_accuracy: 0.9655\n",
            "Epoch 485/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1119 - accuracy: 0.9661 - val_loss: 0.1223 - val_accuracy: 0.9655\n",
            "Epoch 486/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1121 - accuracy: 0.9654 - val_loss: 0.1243 - val_accuracy: 0.9642\n",
            "Epoch 487/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1135 - accuracy: 0.9646 - val_loss: 0.1269 - val_accuracy: 0.9637\n",
            "Epoch 488/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1147 - accuracy: 0.9633 - val_loss: 0.1257 - val_accuracy: 0.9655\n",
            "Epoch 489/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1106 - accuracy: 0.9656 - val_loss: 0.1228 - val_accuracy: 0.9660\n",
            "Epoch 490/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1103 - accuracy: 0.9647 - val_loss: 0.1198 - val_accuracy: 0.9664\n",
            "Epoch 491/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1125 - accuracy: 0.9642 - val_loss: 0.1197 - val_accuracy: 0.9660\n",
            "Epoch 492/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1107 - accuracy: 0.9668 - val_loss: 0.1201 - val_accuracy: 0.9664\n",
            "Epoch 493/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1122 - accuracy: 0.9634 - val_loss: 0.1250 - val_accuracy: 0.9660\n",
            "Epoch 494/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1103 - accuracy: 0.9654 - val_loss: 0.1327 - val_accuracy: 0.9633\n",
            "Epoch 495/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1101 - accuracy: 0.9652 - val_loss: 0.1323 - val_accuracy: 0.9633\n",
            "Epoch 496/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1123 - accuracy: 0.9642 - val_loss: 0.1271 - val_accuracy: 0.9637\n",
            "Epoch 497/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1116 - accuracy: 0.9656 - val_loss: 0.1211 - val_accuracy: 0.9660\n",
            "Epoch 498/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1103 - accuracy: 0.9645 - val_loss: 0.1173 - val_accuracy: 0.9664\n",
            "Epoch 499/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1130 - accuracy: 0.9641 - val_loss: 0.1178 - val_accuracy: 0.9668\n",
            "Epoch 500/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1120 - accuracy: 0.9652 - val_loss: 0.1230 - val_accuracy: 0.9646\n",
            "\n",
            "Score:  [0.1094975620508194, 0.9668610692024231]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rG7B86EL68c",
        "outputId": "3ead7e27-cc40-4238-d88d-114de604bba2"
      },
      "source": [
        "np.round(model.predict(dat_resaXredts))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rY0fqvq1L68c",
        "outputId": "0658a603-e8cf-4c4b-c9cf-18cdbcbd0a2a"
      },
      "source": [
        "ffnn_res=pd.DataFrame(dat_resaXredts)\n",
        "ffnn_res['Disease']=dat_resaYts\n",
        "ffnn_res=pd.concat([ffnn_res,pd.DataFrame(np.round(model.predict(dat_resaXredts))).astype(int)],axis=1)\n",
        "ffnn_res"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>Disease</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.78</td>\n",
              "      <td>5.13e-01</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.09</td>\n",
              "      <td>...</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.51</td>\n",
              "      <td>8.04e-01</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.58</td>\n",
              "      <td>breast</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.70</td>\n",
              "      <td>6.46e-01</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.13</td>\n",
              "      <td>...</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.73</td>\n",
              "      <td>8.81e-01</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.55</td>\n",
              "      <td>breast</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1.22e-01</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.53</td>\n",
              "      <td>...</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.24</td>\n",
              "      <td>9.35e-03</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.07</td>\n",
              "      <td>leukaemia</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.21e-01</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.63</td>\n",
              "      <td>...</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>4.02e-02</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.14</td>\n",
              "      <td>leukaemia</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.09</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.18</td>\n",
              "      <td>7.18e-03</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.32</td>\n",
              "      <td>...</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.37</td>\n",
              "      <td>6.91e-01</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.08</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3767</th>\n",
              "      <td>0.13</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>7.82e-02</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.73</td>\n",
              "      <td>...</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.44</td>\n",
              "      <td>7.39e-02</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.12</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3768</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.18</td>\n",
              "      <td>2.12e-01</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.22</td>\n",
              "      <td>...</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>5.32e-02</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.21</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3769</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.42</td>\n",
              "      <td>3.97e-01</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.32</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.39</td>\n",
              "      <td>3.92e-01</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.17</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3770</th>\n",
              "      <td>0.13</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.25</td>\n",
              "      <td>8.20e-02</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.35</td>\n",
              "      <td>...</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.35</td>\n",
              "      <td>6.23e-01</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.11</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3771</th>\n",
              "      <td>0.78</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.11</td>\n",
              "      <td>6.53e-01</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.11</td>\n",
              "      <td>...</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.48</td>\n",
              "      <td>5.94e-01</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.27</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3772 rows × 105 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0     1     2     3     4     5  ...    99    Disease  0  1  2  3\n",
              "0     0.80  0.80  0.73  0.52  0.67  0.44  ...  0.58     breast  1  0  0  0\n",
              "1     0.47  0.49  0.86  0.63  0.80  0.73  ...  0.55     breast  1  0  0  0\n",
              "2     0.04  0.08  0.07  0.18  0.27  0.16  ...  0.07  leukaemia  0  1  0  0\n",
              "3     0.09  0.11  0.04  0.14  0.16  0.14  ...  0.14  leukaemia  0  1  0  0\n",
              "4     0.09  0.08  0.20  0.51  0.59  0.48  ...  0.08     normal  0  0  0  1\n",
              "...    ...   ...   ...   ...   ...   ...  ...   ...        ... .. .. .. ..\n",
              "3767  0.13  0.13  0.11  0.17  0.08  0.12  ...  0.12     normal  0  0  0  1\n",
              "3768  0.10  0.19  0.08  0.19  0.21  0.18  ...  0.21     normal  0  0  0  1\n",
              "3769  0.22  0.30  0.48  0.23  0.38  0.23  ...  0.17     normal  0  0  0  1\n",
              "3770  0.13  0.12  0.47  0.49  0.61  0.59  ...  0.11     normal  0  0  0  1\n",
              "3771  0.78  0.56  0.67  0.68  0.72  0.67  ...  0.27     normal  0  0  0  1\n",
              "\n",
              "[3772 rows x 105 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0qKnudIL68c"
      },
      "source": [
        "#### pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMSrNpPuL68c"
      },
      "source": [
        "##### logistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S_VqzxWL68c"
      },
      "source": [
        "my_param_grid = [\n",
        "    {'solver': ['newton-cg', 'lbfgs', 'saga'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l2'], 'max_iter': [200]},\n",
        "    {'solver': ['liblinear'], 'C': [100.0, 1.0, 1e-5, 1e-3], 'penalty': ['l1', 'l2'], 'max_iter': [200]}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQQVNPpmL68c"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1t3zQstL68c"
      },
      "source": [
        "modellr = GridSearchCV(estimator=LogisticRegression(n_jobs=-1), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WJLn2l8L68c",
        "outputId": "5c06fab3-ae42-4606-a821-dfe8b79ba8ef"
      },
      "source": [
        "modellr.fit(dat_resaXpcatr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
            "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             estimator=LogisticRegression(n_jobs=-1),\n",
              "             param_grid=[{'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l2'],\n",
              "                          'solver': ['newton-cg', 'lbfgs', 'saga']},\n",
              "                         {'C': [100.0, 1.0, 1e-05, 0.001], 'max_iter': [200],\n",
              "                          'penalty': ['l1', 'l2'], 'solver': ['liblinear']}],\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbjTNwZ3L68d",
        "outputId": "f4ca979b-4f41-4f6b-ca7b-42036d6fb234"
      },
      "source": [
        "print(cross_val_score(modellr.best_estimator_,dat_resaXpcatr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.06177887 -0.04340563 -0.04004635 -0.05497668 -0.03335678 -0.05466626\n",
            " -0.03411894 -0.04856662 -0.03345194 -0.05012339 -0.05112329 -0.0484409\n",
            " -0.03706098 -0.03529167 -0.05363174 -0.05511919 -0.03009381 -0.06095259\n",
            " -0.03786502 -0.0465153  -0.0445084  -0.03239723 -0.05603007 -0.06046039\n",
            " -0.05193588 -0.05285598 -0.06440316 -0.0370718  -0.03255818 -0.03403229\n",
            " -0.05704296 -0.03066989 -0.05082273 -0.03831941 -0.05324556 -0.05740185\n",
            " -0.04309539 -0.03752202 -0.04403688 -0.03892566 -0.05266898 -0.05173119\n",
            " -0.04299965 -0.03653459 -0.0478803  -0.04156085 -0.04436504 -0.0487426\n",
            " -0.05205583 -0.04380332]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l0m1y9sL68d",
        "outputId": "5c73044c-63db-4586-8861-be080cc39c0a"
      },
      "source": [
        "modellr.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0458071591132997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbD2dBjeL68d",
        "outputId": "28aa3c35-ae5d-44cb-859d-9b66ff2a6f89"
      },
      "source": [
        "log_loss(dat_resaYts, modellr.predict_proba(dat_resaXpcats))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05734191277518833"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8BgdNl0L68d"
      },
      "source": [
        "modellr_best = modellr.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77ICu0VtL68d",
        "outputId": "6219b11a-afac-4668-b068-150b58955cb8"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modellr_best.predict(dat_resaXpcatr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       1.00      1.00      1.00      2827\n",
            "   leukaemia       0.99      0.99      0.99      2827\n",
            "    lymphoma       1.00      1.00      1.00      2827\n",
            "      normal       0.99      0.99      0.99      2827\n",
            "\n",
            "    accuracy                           0.99     11308\n",
            "   macro avg       0.99      0.99      0.99     11308\n",
            "weighted avg       0.99      0.99      0.99     11308\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0jhegrYwrec",
        "outputId": "4adf33fe-4533-41e4-b35d-ce3cace71a97"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modellr_best.predict(dat_resaXpcats)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.98      0.99      0.99       943\n",
            "   leukaemia       0.98      0.98      0.98       943\n",
            "    lymphoma       0.99      1.00      0.99       943\n",
            "      normal       0.98      0.96      0.97       943\n",
            "\n",
            "    accuracy                           0.98      3772\n",
            "   macro avg       0.98      0.98      0.98      3772\n",
            "weighted avg       0.98      0.98      0.98      3772\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFspPTh4UK4Z"
      },
      "source": [
        "##### random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj_ppPj7UK4a"
      },
      "source": [
        "my_param_grid = {'bootstrap': [True, False], \n",
        "                 'n_estimators': [10, 50], \n",
        "                 'min_samples_leaf': [20, 40, 60],\n",
        "                 'min_weight_fraction_leaf': [0.01, 0.02, 0.05],\n",
        "                 'criterion': ['gini', 'entropy'], \n",
        "                 'min_impurity_decrease': [1e-5, 1e-6, 1e-7]\n",
        "                 }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJL2tqSFUK4a"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwU4Qe81UK4a"
      },
      "source": [
        "modelrf = GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1,warm_start=True), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WuglKbWUK4a",
        "outputId": "7bf7f0ff-f405-4bcd-a784-a6a686dbc419"
      },
      "source": [
        "modelrf.fit(dat_resaXpcatr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             estimator=RandomForestClassifier(n_jobs=-1, warm_start=True),\n",
              "             param_grid={'bootstrap': [True, False],\n",
              "                         'criterion': ['gini', 'entropy'],\n",
              "                         'min_impurity_decrease': [1e-05, 1e-06, 1e-07],\n",
              "                         'min_samples_leaf': [20, 40, 60],\n",
              "                         'min_weight_fraction_leaf': [0.01, 0.02, 0.05],\n",
              "                         'n_estimators': [10, 50]},\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzDkijPfUK4b",
        "outputId": "5a0869ec-cfe7-4f4e-8f53-fe7a48700b98"
      },
      "source": [
        "print(cross_val_score(modelrf.best_estimator_,dat_resaXpcatr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.40374408 -0.37609308 -0.39367208 -0.39424513 -0.39194156 -0.40480649\n",
            " -0.37937466 -0.3846699  -0.37645221 -0.40762874 -0.36478541 -0.38982932\n",
            " -0.37197505 -0.40351093 -0.38525334 -0.40997579 -0.37624132 -0.38810251\n",
            " -0.39457821 -0.3997106  -0.3985016  -0.3812319  -0.39415409 -0.37081563\n",
            " -0.39786136 -0.37133142 -0.39629568 -0.39257284 -0.37159464 -0.39800645\n",
            " -0.39637349 -0.38438862 -0.38588394 -0.38808024 -0.37286051 -0.39730151\n",
            " -0.37904248 -0.39554715 -0.37084892 -0.36335351 -0.37577692 -0.3887238\n",
            " -0.40384654 -0.37105663 -0.39664209 -0.38648903 -0.37030487 -0.38890668\n",
            " -0.40085905 -0.37889676]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbCtPraAUK4c",
        "outputId": "68b1c78d-fe2b-477a-8f73-b645b2aa79ed"
      },
      "source": [
        "-modelrf.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38232130344473114"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT_IKIZaUK4c",
        "outputId": "d59dd5ff-cd15-4066-b6e8-2878ec4b2563"
      },
      "source": [
        "log_loss(dat_resaYts, modelrf.predict_proba(dat_resaXpcats))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3873202582781111"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sssqu4EOUK4c"
      },
      "source": [
        "modelrf_best = modelrf.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE_UgUc5UK4c",
        "outputId": "3ddebc7a-922f-4499-b660-d2687efd8569"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modelrf_best.predict(dat_resaXpcatr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.98      0.98      0.98      2827\n",
            "   leukaemia       0.95      0.98      0.96      2827\n",
            "    lymphoma       0.97      0.98      0.97      2827\n",
            "      normal       0.96      0.92      0.94      2827\n",
            "\n",
            "    accuracy                           0.96     11308\n",
            "   macro avg       0.96      0.96      0.96     11308\n",
            "weighted avg       0.96      0.96      0.96     11308\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wWo129gUK4d",
        "outputId": "05e1960b-e05a-474b-9a0f-78043444ac8b"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modelrf_best.predict(dat_resaXpcats)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.97      0.97      0.97       943\n",
            "   leukaemia       0.93      0.97      0.95       943\n",
            "    lymphoma       0.97      0.97      0.97       943\n",
            "      normal       0.94      0.91      0.93       943\n",
            "\n",
            "    accuracy                           0.96      3772\n",
            "   macro avg       0.96      0.96      0.96      3772\n",
            "weighted avg       0.96      0.96      0.96      3772\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFhDJWKEUK4d"
      },
      "source": [
        "##### gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDrFUE0cUK4d"
      },
      "source": [
        "my_param_grid = {\n",
        "    \"loss\":[\"deviance\"],\n",
        "    \"learning_rate\": [0.2],\n",
        "    \"min_samples_split\": np.linspace(0.1, 0.5, 1),\n",
        "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 1),\n",
        "    \"max_depth\":[3,8],\n",
        "    \"max_features\":[\"log2\",\"sqrt\"],\n",
        "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
        "    \"subsample\":[0.8],\n",
        "    \"n_estimators\":[30]\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckxH6NzhUK4d"
      },
      "source": [
        "my_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T782UyhUK4d"
      },
      "source": [
        "modelxgb = GridSearchCV(estimator=GradientBoostingClassifier(warm_start=True), \n",
        "                           param_grid=my_param_grid, \n",
        "                           cv=my_cv, \n",
        "                           scoring='neg_log_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bOSnpePUK4d",
        "outputId": "c98e1f10-84ff-4b6f-8391-505dc4aa0435"
      },
      "source": [
        "modelxgb.fit(dat_resaXpcatr, dat_resaYtr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=10, n_splits=5, random_state=111),\n",
              "             estimator=GradientBoostingClassifier(warm_start=True),\n",
              "             param_grid={'criterion': ['friedman_mse', 'mae'],\n",
              "                         'learning_rate': [0.2], 'loss': ['deviance'],\n",
              "                         'max_depth': [3, 8], 'max_features': ['log2', 'sqrt'],\n",
              "                         'min_samples_leaf': array([0.1]),\n",
              "                         'min_samples_split': array([0.1]),\n",
              "                         'n_estimators': [30], 'subsample': [0.8]},\n",
              "             scoring='neg_log_loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQw2dtleUK4d",
        "outputId": "ff654a15-2d67-42be-f0d2-6e5564fbc908"
      },
      "source": [
        "print(cross_val_score(modelxgb.best_estimator_,dat_resaXpcatr, dat_resaYtr,scoring='neg_log_loss',cv=my_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.24799534 -0.22959799 -0.24458441 -0.26454781 -0.25572696 -0.24529758\n",
            " -0.24941348 -0.23583837 -0.26728911 -0.24173287 -0.24605445 -0.27568585\n",
            " -0.26628048 -0.25667343 -0.23278484 -0.26619015 -0.24843875 -0.24973923\n",
            " -0.24208285 -0.24413463 -0.25554283 -0.24669923 -0.24566039 -0.25660443\n",
            " -0.25652263 -0.24916704 -0.26194542 -0.25281543 -0.2596683  -0.26194896\n",
            " -0.25170984 -0.25010429 -0.25162779 -0.25807692 -0.2393608  -0.24825563\n",
            " -0.26307046 -0.24961937 -0.23402335 -0.24385878 -0.25138696 -0.25490008\n",
            " -0.25832481 -0.24308412 -0.2668317  -0.27274562 -0.25892308 -0.24496597\n",
            " -0.26757472 -0.23826949]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcZ3FprkUK4e",
        "outputId": "0fbe4c8c-b893-4905-b68c-d95c8408052f"
      },
      "source": [
        "-modelxgb.best_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2513948786882212"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDoLPhEGUK4e",
        "outputId": "aeed7b33-324f-444a-ca87-74376f3fd8f2"
      },
      "source": [
        "log_loss(dat_resaYts, modelxgb.predict_proba(dat_resaXpcats))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23920610728605515"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEJjYW8MUK4e"
      },
      "source": [
        "modelxgb_best = modelxgb.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRRHgrIpUK4e",
        "outputId": "ca996b4e-5c3c-4e87-ca91-d93ffcddebdb"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYtr, y_pred=modelxgb_best.predict(dat_resaXpcatr)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.98      0.98      0.98      2827\n",
            "   leukaemia       0.96      0.95      0.95      2827\n",
            "    lymphoma       0.97      0.98      0.97      2827\n",
            "      normal       0.94      0.93      0.93      2827\n",
            "\n",
            "    accuracy                           0.96     11308\n",
            "   macro avg       0.96      0.96      0.96     11308\n",
            "weighted avg       0.96      0.96      0.96     11308\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ADnP7_KUK4e",
        "outputId": "f00d63df-9a4b-4076-c24f-c37067a79671"
      },
      "source": [
        "print(classification_report(y_true=dat_resaYts, y_pred=modelxgb_best.predict(dat_resaXpcats)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      breast       0.97      0.97      0.97       943\n",
            "   leukaemia       0.94      0.94      0.94       943\n",
            "    lymphoma       0.97      0.98      0.97       943\n",
            "      normal       0.92      0.91      0.91       943\n",
            "\n",
            "    accuracy                           0.95      3772\n",
            "   macro avg       0.95      0.95      0.95      3772\n",
            "weighted avg       0.95      0.95      0.95      3772\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccVK0RCmL68d"
      },
      "source": [
        "##### fully connected ffnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCs4X7YrL68d",
        "outputId": "706e609d-1e5d-4618-e1b6-19162dedac0a"
      },
      "source": [
        "dat_resaYtr"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['breast', 'normal', 'leukaemia', ..., 'normal', 'normal', 'normal'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIXMohXhL68d"
      },
      "source": [
        "num_classes=4\n",
        "dat_resaYtrx=pd.DataFrame(dat_resaYtr)\n",
        "dat_resaYtrx[0] = pd.Categorical(dat_resaYtrx[0])\n",
        "dat_resaYtrx['code'] = dat_resaYtrx[0].cat.codes\n",
        "dat_resaYtrx_=np.array(dat_resaYtrx['code']\n",
        "                       )\n",
        "dat_resaYtsx=pd.DataFrame(dat_resaYts)\n",
        "dat_resaYtsx[0] = pd.Categorical(dat_resaYtsx[0])\n",
        "dat_resaYtsx['code'] = dat_resaYtsx[0].cat.codes\n",
        "dat_resaYtsx_=np.array(dat_resaYtsx['code'])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIgGiWkUL68d",
        "outputId": "c9ae9355-e256-45f5-e966-166d9b2ef41f"
      },
      "source": [
        "dat_resaYtrcl = to_categorical(dat_resaYtrx_, num_classes)\n",
        "dat_resaYtscl = to_categorical(dat_resaYtsx_, num_classes)\n",
        "dat_resaYtrcl"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNy_x4QWL68d"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(30, activation='relu', input_shape=(100,)))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZNdLW-CL68d"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnjUmpXqL68d",
        "outputId": "3526d50a-cf83-4570-c051-fd7bc2acd07d"
      },
      "source": [
        "history_ = model.fit(dat_resaXpcatr, dat_resaYtrcl, \n",
        "                            batch_size=8000, epochs=500, verbose=1, validation_split=0.2)\n",
        " \n",
        "score_ = model.evaluate(dat_resaXpcats,dat_resaYtscl , verbose=0)\n",
        "print('\\nScore: ', score_)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "2/2 [==============================] - 0s 88ms/step - loss: 3.7747 - accuracy: 0.1010 - val_loss: 4.6979 - val_accuracy: 0.0097\n",
            "Epoch 2/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.5276 - accuracy: 0.1096 - val_loss: 4.3626 - val_accuracy: 0.0128\n",
            "Epoch 3/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.2883 - accuracy: 0.1168 - val_loss: 4.0481 - val_accuracy: 0.0168\n",
            "Epoch 4/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.0657 - accuracy: 0.1217 - val_loss: 3.7581 - val_accuracy: 0.0261\n",
            "Epoch 5/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.8638 - accuracy: 0.1301 - val_loss: 3.4897 - val_accuracy: 0.0336\n",
            "Epoch 6/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.6674 - accuracy: 0.1424 - val_loss: 3.2420 - val_accuracy: 0.0495\n",
            "Epoch 7/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.4729 - accuracy: 0.1616 - val_loss: 3.0148 - val_accuracy: 0.0672\n",
            "Epoch 8/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.2960 - accuracy: 0.1813 - val_loss: 2.8054 - val_accuracy: 0.0827\n",
            "Epoch 9/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.1784 - accuracy: 0.1957 - val_loss: 2.6132 - val_accuracy: 0.1034\n",
            "Epoch 10/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.0450 - accuracy: 0.2188 - val_loss: 2.4370 - val_accuracy: 0.1313\n",
            "Epoch 11/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.8823 - accuracy: 0.2504 - val_loss: 2.2762 - val_accuracy: 0.1490\n",
            "Epoch 12/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.7769 - accuracy: 0.2818 - val_loss: 2.1304 - val_accuracy: 0.1680\n",
            "Epoch 13/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.6842 - accuracy: 0.3016 - val_loss: 1.9969 - val_accuracy: 0.1958\n",
            "Epoch 14/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.5660 - accuracy: 0.3373 - val_loss: 1.8740 - val_accuracy: 0.2237\n",
            "Epoch 15/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.4763 - accuracy: 0.3714 - val_loss: 1.7611 - val_accuracy: 0.2622\n",
            "Epoch 16/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.3940 - accuracy: 0.4017 - val_loss: 1.6583 - val_accuracy: 0.2935\n",
            "Epoch 17/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.3234 - accuracy: 0.4338 - val_loss: 1.5633 - val_accuracy: 0.3267\n",
            "Epoch 18/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.2577 - accuracy: 0.4662 - val_loss: 1.4758 - val_accuracy: 0.3563\n",
            "Epoch 19/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1813 - accuracy: 0.4993 - val_loss: 1.3958 - val_accuracy: 0.3943\n",
            "Epoch 20/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1245 - accuracy: 0.5303 - val_loss: 1.3224 - val_accuracy: 0.4200\n",
            "Epoch 21/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0651 - accuracy: 0.5623 - val_loss: 1.2553 - val_accuracy: 0.4474\n",
            "Epoch 22/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0226 - accuracy: 0.5883 - val_loss: 1.1942 - val_accuracy: 0.4814\n",
            "Epoch 23/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9754 - accuracy: 0.6131 - val_loss: 1.1384 - val_accuracy: 0.5071\n",
            "Epoch 24/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9297 - accuracy: 0.6310 - val_loss: 1.0866 - val_accuracy: 0.5256\n",
            "Epoch 25/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.8887 - accuracy: 0.6597 - val_loss: 1.0383 - val_accuracy: 0.5451\n",
            "Epoch 26/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8709 - accuracy: 0.6697 - val_loss: 0.9936 - val_accuracy: 0.5659\n",
            "Epoch 27/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8189 - accuracy: 0.6866 - val_loss: 0.9527 - val_accuracy: 0.5827\n",
            "Epoch 28/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7991 - accuracy: 0.7046 - val_loss: 0.9146 - val_accuracy: 0.6017\n",
            "Epoch 29/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7584 - accuracy: 0.7223 - val_loss: 0.8792 - val_accuracy: 0.6158\n",
            "Epoch 30/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7399 - accuracy: 0.7320 - val_loss: 0.8463 - val_accuracy: 0.6357\n",
            "Epoch 31/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7094 - accuracy: 0.7471 - val_loss: 0.8153 - val_accuracy: 0.6534\n",
            "Epoch 32/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6843 - accuracy: 0.7646 - val_loss: 0.7859 - val_accuracy: 0.6684\n",
            "Epoch 33/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6652 - accuracy: 0.7656 - val_loss: 0.7576 - val_accuracy: 0.6790\n",
            "Epoch 34/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6470 - accuracy: 0.7747 - val_loss: 0.7307 - val_accuracy: 0.6963\n",
            "Epoch 35/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6245 - accuracy: 0.7845 - val_loss: 0.7048 - val_accuracy: 0.7122\n",
            "Epoch 36/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6032 - accuracy: 0.7978 - val_loss: 0.6800 - val_accuracy: 0.7312\n",
            "Epoch 37/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5800 - accuracy: 0.8038 - val_loss: 0.6564 - val_accuracy: 0.7436\n",
            "Epoch 38/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5524 - accuracy: 0.8142 - val_loss: 0.6338 - val_accuracy: 0.7555\n",
            "Epoch 39/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5488 - accuracy: 0.8140 - val_loss: 0.6126 - val_accuracy: 0.7670\n",
            "Epoch 40/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5243 - accuracy: 0.8245 - val_loss: 0.5927 - val_accuracy: 0.7754\n",
            "Epoch 41/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5127 - accuracy: 0.8308 - val_loss: 0.5740 - val_accuracy: 0.7851\n",
            "Epoch 42/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.4903 - accuracy: 0.8409 - val_loss: 0.5563 - val_accuracy: 0.7931\n",
            "Epoch 43/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4782 - accuracy: 0.8467 - val_loss: 0.5395 - val_accuracy: 0.8019\n",
            "Epoch 44/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4627 - accuracy: 0.8487 - val_loss: 0.5233 - val_accuracy: 0.8152\n",
            "Epoch 45/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4457 - accuracy: 0.8525 - val_loss: 0.5078 - val_accuracy: 0.8236\n",
            "Epoch 46/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4386 - accuracy: 0.8567 - val_loss: 0.4931 - val_accuracy: 0.8307\n",
            "Epoch 47/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4309 - accuracy: 0.8651 - val_loss: 0.4788 - val_accuracy: 0.8364\n",
            "Epoch 48/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4155 - accuracy: 0.8640 - val_loss: 0.4650 - val_accuracy: 0.8439\n",
            "Epoch 49/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3970 - accuracy: 0.8764 - val_loss: 0.4521 - val_accuracy: 0.8528\n",
            "Epoch 50/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3990 - accuracy: 0.8709 - val_loss: 0.4396 - val_accuracy: 0.8581\n",
            "Epoch 51/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3855 - accuracy: 0.8775 - val_loss: 0.4274 - val_accuracy: 0.8652\n",
            "Epoch 52/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3685 - accuracy: 0.8840 - val_loss: 0.4157 - val_accuracy: 0.8722\n",
            "Epoch 53/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3603 - accuracy: 0.8859 - val_loss: 0.4045 - val_accuracy: 0.8802\n",
            "Epoch 54/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3557 - accuracy: 0.8871 - val_loss: 0.3935 - val_accuracy: 0.8868\n",
            "Epoch 55/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3437 - accuracy: 0.8914 - val_loss: 0.3827 - val_accuracy: 0.8890\n",
            "Epoch 56/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3316 - accuracy: 0.8971 - val_loss: 0.3723 - val_accuracy: 0.8935\n",
            "Epoch 57/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3288 - accuracy: 0.8975 - val_loss: 0.3622 - val_accuracy: 0.8948\n",
            "Epoch 58/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3180 - accuracy: 0.9006 - val_loss: 0.3523 - val_accuracy: 0.8970\n",
            "Epoch 59/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3129 - accuracy: 0.9011 - val_loss: 0.3425 - val_accuracy: 0.9032\n",
            "Epoch 60/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3000 - accuracy: 0.9048 - val_loss: 0.3328 - val_accuracy: 0.9067\n",
            "Epoch 61/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2954 - accuracy: 0.9089 - val_loss: 0.3234 - val_accuracy: 0.9085\n",
            "Epoch 62/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2919 - accuracy: 0.9109 - val_loss: 0.3141 - val_accuracy: 0.9125\n",
            "Epoch 63/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2842 - accuracy: 0.9126 - val_loss: 0.3048 - val_accuracy: 0.9151\n",
            "Epoch 64/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2702 - accuracy: 0.9178 - val_loss: 0.2957 - val_accuracy: 0.9204\n",
            "Epoch 65/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2716 - accuracy: 0.9150 - val_loss: 0.2871 - val_accuracy: 0.9235\n",
            "Epoch 66/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2651 - accuracy: 0.9159 - val_loss: 0.2790 - val_accuracy: 0.9279\n",
            "Epoch 67/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2616 - accuracy: 0.9210 - val_loss: 0.2709 - val_accuracy: 0.9315\n",
            "Epoch 68/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2481 - accuracy: 0.9252 - val_loss: 0.2632 - val_accuracy: 0.9337\n",
            "Epoch 69/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2503 - accuracy: 0.9204 - val_loss: 0.2556 - val_accuracy: 0.9368\n",
            "Epoch 70/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2403 - accuracy: 0.9289 - val_loss: 0.2485 - val_accuracy: 0.9390\n",
            "Epoch 71/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2350 - accuracy: 0.9272 - val_loss: 0.2416 - val_accuracy: 0.9403\n",
            "Epoch 72/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2340 - accuracy: 0.9307 - val_loss: 0.2346 - val_accuracy: 0.9408\n",
            "Epoch 73/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2262 - accuracy: 0.9331 - val_loss: 0.2273 - val_accuracy: 0.9434\n",
            "Epoch 74/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2232 - accuracy: 0.9343 - val_loss: 0.2206 - val_accuracy: 0.9456\n",
            "Epoch 75/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2269 - accuracy: 0.9309 - val_loss: 0.2143 - val_accuracy: 0.9483\n",
            "Epoch 76/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2131 - accuracy: 0.9358 - val_loss: 0.2083 - val_accuracy: 0.9492\n",
            "Epoch 77/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2120 - accuracy: 0.9377 - val_loss: 0.2026 - val_accuracy: 0.9514\n",
            "Epoch 78/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2075 - accuracy: 0.9396 - val_loss: 0.1971 - val_accuracy: 0.9527\n",
            "Epoch 79/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2051 - accuracy: 0.9398 - val_loss: 0.1919 - val_accuracy: 0.9536\n",
            "Epoch 80/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1975 - accuracy: 0.9414 - val_loss: 0.1871 - val_accuracy: 0.9545\n",
            "Epoch 81/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1979 - accuracy: 0.9414 - val_loss: 0.1825 - val_accuracy: 0.9549\n",
            "Epoch 82/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1911 - accuracy: 0.9443 - val_loss: 0.1778 - val_accuracy: 0.9562\n",
            "Epoch 83/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1941 - accuracy: 0.9396 - val_loss: 0.1731 - val_accuracy: 0.9576\n",
            "Epoch 84/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1897 - accuracy: 0.9445 - val_loss: 0.1685 - val_accuracy: 0.9593\n",
            "Epoch 85/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1802 - accuracy: 0.9447 - val_loss: 0.1640 - val_accuracy: 0.9607\n",
            "Epoch 86/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1796 - accuracy: 0.9466 - val_loss: 0.1599 - val_accuracy: 0.9615\n",
            "Epoch 87/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1769 - accuracy: 0.9473 - val_loss: 0.1560 - val_accuracy: 0.9624\n",
            "Epoch 88/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1714 - accuracy: 0.9501 - val_loss: 0.1522 - val_accuracy: 0.9629\n",
            "Epoch 89/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1697 - accuracy: 0.9499 - val_loss: 0.1485 - val_accuracy: 0.9633\n",
            "Epoch 90/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1646 - accuracy: 0.9496 - val_loss: 0.1448 - val_accuracy: 0.9642\n",
            "Epoch 91/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1642 - accuracy: 0.9511 - val_loss: 0.1413 - val_accuracy: 0.9651\n",
            "Epoch 92/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1601 - accuracy: 0.9533 - val_loss: 0.1381 - val_accuracy: 0.9655\n",
            "Epoch 93/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1597 - accuracy: 0.9537 - val_loss: 0.1351 - val_accuracy: 0.9664\n",
            "Epoch 94/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1596 - accuracy: 0.9533 - val_loss: 0.1323 - val_accuracy: 0.9673\n",
            "Epoch 95/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1499 - accuracy: 0.9568 - val_loss: 0.1297 - val_accuracy: 0.9673\n",
            "Epoch 96/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1512 - accuracy: 0.9568 - val_loss: 0.1270 - val_accuracy: 0.9673\n",
            "Epoch 97/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1468 - accuracy: 0.9580 - val_loss: 0.1245 - val_accuracy: 0.9686\n",
            "Epoch 98/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1467 - accuracy: 0.9573 - val_loss: 0.1221 - val_accuracy: 0.9699\n",
            "Epoch 99/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1487 - accuracy: 0.9564 - val_loss: 0.1198 - val_accuracy: 0.9699\n",
            "Epoch 100/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1423 - accuracy: 0.9597 - val_loss: 0.1176 - val_accuracy: 0.9704\n",
            "Epoch 101/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1435 - accuracy: 0.9579 - val_loss: 0.1154 - val_accuracy: 0.9708\n",
            "Epoch 102/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1339 - accuracy: 0.9619 - val_loss: 0.1134 - val_accuracy: 0.9721\n",
            "Epoch 103/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1435 - accuracy: 0.9569 - val_loss: 0.1115 - val_accuracy: 0.9721\n",
            "Epoch 104/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1330 - accuracy: 0.9612 - val_loss: 0.1098 - val_accuracy: 0.9730\n",
            "Epoch 105/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1340 - accuracy: 0.9613 - val_loss: 0.1081 - val_accuracy: 0.9735\n",
            "Epoch 106/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1327 - accuracy: 0.9603 - val_loss: 0.1066 - val_accuracy: 0.9739\n",
            "Epoch 107/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1296 - accuracy: 0.9619 - val_loss: 0.1050 - val_accuracy: 0.9744\n",
            "Epoch 108/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1302 - accuracy: 0.9615 - val_loss: 0.1034 - val_accuracy: 0.9744\n",
            "Epoch 109/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1237 - accuracy: 0.9627 - val_loss: 0.1017 - val_accuracy: 0.9739\n",
            "Epoch 110/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1302 - accuracy: 0.9616 - val_loss: 0.1000 - val_accuracy: 0.9748\n",
            "Epoch 111/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1233 - accuracy: 0.9642 - val_loss: 0.0983 - val_accuracy: 0.9757\n",
            "Epoch 112/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1268 - accuracy: 0.9631 - val_loss: 0.0966 - val_accuracy: 0.9761\n",
            "Epoch 113/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1194 - accuracy: 0.9648 - val_loss: 0.0949 - val_accuracy: 0.9766\n",
            "Epoch 114/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1195 - accuracy: 0.9656 - val_loss: 0.0933 - val_accuracy: 0.9766\n",
            "Epoch 115/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1193 - accuracy: 0.9639 - val_loss: 0.0918 - val_accuracy: 0.9770\n",
            "Epoch 116/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1226 - accuracy: 0.9608 - val_loss: 0.0903 - val_accuracy: 0.9775\n",
            "Epoch 117/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1163 - accuracy: 0.9641 - val_loss: 0.0889 - val_accuracy: 0.9779\n",
            "Epoch 118/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1191 - accuracy: 0.9661 - val_loss: 0.0876 - val_accuracy: 0.9783\n",
            "Epoch 119/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1179 - accuracy: 0.9665 - val_loss: 0.0863 - val_accuracy: 0.9797\n",
            "Epoch 120/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1117 - accuracy: 0.9674 - val_loss: 0.0851 - val_accuracy: 0.9801\n",
            "Epoch 121/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1113 - accuracy: 0.9671 - val_loss: 0.0840 - val_accuracy: 0.9801\n",
            "Epoch 122/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1074 - accuracy: 0.9696 - val_loss: 0.0829 - val_accuracy: 0.9801\n",
            "Epoch 123/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1042 - accuracy: 0.9695 - val_loss: 0.0820 - val_accuracy: 0.9801\n",
            "Epoch 124/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1054 - accuracy: 0.9683 - val_loss: 0.0810 - val_accuracy: 0.9801\n",
            "Epoch 125/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1134 - accuracy: 0.9641 - val_loss: 0.0798 - val_accuracy: 0.9801\n",
            "Epoch 126/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1069 - accuracy: 0.9675 - val_loss: 0.0786 - val_accuracy: 0.9801\n",
            "Epoch 127/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1084 - accuracy: 0.9678 - val_loss: 0.0772 - val_accuracy: 0.9805\n",
            "Epoch 128/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1045 - accuracy: 0.9704 - val_loss: 0.0757 - val_accuracy: 0.9814\n",
            "Epoch 129/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1028 - accuracy: 0.9697 - val_loss: 0.0743 - val_accuracy: 0.9814\n",
            "Epoch 130/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1035 - accuracy: 0.9694 - val_loss: 0.0731 - val_accuracy: 0.9819\n",
            "Epoch 131/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1034 - accuracy: 0.9673 - val_loss: 0.0720 - val_accuracy: 0.9823\n",
            "Epoch 132/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1049 - accuracy: 0.9674 - val_loss: 0.0711 - val_accuracy: 0.9823\n",
            "Epoch 133/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0995 - accuracy: 0.9698 - val_loss: 0.0704 - val_accuracy: 0.9828\n",
            "Epoch 134/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1014 - accuracy: 0.9681 - val_loss: 0.0698 - val_accuracy: 0.9828\n",
            "Epoch 135/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0989 - accuracy: 0.9688 - val_loss: 0.0694 - val_accuracy: 0.9828\n",
            "Epoch 136/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0996 - accuracy: 0.9710 - val_loss: 0.0689 - val_accuracy: 0.9823\n",
            "Epoch 137/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0978 - accuracy: 0.9694 - val_loss: 0.0683 - val_accuracy: 0.9828\n",
            "Epoch 138/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0994 - accuracy: 0.9699 - val_loss: 0.0678 - val_accuracy: 0.9828\n",
            "Epoch 139/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0926 - accuracy: 0.9728 - val_loss: 0.0672 - val_accuracy: 0.9823\n",
            "Epoch 140/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0934 - accuracy: 0.9707 - val_loss: 0.0667 - val_accuracy: 0.9828\n",
            "Epoch 141/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0935 - accuracy: 0.9706 - val_loss: 0.0662 - val_accuracy: 0.9828\n",
            "Epoch 142/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0911 - accuracy: 0.9737 - val_loss: 0.0655 - val_accuracy: 0.9828\n",
            "Epoch 143/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0938 - accuracy: 0.9731 - val_loss: 0.0647 - val_accuracy: 0.9828\n",
            "Epoch 144/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0896 - accuracy: 0.9739 - val_loss: 0.0639 - val_accuracy: 0.9836\n",
            "Epoch 145/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0953 - accuracy: 0.9728 - val_loss: 0.0629 - val_accuracy: 0.9836\n",
            "Epoch 146/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0917 - accuracy: 0.9730 - val_loss: 0.0619 - val_accuracy: 0.9845\n",
            "Epoch 147/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0865 - accuracy: 0.9745 - val_loss: 0.0611 - val_accuracy: 0.9850\n",
            "Epoch 148/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0870 - accuracy: 0.9725 - val_loss: 0.0604 - val_accuracy: 0.9854\n",
            "Epoch 149/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0867 - accuracy: 0.9732 - val_loss: 0.0600 - val_accuracy: 0.9854\n",
            "Epoch 150/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0945 - accuracy: 0.9690 - val_loss: 0.0595 - val_accuracy: 0.9854\n",
            "Epoch 151/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0846 - accuracy: 0.9731 - val_loss: 0.0589 - val_accuracy: 0.9854\n",
            "Epoch 152/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0880 - accuracy: 0.9721 - val_loss: 0.0585 - val_accuracy: 0.9859\n",
            "Epoch 153/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0856 - accuracy: 0.9727 - val_loss: 0.0582 - val_accuracy: 0.9854\n",
            "Epoch 154/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0876 - accuracy: 0.9732 - val_loss: 0.0575 - val_accuracy: 0.9859\n",
            "Epoch 155/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0841 - accuracy: 0.9746 - val_loss: 0.0568 - val_accuracy: 0.9863\n",
            "Epoch 156/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0849 - accuracy: 0.9748 - val_loss: 0.0561 - val_accuracy: 0.9867\n",
            "Epoch 157/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0820 - accuracy: 0.9749 - val_loss: 0.0556 - val_accuracy: 0.9872\n",
            "Epoch 158/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0851 - accuracy: 0.9729 - val_loss: 0.0551 - val_accuracy: 0.9876\n",
            "Epoch 159/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0847 - accuracy: 0.9737 - val_loss: 0.0546 - val_accuracy: 0.9872\n",
            "Epoch 160/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0827 - accuracy: 0.9746 - val_loss: 0.0540 - val_accuracy: 0.9872\n",
            "Epoch 161/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0839 - accuracy: 0.9741 - val_loss: 0.0536 - val_accuracy: 0.9872\n",
            "Epoch 162/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0816 - accuracy: 0.9751 - val_loss: 0.0531 - val_accuracy: 0.9863\n",
            "Epoch 163/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0852 - accuracy: 0.9752 - val_loss: 0.0528 - val_accuracy: 0.9859\n",
            "Epoch 164/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0768 - accuracy: 0.9765 - val_loss: 0.0525 - val_accuracy: 0.9859\n",
            "Epoch 165/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0798 - accuracy: 0.9756 - val_loss: 0.0521 - val_accuracy: 0.9863\n",
            "Epoch 166/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0797 - accuracy: 0.9747 - val_loss: 0.0518 - val_accuracy: 0.9854\n",
            "Epoch 167/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0812 - accuracy: 0.9748 - val_loss: 0.0516 - val_accuracy: 0.9850\n",
            "Epoch 168/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0740 - accuracy: 0.9778 - val_loss: 0.0513 - val_accuracy: 0.9850\n",
            "Epoch 169/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0795 - accuracy: 0.9761 - val_loss: 0.0510 - val_accuracy: 0.9850\n",
            "Epoch 170/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0752 - accuracy: 0.9771 - val_loss: 0.0506 - val_accuracy: 0.9850\n",
            "Epoch 171/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0771 - accuracy: 0.9770 - val_loss: 0.0502 - val_accuracy: 0.9850\n",
            "Epoch 172/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0763 - accuracy: 0.9768 - val_loss: 0.0498 - val_accuracy: 0.9854\n",
            "Epoch 173/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0781 - accuracy: 0.9763 - val_loss: 0.0494 - val_accuracy: 0.9859\n",
            "Epoch 174/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0737 - accuracy: 0.9765 - val_loss: 0.0488 - val_accuracy: 0.9867\n",
            "Epoch 175/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0742 - accuracy: 0.9770 - val_loss: 0.0481 - val_accuracy: 0.9867\n",
            "Epoch 176/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0768 - accuracy: 0.9772 - val_loss: 0.0475 - val_accuracy: 0.9876\n",
            "Epoch 177/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0774 - accuracy: 0.9769 - val_loss: 0.0470 - val_accuracy: 0.9881\n",
            "Epoch 178/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0759 - accuracy: 0.9761 - val_loss: 0.0466 - val_accuracy: 0.9885\n",
            "Epoch 179/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0727 - accuracy: 0.9774 - val_loss: 0.0462 - val_accuracy: 0.9889\n",
            "Epoch 180/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0714 - accuracy: 0.9793 - val_loss: 0.0459 - val_accuracy: 0.9889\n",
            "Epoch 181/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0712 - accuracy: 0.9770 - val_loss: 0.0455 - val_accuracy: 0.9889\n",
            "Epoch 182/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0694 - accuracy: 0.9794 - val_loss: 0.0452 - val_accuracy: 0.9894\n",
            "Epoch 183/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0744 - accuracy: 0.9769 - val_loss: 0.0451 - val_accuracy: 0.9898\n",
            "Epoch 184/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0687 - accuracy: 0.9792 - val_loss: 0.0450 - val_accuracy: 0.9894\n",
            "Epoch 185/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0726 - accuracy: 0.9777 - val_loss: 0.0448 - val_accuracy: 0.9894\n",
            "Epoch 186/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0675 - accuracy: 0.9790 - val_loss: 0.0445 - val_accuracy: 0.9889\n",
            "Epoch 187/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0688 - accuracy: 0.9780 - val_loss: 0.0440 - val_accuracy: 0.9894\n",
            "Epoch 188/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0675 - accuracy: 0.9794 - val_loss: 0.0437 - val_accuracy: 0.9894\n",
            "Epoch 189/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0720 - accuracy: 0.9777 - val_loss: 0.0436 - val_accuracy: 0.9898\n",
            "Epoch 190/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0654 - accuracy: 0.9800 - val_loss: 0.0434 - val_accuracy: 0.9898\n",
            "Epoch 191/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0688 - accuracy: 0.9782 - val_loss: 0.0433 - val_accuracy: 0.9894\n",
            "Epoch 192/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0673 - accuracy: 0.9803 - val_loss: 0.0432 - val_accuracy: 0.9894\n",
            "Epoch 193/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0713 - accuracy: 0.9780 - val_loss: 0.0431 - val_accuracy: 0.9894\n",
            "Epoch 194/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0670 - accuracy: 0.9794 - val_loss: 0.0429 - val_accuracy: 0.9894\n",
            "Epoch 195/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0667 - accuracy: 0.9800 - val_loss: 0.0428 - val_accuracy: 0.9894\n",
            "Epoch 196/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0683 - accuracy: 0.9784 - val_loss: 0.0427 - val_accuracy: 0.9889\n",
            "Epoch 197/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0648 - accuracy: 0.9805 - val_loss: 0.0428 - val_accuracy: 0.9885\n",
            "Epoch 198/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0649 - accuracy: 0.9811 - val_loss: 0.0428 - val_accuracy: 0.9881\n",
            "Epoch 199/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0649 - accuracy: 0.9802 - val_loss: 0.0424 - val_accuracy: 0.9885\n",
            "Epoch 200/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0652 - accuracy: 0.9789 - val_loss: 0.0419 - val_accuracy: 0.9894\n",
            "Epoch 201/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0633 - accuracy: 0.9807 - val_loss: 0.0413 - val_accuracy: 0.9898\n",
            "Epoch 202/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0642 - accuracy: 0.9798 - val_loss: 0.0408 - val_accuracy: 0.9898\n",
            "Epoch 203/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0623 - accuracy: 0.9801 - val_loss: 0.0403 - val_accuracy: 0.9907\n",
            "Epoch 204/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0649 - accuracy: 0.9789 - val_loss: 0.0398 - val_accuracy: 0.9912\n",
            "Epoch 205/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0635 - accuracy: 0.9792 - val_loss: 0.0394 - val_accuracy: 0.9912\n",
            "Epoch 206/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0607 - accuracy: 0.9826 - val_loss: 0.0389 - val_accuracy: 0.9912\n",
            "Epoch 207/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0627 - accuracy: 0.9813 - val_loss: 0.0386 - val_accuracy: 0.9912\n",
            "Epoch 208/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0625 - accuracy: 0.9800 - val_loss: 0.0384 - val_accuracy: 0.9907\n",
            "Epoch 209/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0642 - accuracy: 0.9799 - val_loss: 0.0383 - val_accuracy: 0.9907\n",
            "Epoch 210/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0660 - accuracy: 0.9790 - val_loss: 0.0382 - val_accuracy: 0.9903\n",
            "Epoch 211/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0617 - accuracy: 0.9801 - val_loss: 0.0380 - val_accuracy: 0.9903\n",
            "Epoch 212/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0616 - accuracy: 0.9812 - val_loss: 0.0378 - val_accuracy: 0.9903\n",
            "Epoch 213/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0606 - accuracy: 0.9815 - val_loss: 0.0376 - val_accuracy: 0.9907\n",
            "Epoch 214/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0639 - accuracy: 0.9794 - val_loss: 0.0373 - val_accuracy: 0.9907\n",
            "Epoch 215/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0618 - accuracy: 0.9809 - val_loss: 0.0370 - val_accuracy: 0.9912\n",
            "Epoch 216/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0588 - accuracy: 0.9828 - val_loss: 0.0367 - val_accuracy: 0.9912\n",
            "Epoch 217/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0636 - accuracy: 0.9801 - val_loss: 0.0366 - val_accuracy: 0.9916\n",
            "Epoch 218/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0599 - accuracy: 0.9823 - val_loss: 0.0366 - val_accuracy: 0.9912\n",
            "Epoch 219/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0586 - accuracy: 0.9797 - val_loss: 0.0366 - val_accuracy: 0.9912\n",
            "Epoch 220/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0569 - accuracy: 0.9824 - val_loss: 0.0366 - val_accuracy: 0.9907\n",
            "Epoch 221/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0594 - accuracy: 0.9832 - val_loss: 0.0364 - val_accuracy: 0.9907\n",
            "Epoch 222/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0579 - accuracy: 0.9834 - val_loss: 0.0364 - val_accuracy: 0.9907\n",
            "Epoch 223/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0581 - accuracy: 0.9829 - val_loss: 0.0363 - val_accuracy: 0.9907\n",
            "Epoch 224/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0615 - accuracy: 0.9805 - val_loss: 0.0361 - val_accuracy: 0.9907\n",
            "Epoch 225/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0585 - accuracy: 0.9830 - val_loss: 0.0360 - val_accuracy: 0.9907\n",
            "Epoch 226/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0585 - accuracy: 0.9822 - val_loss: 0.0357 - val_accuracy: 0.9907\n",
            "Epoch 227/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0582 - accuracy: 0.9821 - val_loss: 0.0355 - val_accuracy: 0.9907\n",
            "Epoch 228/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0585 - accuracy: 0.9837 - val_loss: 0.0353 - val_accuracy: 0.9907\n",
            "Epoch 229/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0565 - accuracy: 0.9828 - val_loss: 0.0351 - val_accuracy: 0.9916\n",
            "Epoch 230/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0600 - accuracy: 0.9813 - val_loss: 0.0348 - val_accuracy: 0.9916\n",
            "Epoch 231/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0549 - accuracy: 0.9826 - val_loss: 0.0345 - val_accuracy: 0.9916\n",
            "Epoch 232/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0581 - accuracy: 0.9809 - val_loss: 0.0342 - val_accuracy: 0.9916\n",
            "Epoch 233/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0579 - accuracy: 0.9810 - val_loss: 0.0339 - val_accuracy: 0.9916\n",
            "Epoch 234/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0570 - accuracy: 0.9825 - val_loss: 0.0335 - val_accuracy: 0.9916\n",
            "Epoch 235/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0581 - accuracy: 0.9808 - val_loss: 0.0331 - val_accuracy: 0.9916\n",
            "Epoch 236/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0542 - accuracy: 0.9842 - val_loss: 0.0330 - val_accuracy: 0.9916\n",
            "Epoch 237/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0545 - accuracy: 0.9824 - val_loss: 0.0329 - val_accuracy: 0.9916\n",
            "Epoch 238/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0556 - accuracy: 0.9819 - val_loss: 0.0328 - val_accuracy: 0.9916\n",
            "Epoch 239/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0580 - accuracy: 0.9803 - val_loss: 0.0327 - val_accuracy: 0.9916\n",
            "Epoch 240/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0534 - accuracy: 0.9834 - val_loss: 0.0326 - val_accuracy: 0.9916\n",
            "Epoch 241/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0546 - accuracy: 0.9816 - val_loss: 0.0324 - val_accuracy: 0.9916\n",
            "Epoch 242/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0321 - val_accuracy: 0.9916\n",
            "Epoch 243/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0542 - accuracy: 0.9826 - val_loss: 0.0318 - val_accuracy: 0.9916\n",
            "Epoch 244/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0526 - accuracy: 0.9837 - val_loss: 0.0317 - val_accuracy: 0.9916\n",
            "Epoch 245/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0318 - val_accuracy: 0.9916\n",
            "Epoch 246/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0529 - accuracy: 0.9841 - val_loss: 0.0319 - val_accuracy: 0.9916\n",
            "Epoch 247/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0517 - accuracy: 0.9841 - val_loss: 0.0321 - val_accuracy: 0.9916\n",
            "Epoch 248/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0554 - accuracy: 0.9828 - val_loss: 0.0322 - val_accuracy: 0.9916\n",
            "Epoch 249/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0521 - accuracy: 0.9844 - val_loss: 0.0321 - val_accuracy: 0.9916\n",
            "Epoch 250/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0516 - accuracy: 0.9834 - val_loss: 0.0319 - val_accuracy: 0.9916\n",
            "Epoch 251/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0509 - accuracy: 0.9843 - val_loss: 0.0316 - val_accuracy: 0.9916\n",
            "Epoch 252/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0497 - accuracy: 0.9840 - val_loss: 0.0315 - val_accuracy: 0.9916\n",
            "Epoch 253/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0536 - accuracy: 0.9830 - val_loss: 0.0313 - val_accuracy: 0.9916\n",
            "Epoch 254/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0509 - accuracy: 0.9859 - val_loss: 0.0311 - val_accuracy: 0.9916\n",
            "Epoch 255/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0495 - accuracy: 0.9845 - val_loss: 0.0309 - val_accuracy: 0.9916\n",
            "Epoch 256/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0490 - accuracy: 0.9846 - val_loss: 0.0308 - val_accuracy: 0.9916\n",
            "Epoch 257/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0485 - accuracy: 0.9855 - val_loss: 0.0307 - val_accuracy: 0.9916\n",
            "Epoch 258/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0499 - accuracy: 0.9860 - val_loss: 0.0304 - val_accuracy: 0.9920\n",
            "Epoch 259/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0499 - accuracy: 0.9842 - val_loss: 0.0303 - val_accuracy: 0.9920\n",
            "Epoch 260/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0491 - accuracy: 0.9845 - val_loss: 0.0303 - val_accuracy: 0.9920\n",
            "Epoch 261/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0508 - accuracy: 0.9842 - val_loss: 0.0304 - val_accuracy: 0.9916\n",
            "Epoch 262/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0495 - accuracy: 0.9851 - val_loss: 0.0304 - val_accuracy: 0.9916\n",
            "Epoch 263/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0482 - accuracy: 0.9856 - val_loss: 0.0303 - val_accuracy: 0.9916\n",
            "Epoch 264/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0500 - accuracy: 0.9828 - val_loss: 0.0302 - val_accuracy: 0.9916\n",
            "Epoch 265/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0507 - accuracy: 0.9834 - val_loss: 0.0301 - val_accuracy: 0.9916\n",
            "Epoch 266/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0513 - accuracy: 0.9845 - val_loss: 0.0301 - val_accuracy: 0.9916\n",
            "Epoch 267/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0480 - accuracy: 0.9864 - val_loss: 0.0299 - val_accuracy: 0.9916\n",
            "Epoch 268/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0493 - accuracy: 0.9842 - val_loss: 0.0296 - val_accuracy: 0.9920\n",
            "Epoch 269/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0497 - accuracy: 0.9850 - val_loss: 0.0294 - val_accuracy: 0.9920\n",
            "Epoch 270/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0491 - accuracy: 0.9846 - val_loss: 0.0294 - val_accuracy: 0.9916\n",
            "Epoch 271/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0514 - accuracy: 0.9845 - val_loss: 0.0292 - val_accuracy: 0.9916\n",
            "Epoch 272/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0471 - accuracy: 0.9859 - val_loss: 0.0289 - val_accuracy: 0.9916\n",
            "Epoch 273/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0472 - accuracy: 0.9863 - val_loss: 0.0285 - val_accuracy: 0.9920\n",
            "Epoch 274/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0462 - accuracy: 0.9861 - val_loss: 0.0283 - val_accuracy: 0.9920\n",
            "Epoch 275/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0457 - accuracy: 0.9851 - val_loss: 0.0281 - val_accuracy: 0.9920\n",
            "Epoch 276/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0471 - accuracy: 0.9867 - val_loss: 0.0281 - val_accuracy: 0.9920\n",
            "Epoch 277/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0485 - accuracy: 0.9850 - val_loss: 0.0281 - val_accuracy: 0.9920\n",
            "Epoch 278/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0449 - accuracy: 0.9862 - val_loss: 0.0280 - val_accuracy: 0.9920\n",
            "Epoch 279/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0459 - accuracy: 0.9845 - val_loss: 0.0278 - val_accuracy: 0.9920\n",
            "Epoch 280/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0450 - accuracy: 0.9859 - val_loss: 0.0278 - val_accuracy: 0.9920\n",
            "Epoch 281/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0509 - accuracy: 0.9840 - val_loss: 0.0279 - val_accuracy: 0.9920\n",
            "Epoch 282/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0463 - accuracy: 0.9851 - val_loss: 0.0281 - val_accuracy: 0.9920\n",
            "Epoch 283/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0459 - accuracy: 0.9867 - val_loss: 0.0282 - val_accuracy: 0.9920\n",
            "Epoch 284/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0487 - accuracy: 0.9851 - val_loss: 0.0282 - val_accuracy: 0.9916\n",
            "Epoch 285/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0462 - accuracy: 0.9852 - val_loss: 0.0283 - val_accuracy: 0.9916\n",
            "Epoch 286/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0466 - accuracy: 0.9852 - val_loss: 0.0284 - val_accuracy: 0.9916\n",
            "Epoch 287/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0511 - accuracy: 0.9841 - val_loss: 0.0284 - val_accuracy: 0.9916\n",
            "Epoch 288/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0429 - accuracy: 0.9866 - val_loss: 0.0284 - val_accuracy: 0.9916\n",
            "Epoch 289/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0434 - accuracy: 0.9867 - val_loss: 0.0284 - val_accuracy: 0.9916\n",
            "Epoch 290/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0440 - accuracy: 0.9846 - val_loss: 0.0283 - val_accuracy: 0.9916\n",
            "Epoch 291/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0445 - accuracy: 0.9857 - val_loss: 0.0282 - val_accuracy: 0.9916\n",
            "Epoch 292/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0440 - accuracy: 0.9852 - val_loss: 0.0279 - val_accuracy: 0.9916\n",
            "Epoch 293/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0450 - accuracy: 0.9854 - val_loss: 0.0276 - val_accuracy: 0.9920\n",
            "Epoch 294/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0461 - accuracy: 0.9854 - val_loss: 0.0273 - val_accuracy: 0.9920\n",
            "Epoch 295/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0469 - accuracy: 0.9854 - val_loss: 0.0271 - val_accuracy: 0.9920\n",
            "Epoch 296/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0480 - accuracy: 0.9840 - val_loss: 0.0268 - val_accuracy: 0.9920\n",
            "Epoch 297/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0421 - accuracy: 0.9873 - val_loss: 0.0265 - val_accuracy: 0.9920\n",
            "Epoch 298/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0435 - accuracy: 0.9862 - val_loss: 0.0264 - val_accuracy: 0.9920\n",
            "Epoch 299/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0469 - accuracy: 0.9836 - val_loss: 0.0262 - val_accuracy: 0.9920\n",
            "Epoch 300/500\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0440 - accuracy: 0.9857 - val_loss: 0.0260 - val_accuracy: 0.9920\n",
            "Epoch 301/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0466 - accuracy: 0.9840 - val_loss: 0.0258 - val_accuracy: 0.9920\n",
            "Epoch 302/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0428 - accuracy: 0.9864 - val_loss: 0.0255 - val_accuracy: 0.9920\n",
            "Epoch 303/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0422 - accuracy: 0.9862 - val_loss: 0.0253 - val_accuracy: 0.9920\n",
            "Epoch 304/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0456 - accuracy: 0.9865 - val_loss: 0.0252 - val_accuracy: 0.9920\n",
            "Epoch 305/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0448 - accuracy: 0.9864 - val_loss: 0.0251 - val_accuracy: 0.9920\n",
            "Epoch 306/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0422 - accuracy: 0.9867 - val_loss: 0.0252 - val_accuracy: 0.9920\n",
            "Epoch 307/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0424 - accuracy: 0.9863 - val_loss: 0.0252 - val_accuracy: 0.9920\n",
            "Epoch 308/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0430 - accuracy: 0.9867 - val_loss: 0.0253 - val_accuracy: 0.9920\n",
            "Epoch 309/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0416 - accuracy: 0.9873 - val_loss: 0.0252 - val_accuracy: 0.9916\n",
            "Epoch 310/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0427 - accuracy: 0.9872 - val_loss: 0.0251 - val_accuracy: 0.9920\n",
            "Epoch 311/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0437 - accuracy: 0.9856 - val_loss: 0.0250 - val_accuracy: 0.9920\n",
            "Epoch 312/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0427 - accuracy: 0.9854 - val_loss: 0.0247 - val_accuracy: 0.9920\n",
            "Epoch 313/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0413 - accuracy: 0.9874 - val_loss: 0.0245 - val_accuracy: 0.9920\n",
            "Epoch 314/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0394 - accuracy: 0.9876 - val_loss: 0.0242 - val_accuracy: 0.9920\n",
            "Epoch 315/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0433 - accuracy: 0.9852 - val_loss: 0.0239 - val_accuracy: 0.9920\n",
            "Epoch 316/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0376 - accuracy: 0.9886 - val_loss: 0.0238 - val_accuracy: 0.9925\n",
            "Epoch 317/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0424 - accuracy: 0.9863 - val_loss: 0.0237 - val_accuracy: 0.9925\n",
            "Epoch 318/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0396 - accuracy: 0.9875 - val_loss: 0.0237 - val_accuracy: 0.9925\n",
            "Epoch 319/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0409 - accuracy: 0.9870 - val_loss: 0.0237 - val_accuracy: 0.9925\n",
            "Epoch 320/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0419 - accuracy: 0.9875 - val_loss: 0.0236 - val_accuracy: 0.9925\n",
            "Epoch 321/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0417 - accuracy: 0.9862 - val_loss: 0.0235 - val_accuracy: 0.9925\n",
            "Epoch 322/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0401 - accuracy: 0.9870 - val_loss: 0.0234 - val_accuracy: 0.9925\n",
            "Epoch 323/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0391 - accuracy: 0.9887 - val_loss: 0.0233 - val_accuracy: 0.9925\n",
            "Epoch 324/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0420 - accuracy: 0.9854 - val_loss: 0.0232 - val_accuracy: 0.9925\n",
            "Epoch 325/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0394 - accuracy: 0.9881 - val_loss: 0.0233 - val_accuracy: 0.9925\n",
            "Epoch 326/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0380 - accuracy: 0.9880 - val_loss: 0.0235 - val_accuracy: 0.9925\n",
            "Epoch 327/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0404 - accuracy: 0.9863 - val_loss: 0.0238 - val_accuracy: 0.9925\n",
            "Epoch 328/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 0.9877 - val_loss: 0.0239 - val_accuracy: 0.9920\n",
            "Epoch 329/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0398 - accuracy: 0.9856 - val_loss: 0.0240 - val_accuracy: 0.9920\n",
            "Epoch 330/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0355 - accuracy: 0.9895 - val_loss: 0.0239 - val_accuracy: 0.9920\n",
            "Epoch 331/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0395 - accuracy: 0.9880 - val_loss: 0.0238 - val_accuracy: 0.9920\n",
            "Epoch 332/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0416 - accuracy: 0.9873 - val_loss: 0.0236 - val_accuracy: 0.9925\n",
            "Epoch 333/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0406 - accuracy: 0.9875 - val_loss: 0.0234 - val_accuracy: 0.9925\n",
            "Epoch 334/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0398 - accuracy: 0.9883 - val_loss: 0.0231 - val_accuracy: 0.9929\n",
            "Epoch 335/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0352 - accuracy: 0.9881 - val_loss: 0.0228 - val_accuracy: 0.9929\n",
            "Epoch 336/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0388 - accuracy: 0.9868 - val_loss: 0.0223 - val_accuracy: 0.9934\n",
            "Epoch 337/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0399 - accuracy: 0.9874 - val_loss: 0.0219 - val_accuracy: 0.9938\n",
            "Epoch 338/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0405 - accuracy: 0.9867 - val_loss: 0.0216 - val_accuracy: 0.9938\n",
            "Epoch 339/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0379 - accuracy: 0.9878 - val_loss: 0.0214 - val_accuracy: 0.9943\n",
            "Epoch 340/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0394 - accuracy: 0.9866 - val_loss: 0.0213 - val_accuracy: 0.9943\n",
            "Epoch 341/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0386 - accuracy: 0.9880 - val_loss: 0.0212 - val_accuracy: 0.9943\n",
            "Epoch 342/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0378 - accuracy: 0.9873 - val_loss: 0.0212 - val_accuracy: 0.9943\n",
            "Epoch 343/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0400 - accuracy: 0.9860 - val_loss: 0.0212 - val_accuracy: 0.9934\n",
            "Epoch 344/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0390 - accuracy: 0.9878 - val_loss: 0.0211 - val_accuracy: 0.9934\n",
            "Epoch 345/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0385 - accuracy: 0.9878 - val_loss: 0.0212 - val_accuracy: 0.9934\n",
            "Epoch 346/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0362 - accuracy: 0.9894 - val_loss: 0.0212 - val_accuracy: 0.9929\n",
            "Epoch 347/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0361 - accuracy: 0.9888 - val_loss: 0.0210 - val_accuracy: 0.9929\n",
            "Epoch 348/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0364 - accuracy: 0.9884 - val_loss: 0.0209 - val_accuracy: 0.9929\n",
            "Epoch 349/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0398 - accuracy: 0.9883 - val_loss: 0.0208 - val_accuracy: 0.9929\n",
            "Epoch 350/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0377 - accuracy: 0.9872 - val_loss: 0.0206 - val_accuracy: 0.9934\n",
            "Epoch 351/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0337 - accuracy: 0.9895 - val_loss: 0.0204 - val_accuracy: 0.9934\n",
            "Epoch 352/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0346 - accuracy: 0.9888 - val_loss: 0.0203 - val_accuracy: 0.9938\n",
            "Epoch 353/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0376 - accuracy: 0.9880 - val_loss: 0.0202 - val_accuracy: 0.9938\n",
            "Epoch 354/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0375 - accuracy: 0.9878 - val_loss: 0.0201 - val_accuracy: 0.9938\n",
            "Epoch 355/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0371 - accuracy: 0.9877 - val_loss: 0.0199 - val_accuracy: 0.9934\n",
            "Epoch 356/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0361 - accuracy: 0.9877 - val_loss: 0.0199 - val_accuracy: 0.9934\n",
            "Epoch 357/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0360 - accuracy: 0.9886 - val_loss: 0.0201 - val_accuracy: 0.9934\n",
            "Epoch 358/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0372 - accuracy: 0.9887 - val_loss: 0.0201 - val_accuracy: 0.9934\n",
            "Epoch 359/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0373 - accuracy: 0.9885 - val_loss: 0.0200 - val_accuracy: 0.9934\n",
            "Epoch 360/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0375 - accuracy: 0.9884 - val_loss: 0.0201 - val_accuracy: 0.9934\n",
            "Epoch 361/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0359 - accuracy: 0.9886 - val_loss: 0.0200 - val_accuracy: 0.9934\n",
            "Epoch 362/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0362 - accuracy: 0.9881 - val_loss: 0.0200 - val_accuracy: 0.9934\n",
            "Epoch 363/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0363 - accuracy: 0.9881 - val_loss: 0.0199 - val_accuracy: 0.9934\n",
            "Epoch 364/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0347 - accuracy: 0.9891 - val_loss: 0.0198 - val_accuracy: 0.9929\n",
            "Epoch 365/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0351 - accuracy: 0.9887 - val_loss: 0.0198 - val_accuracy: 0.9929\n",
            "Epoch 366/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0338 - accuracy: 0.9889 - val_loss: 0.0198 - val_accuracy: 0.9934\n",
            "Epoch 367/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0341 - accuracy: 0.9886 - val_loss: 0.0197 - val_accuracy: 0.9934\n",
            "Epoch 368/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0359 - accuracy: 0.9878 - val_loss: 0.0197 - val_accuracy: 0.9934\n",
            "Epoch 369/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0328 - accuracy: 0.9896 - val_loss: 0.0199 - val_accuracy: 0.9929\n",
            "Epoch 370/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0341 - accuracy: 0.9891 - val_loss: 0.0201 - val_accuracy: 0.9929\n",
            "Epoch 371/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0340 - accuracy: 0.9896 - val_loss: 0.0205 - val_accuracy: 0.9929\n",
            "Epoch 372/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0347 - accuracy: 0.9878 - val_loss: 0.0207 - val_accuracy: 0.9929\n",
            "Epoch 373/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0370 - accuracy: 0.9875 - val_loss: 0.0208 - val_accuracy: 0.9929\n",
            "Epoch 374/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0339 - accuracy: 0.9888 - val_loss: 0.0207 - val_accuracy: 0.9929\n",
            "Epoch 375/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0364 - accuracy: 0.9883 - val_loss: 0.0204 - val_accuracy: 0.9929\n",
            "Epoch 376/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0344 - accuracy: 0.9899 - val_loss: 0.0201 - val_accuracy: 0.9929\n",
            "Epoch 377/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0361 - accuracy: 0.9885 - val_loss: 0.0199 - val_accuracy: 0.9934\n",
            "Epoch 378/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0343 - accuracy: 0.9894 - val_loss: 0.0198 - val_accuracy: 0.9934\n",
            "Epoch 379/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0353 - accuracy: 0.9882 - val_loss: 0.0196 - val_accuracy: 0.9934\n",
            "Epoch 380/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0354 - accuracy: 0.9887 - val_loss: 0.0196 - val_accuracy: 0.9938\n",
            "Epoch 381/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0330 - accuracy: 0.9887 - val_loss: 0.0194 - val_accuracy: 0.9938\n",
            "Epoch 382/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0365 - accuracy: 0.9886 - val_loss: 0.0193 - val_accuracy: 0.9938\n",
            "Epoch 383/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0311 - accuracy: 0.9902 - val_loss: 0.0191 - val_accuracy: 0.9938\n",
            "Epoch 384/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0344 - accuracy: 0.9896 - val_loss: 0.0188 - val_accuracy: 0.9938\n",
            "Epoch 385/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0351 - accuracy: 0.9891 - val_loss: 0.0187 - val_accuracy: 0.9938\n",
            "Epoch 386/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0320 - accuracy: 0.9907 - val_loss: 0.0184 - val_accuracy: 0.9938\n",
            "Epoch 387/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0323 - accuracy: 0.9898 - val_loss: 0.0182 - val_accuracy: 0.9943\n",
            "Epoch 388/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0325 - accuracy: 0.9904 - val_loss: 0.0181 - val_accuracy: 0.9947\n",
            "Epoch 389/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0300 - accuracy: 0.9917 - val_loss: 0.0180 - val_accuracy: 0.9943\n",
            "Epoch 390/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0323 - accuracy: 0.9895 - val_loss: 0.0179 - val_accuracy: 0.9943\n",
            "Epoch 391/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0330 - accuracy: 0.9901 - val_loss: 0.0177 - val_accuracy: 0.9943\n",
            "Epoch 392/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0343 - accuracy: 0.9894 - val_loss: 0.0175 - val_accuracy: 0.9943\n",
            "Epoch 393/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0328 - accuracy: 0.9891 - val_loss: 0.0175 - val_accuracy: 0.9943\n",
            "Epoch 394/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0333 - accuracy: 0.9895 - val_loss: 0.0177 - val_accuracy: 0.9943\n",
            "Epoch 395/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0332 - accuracy: 0.9887 - val_loss: 0.0180 - val_accuracy: 0.9943\n",
            "Epoch 396/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0338 - accuracy: 0.9898 - val_loss: 0.0183 - val_accuracy: 0.9943\n",
            "Epoch 397/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0318 - accuracy: 0.9897 - val_loss: 0.0186 - val_accuracy: 0.9938\n",
            "Epoch 398/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0331 - accuracy: 0.9895 - val_loss: 0.0190 - val_accuracy: 0.9938\n",
            "Epoch 399/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0303 - accuracy: 0.9907 - val_loss: 0.0192 - val_accuracy: 0.9938\n",
            "Epoch 400/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0324 - accuracy: 0.9897 - val_loss: 0.0192 - val_accuracy: 0.9934\n",
            "Epoch 401/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0333 - accuracy: 0.9892 - val_loss: 0.0190 - val_accuracy: 0.9934\n",
            "Epoch 402/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0322 - accuracy: 0.9892 - val_loss: 0.0188 - val_accuracy: 0.9934\n",
            "Epoch 403/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0317 - accuracy: 0.9906 - val_loss: 0.0184 - val_accuracy: 0.9934\n",
            "Epoch 404/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0320 - accuracy: 0.9902 - val_loss: 0.0180 - val_accuracy: 0.9934\n",
            "Epoch 405/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0341 - accuracy: 0.9895 - val_loss: 0.0178 - val_accuracy: 0.9934\n",
            "Epoch 406/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0310 - accuracy: 0.9901 - val_loss: 0.0175 - val_accuracy: 0.9938\n",
            "Epoch 407/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0308 - accuracy: 0.9908 - val_loss: 0.0174 - val_accuracy: 0.9938\n",
            "Epoch 408/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0316 - accuracy: 0.9896 - val_loss: 0.0174 - val_accuracy: 0.9943\n",
            "Epoch 409/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0333 - accuracy: 0.9891 - val_loss: 0.0174 - val_accuracy: 0.9943\n",
            "Epoch 410/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0296 - accuracy: 0.9907 - val_loss: 0.0173 - val_accuracy: 0.9943\n",
            "Epoch 411/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0317 - accuracy: 0.9902 - val_loss: 0.0173 - val_accuracy: 0.9943\n",
            "Epoch 412/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0299 - accuracy: 0.9906 - val_loss: 0.0173 - val_accuracy: 0.9943\n",
            "Epoch 413/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0324 - accuracy: 0.9891 - val_loss: 0.0172 - val_accuracy: 0.9943\n",
            "Epoch 414/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0304 - accuracy: 0.9913 - val_loss: 0.0172 - val_accuracy: 0.9943\n",
            "Epoch 415/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0316 - accuracy: 0.9898 - val_loss: 0.0172 - val_accuracy: 0.9947\n",
            "Epoch 416/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0334 - accuracy: 0.9896 - val_loss: 0.0172 - val_accuracy: 0.9947\n",
            "Epoch 417/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0313 - accuracy: 0.9897 - val_loss: 0.0174 - val_accuracy: 0.9943\n",
            "Epoch 418/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0284 - accuracy: 0.9917 - val_loss: 0.0174 - val_accuracy: 0.9943\n",
            "Epoch 419/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0320 - accuracy: 0.9893 - val_loss: 0.0175 - val_accuracy: 0.9943\n",
            "Epoch 420/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9923 - val_loss: 0.0175 - val_accuracy: 0.9943\n",
            "Epoch 421/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0295 - accuracy: 0.9903 - val_loss: 0.0175 - val_accuracy: 0.9943\n",
            "Epoch 422/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.0174 - val_accuracy: 0.9943\n",
            "Epoch 423/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0300 - accuracy: 0.9907 - val_loss: 0.0173 - val_accuracy: 0.9943\n",
            "Epoch 424/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0296 - accuracy: 0.9909 - val_loss: 0.0172 - val_accuracy: 0.9943\n",
            "Epoch 425/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0305 - accuracy: 0.9909 - val_loss: 0.0169 - val_accuracy: 0.9943\n",
            "Epoch 426/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0303 - accuracy: 0.9914 - val_loss: 0.0168 - val_accuracy: 0.9943\n",
            "Epoch 427/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0293 - accuracy: 0.9912 - val_loss: 0.0166 - val_accuracy: 0.9947\n",
            "Epoch 428/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.0165 - val_accuracy: 0.9947\n",
            "Epoch 429/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0302 - accuracy: 0.9901 - val_loss: 0.0164 - val_accuracy: 0.9947\n",
            "Epoch 430/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0302 - accuracy: 0.9906 - val_loss: 0.0162 - val_accuracy: 0.9947\n",
            "Epoch 431/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0288 - accuracy: 0.9912 - val_loss: 0.0160 - val_accuracy: 0.9947\n",
            "Epoch 432/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0288 - accuracy: 0.9908 - val_loss: 0.0159 - val_accuracy: 0.9951\n",
            "Epoch 433/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0309 - accuracy: 0.9905 - val_loss: 0.0158 - val_accuracy: 0.9956\n",
            "Epoch 434/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0296 - accuracy: 0.9906 - val_loss: 0.0158 - val_accuracy: 0.9956\n",
            "Epoch 435/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0317 - accuracy: 0.9905 - val_loss: 0.0159 - val_accuracy: 0.9951\n",
            "Epoch 436/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0297 - accuracy: 0.9910 - val_loss: 0.0159 - val_accuracy: 0.9947\n",
            "Epoch 437/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9903 - val_loss: 0.0160 - val_accuracy: 0.9947\n",
            "Epoch 438/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0289 - accuracy: 0.9912 - val_loss: 0.0161 - val_accuracy: 0.9951\n",
            "Epoch 439/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0277 - accuracy: 0.9910 - val_loss: 0.0161 - val_accuracy: 0.9951\n",
            "Epoch 440/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0285 - accuracy: 0.9908 - val_loss: 0.0162 - val_accuracy: 0.9947\n",
            "Epoch 441/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0304 - accuracy: 0.9907 - val_loss: 0.0163 - val_accuracy: 0.9947\n",
            "Epoch 442/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0279 - accuracy: 0.9907 - val_loss: 0.0165 - val_accuracy: 0.9947\n",
            "Epoch 443/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0287 - accuracy: 0.9914 - val_loss: 0.0166 - val_accuracy: 0.9947\n",
            "Epoch 444/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0300 - accuracy: 0.9904 - val_loss: 0.0167 - val_accuracy: 0.9947\n",
            "Epoch 445/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0288 - accuracy: 0.9914 - val_loss: 0.0168 - val_accuracy: 0.9947\n",
            "Epoch 446/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0277 - accuracy: 0.9919 - val_loss: 0.0169 - val_accuracy: 0.9947\n",
            "Epoch 447/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0281 - accuracy: 0.9910 - val_loss: 0.0168 - val_accuracy: 0.9947\n",
            "Epoch 448/500\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0301 - accuracy: 0.9898 - val_loss: 0.0165 - val_accuracy: 0.9947\n",
            "Epoch 449/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0275 - accuracy: 0.9914 - val_loss: 0.0161 - val_accuracy: 0.9947\n",
            "Epoch 450/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0283 - accuracy: 0.9922 - val_loss: 0.0159 - val_accuracy: 0.9947\n",
            "Epoch 451/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0287 - accuracy: 0.9913 - val_loss: 0.0157 - val_accuracy: 0.9956\n",
            "Epoch 452/500\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0284 - accuracy: 0.9915 - val_loss: 0.0155 - val_accuracy: 0.9956\n",
            "Epoch 453/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0281 - accuracy: 0.9910 - val_loss: 0.0154 - val_accuracy: 0.9956\n",
            "Epoch 454/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9912 - val_loss: 0.0154 - val_accuracy: 0.9956\n",
            "Epoch 455/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0259 - accuracy: 0.9916 - val_loss: 0.0154 - val_accuracy: 0.9956\n",
            "Epoch 456/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0278 - accuracy: 0.9902 - val_loss: 0.0154 - val_accuracy: 0.9956\n",
            "Epoch 457/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0262 - accuracy: 0.9925 - val_loss: 0.0155 - val_accuracy: 0.9947\n",
            "Epoch 458/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0278 - accuracy: 0.9904 - val_loss: 0.0157 - val_accuracy: 0.9947\n",
            "Epoch 459/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0273 - accuracy: 0.9906 - val_loss: 0.0158 - val_accuracy: 0.9943\n",
            "Epoch 460/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0252 - accuracy: 0.9930 - val_loss: 0.0158 - val_accuracy: 0.9943\n",
            "Epoch 461/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0265 - accuracy: 0.9924 - val_loss: 0.0158 - val_accuracy: 0.9943\n",
            "Epoch 462/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0264 - accuracy: 0.9925 - val_loss: 0.0157 - val_accuracy: 0.9947\n",
            "Epoch 463/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0263 - accuracy: 0.9923 - val_loss: 0.0155 - val_accuracy: 0.9947\n",
            "Epoch 464/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0273 - accuracy: 0.9915 - val_loss: 0.0154 - val_accuracy: 0.9947\n",
            "Epoch 465/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0264 - accuracy: 0.9910 - val_loss: 0.0153 - val_accuracy: 0.9947\n",
            "Epoch 466/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0268 - accuracy: 0.9917 - val_loss: 0.0153 - val_accuracy: 0.9947\n",
            "Epoch 467/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0253 - accuracy: 0.9916 - val_loss: 0.0155 - val_accuracy: 0.9947\n",
            "Epoch 468/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0273 - accuracy: 0.9914 - val_loss: 0.0155 - val_accuracy: 0.9947\n",
            "Epoch 469/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0289 - accuracy: 0.9899 - val_loss: 0.0155 - val_accuracy: 0.9947\n",
            "Epoch 470/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0270 - accuracy: 0.9910 - val_loss: 0.0154 - val_accuracy: 0.9951\n",
            "Epoch 471/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0240 - accuracy: 0.9926 - val_loss: 0.0152 - val_accuracy: 0.9956\n",
            "Epoch 472/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0259 - accuracy: 0.9925 - val_loss: 0.0151 - val_accuracy: 0.9956\n",
            "Epoch 473/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 0.9925 - val_loss: 0.0151 - val_accuracy: 0.9956\n",
            "Epoch 474/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0271 - accuracy: 0.9924 - val_loss: 0.0151 - val_accuracy: 0.9951\n",
            "Epoch 475/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0245 - accuracy: 0.9924 - val_loss: 0.0151 - val_accuracy: 0.9947\n",
            "Epoch 476/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0249 - accuracy: 0.9931 - val_loss: 0.0151 - val_accuracy: 0.9947\n",
            "Epoch 477/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0261 - accuracy: 0.9909 - val_loss: 0.0149 - val_accuracy: 0.9947\n",
            "Epoch 478/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0264 - accuracy: 0.9923 - val_loss: 0.0147 - val_accuracy: 0.9947\n",
            "Epoch 479/500\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0263 - accuracy: 0.9908 - val_loss: 0.0147 - val_accuracy: 0.9951\n",
            "Epoch 480/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0265 - accuracy: 0.9919 - val_loss: 0.0147 - val_accuracy: 0.9951\n",
            "Epoch 481/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0264 - accuracy: 0.9918 - val_loss: 0.0148 - val_accuracy: 0.9951\n",
            "Epoch 482/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0265 - accuracy: 0.9920 - val_loss: 0.0149 - val_accuracy: 0.9947\n",
            "Epoch 483/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0260 - accuracy: 0.9924 - val_loss: 0.0149 - val_accuracy: 0.9947\n",
            "Epoch 484/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0235 - accuracy: 0.9930 - val_loss: 0.0148 - val_accuracy: 0.9947\n",
            "Epoch 485/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0263 - accuracy: 0.9916 - val_loss: 0.0147 - val_accuracy: 0.9947\n",
            "Epoch 486/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0258 - accuracy: 0.9919 - val_loss: 0.0146 - val_accuracy: 0.9947\n",
            "Epoch 487/500\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0243 - accuracy: 0.9920 - val_loss: 0.0146 - val_accuracy: 0.9947\n",
            "Epoch 488/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0237 - accuracy: 0.9934 - val_loss: 0.0145 - val_accuracy: 0.9947\n",
            "Epoch 489/500\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0255 - accuracy: 0.9916 - val_loss: 0.0145 - val_accuracy: 0.9947\n",
            "Epoch 490/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 0.0145 - val_accuracy: 0.9947\n",
            "Epoch 491/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0243 - accuracy: 0.9933 - val_loss: 0.0145 - val_accuracy: 0.9947\n",
            "Epoch 492/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0239 - accuracy: 0.9926 - val_loss: 0.0146 - val_accuracy: 0.9947\n",
            "Epoch 493/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0245 - accuracy: 0.9927 - val_loss: 0.0145 - val_accuracy: 0.9947\n",
            "Epoch 494/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0239 - accuracy: 0.9926 - val_loss: 0.0144 - val_accuracy: 0.9947\n",
            "Epoch 495/500\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0245 - accuracy: 0.9928 - val_loss: 0.0143 - val_accuracy: 0.9947\n",
            "Epoch 496/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 0.0142 - val_accuracy: 0.9947\n",
            "Epoch 497/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0255 - accuracy: 0.9924 - val_loss: 0.0140 - val_accuracy: 0.9947\n",
            "Epoch 498/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0243 - accuracy: 0.9926 - val_loss: 0.0140 - val_accuracy: 0.9947\n",
            "Epoch 499/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0251 - accuracy: 0.9918 - val_loss: 0.0139 - val_accuracy: 0.9951\n",
            "Epoch 500/500\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0236 - accuracy: 0.9935 - val_loss: 0.0139 - val_accuracy: 0.9951\n",
            "\n",
            "Score:  [0.03475143015384674, 0.9909862279891968]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTQ2zuWCL68f",
        "outputId": "617ce233-41d3-4529-8828-bb7dd13d6801"
      },
      "source": [
        "np.round(model.predict(dat_resaXpcats))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "l-S7XQRTL68f",
        "outputId": "afce0767-7748-4eaa-a80b-562773c1ff43"
      },
      "source": [
        "ffnn_res=pd.DataFrame(dat_resaXpcats)\n",
        "ffnn_res['Disease']=dat_resaYts\n",
        "ffnn_res=pd.concat([ffnn_res,pd.DataFrame(np.round(model.predict(dat_resaXpcats))).astype(int)],axis=1)\n",
        "ffnn_res"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>Disease</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-8.97</td>\n",
              "      <td>23.64</td>\n",
              "      <td>-2.83</td>\n",
              "      <td>-6.95</td>\n",
              "      <td>11.87</td>\n",
              "      <td>-3.77</td>\n",
              "      <td>1.73</td>\n",
              "      <td>-5.61</td>\n",
              "      <td>5.52</td>\n",
              "      <td>-2.12</td>\n",
              "      <td>2.16</td>\n",
              "      <td>6.46</td>\n",
              "      <td>0.65</td>\n",
              "      <td>-3.19</td>\n",
              "      <td>0.16</td>\n",
              "      <td>5.13</td>\n",
              "      <td>0.51</td>\n",
              "      <td>-2.99</td>\n",
              "      <td>2.41</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-1.50</td>\n",
              "      <td>0.85</td>\n",
              "      <td>2.68</td>\n",
              "      <td>-1.19</td>\n",
              "      <td>0.99</td>\n",
              "      <td>2.12</td>\n",
              "      <td>2.27</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>1.12</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>-1.18</td>\n",
              "      <td>1.09</td>\n",
              "      <td>1.99</td>\n",
              "      <td>-0.38</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>1.40</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>0.16</td>\n",
              "      <td>-0.59</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.75e-03</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.92</td>\n",
              "      <td>1.34</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.12</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-0.87</td>\n",
              "      <td>-0.63</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>-1.71</td>\n",
              "      <td>-0.54</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>-2.11</td>\n",
              "      <td>2.01</td>\n",
              "      <td>0.54</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.49</td>\n",
              "      <td>-1.48</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.65e+00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>0.41</td>\n",
              "      <td>-1.29</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>-0.59</td>\n",
              "      <td>1.26</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.75</td>\n",
              "      <td>-0.91</td>\n",
              "      <td>0.57</td>\n",
              "      <td>2.32e-01</td>\n",
              "      <td>breast</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-11.55</td>\n",
              "      <td>-4.77</td>\n",
              "      <td>2.80</td>\n",
              "      <td>-12.24</td>\n",
              "      <td>10.07</td>\n",
              "      <td>-6.09</td>\n",
              "      <td>10.36</td>\n",
              "      <td>-2.13</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>2.24</td>\n",
              "      <td>4.34</td>\n",
              "      <td>0.19</td>\n",
              "      <td>4.06</td>\n",
              "      <td>-5.21</td>\n",
              "      <td>2.93</td>\n",
              "      <td>-4.15</td>\n",
              "      <td>-3.13</td>\n",
              "      <td>3.87</td>\n",
              "      <td>2.55</td>\n",
              "      <td>-1.31</td>\n",
              "      <td>-1.95</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-1.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>-1.69</td>\n",
              "      <td>-0.87</td>\n",
              "      <td>-4.07</td>\n",
              "      <td>-1.53</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-1.88</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>-0.88</td>\n",
              "      <td>-2.61</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.19</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.84</td>\n",
              "      <td>-0.52</td>\n",
              "      <td>-0.82</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>...</td>\n",
              "      <td>1.71e+00</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>0.21</td>\n",
              "      <td>-1.64</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.97</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.69</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.62</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>-0.51</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>-0.62</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.95e-01</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>0.59</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.80</td>\n",
              "      <td>-1.31</td>\n",
              "      <td>0.29</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>-3.67e-02</td>\n",
              "      <td>breast</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.57</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>-4.87</td>\n",
              "      <td>-6.36</td>\n",
              "      <td>-5.78</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>0.82</td>\n",
              "      <td>4.50</td>\n",
              "      <td>-0.27</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.74</td>\n",
              "      <td>0.09</td>\n",
              "      <td>-0.61</td>\n",
              "      <td>-2.78</td>\n",
              "      <td>-0.75</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-3.36</td>\n",
              "      <td>0.69</td>\n",
              "      <td>-4.33</td>\n",
              "      <td>5.67</td>\n",
              "      <td>1.78</td>\n",
              "      <td>-4.07</td>\n",
              "      <td>-0.70</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.94</td>\n",
              "      <td>3.53</td>\n",
              "      <td>-1.33</td>\n",
              "      <td>-2.23</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.29</td>\n",
              "      <td>-2.25</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>0.70</td>\n",
              "      <td>-3.23</td>\n",
              "      <td>-0.28</td>\n",
              "      <td>-1.82</td>\n",
              "      <td>-2.45</td>\n",
              "      <td>-0.42</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.30e-01</td>\n",
              "      <td>-0.27</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.06</td>\n",
              "      <td>1.63</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.75</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.12</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>0.68</td>\n",
              "      <td>-0.51</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>0.31</td>\n",
              "      <td>1.11</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-1.79</td>\n",
              "      <td>1.24</td>\n",
              "      <td>-0.73</td>\n",
              "      <td>7.46e-01</td>\n",
              "      <td>1.09</td>\n",
              "      <td>-0.26</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.73</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.82</td>\n",
              "      <td>0.16</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>-0.89</td>\n",
              "      <td>-1.47e-01</td>\n",
              "      <td>leukaemia</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12.43</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>-1.82</td>\n",
              "      <td>-4.64</td>\n",
              "      <td>-3.14</td>\n",
              "      <td>-7.11</td>\n",
              "      <td>-6.30</td>\n",
              "      <td>-6.21</td>\n",
              "      <td>1.97</td>\n",
              "      <td>2.03</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-3.37</td>\n",
              "      <td>1.02</td>\n",
              "      <td>2.41</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>4.03</td>\n",
              "      <td>-4.20</td>\n",
              "      <td>4.69</td>\n",
              "      <td>-0.80</td>\n",
              "      <td>0.21</td>\n",
              "      <td>-2.14</td>\n",
              "      <td>-0.83</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>-1.55</td>\n",
              "      <td>-1.18</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1.03</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>0.28</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.93</td>\n",
              "      <td>-0.26</td>\n",
              "      <td>-2.26</td>\n",
              "      <td>1.78</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.74</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.64</td>\n",
              "      <td>...</td>\n",
              "      <td>2.06e-01</td>\n",
              "      <td>-2.30</td>\n",
              "      <td>-0.38</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-1.40</td>\n",
              "      <td>0.24</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-1.12</td>\n",
              "      <td>0.22</td>\n",
              "      <td>-1.09</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.16</td>\n",
              "      <td>-0.72</td>\n",
              "      <td>-1.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-7.38e-03</td>\n",
              "      <td>0.57</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.30</td>\n",
              "      <td>-0.88</td>\n",
              "      <td>-0.95</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>1.17</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-4.77e-01</td>\n",
              "      <td>leukaemia</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-13.42</td>\n",
              "      <td>12.00</td>\n",
              "      <td>1.52</td>\n",
              "      <td>-7.41</td>\n",
              "      <td>1.89</td>\n",
              "      <td>20.27</td>\n",
              "      <td>-7.85</td>\n",
              "      <td>9.33</td>\n",
              "      <td>8.80</td>\n",
              "      <td>18.81</td>\n",
              "      <td>-3.86</td>\n",
              "      <td>-2.20</td>\n",
              "      <td>16.12</td>\n",
              "      <td>7.34</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-5.33</td>\n",
              "      <td>2.80</td>\n",
              "      <td>-2.12</td>\n",
              "      <td>3.49</td>\n",
              "      <td>-4.72</td>\n",
              "      <td>-4.09</td>\n",
              "      <td>-1.85</td>\n",
              "      <td>-4.35</td>\n",
              "      <td>0.44</td>\n",
              "      <td>1.43</td>\n",
              "      <td>0.91</td>\n",
              "      <td>1.22</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>-1.53</td>\n",
              "      <td>-1.76</td>\n",
              "      <td>-1.10</td>\n",
              "      <td>-2.44</td>\n",
              "      <td>0.41</td>\n",
              "      <td>-0.85</td>\n",
              "      <td>-0.95</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.84</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.51e+00</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-1.86</td>\n",
              "      <td>-0.85</td>\n",
              "      <td>-1.87</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-1.68</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.09</td>\n",
              "      <td>-2.56</td>\n",
              "      <td>-1.69</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.81</td>\n",
              "      <td>1.62</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.02</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.99</td>\n",
              "      <td>1.41</td>\n",
              "      <td>0.33</td>\n",
              "      <td>-1.16e+00</td>\n",
              "      <td>-0.92</td>\n",
              "      <td>0.41</td>\n",
              "      <td>-0.62</td>\n",
              "      <td>0.85</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.66</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>0.46</td>\n",
              "      <td>1.20</td>\n",
              "      <td>5.57e-01</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3767</th>\n",
              "      <td>15.39</td>\n",
              "      <td>6.05</td>\n",
              "      <td>3.04</td>\n",
              "      <td>-2.51</td>\n",
              "      <td>-11.49</td>\n",
              "      <td>-11.66</td>\n",
              "      <td>0.56</td>\n",
              "      <td>6.77</td>\n",
              "      <td>5.28</td>\n",
              "      <td>7.02</td>\n",
              "      <td>9.97</td>\n",
              "      <td>-2.34</td>\n",
              "      <td>1.29</td>\n",
              "      <td>-1.67</td>\n",
              "      <td>2.20</td>\n",
              "      <td>-3.12</td>\n",
              "      <td>3.40</td>\n",
              "      <td>-6.42</td>\n",
              "      <td>-5.50</td>\n",
              "      <td>-3.22</td>\n",
              "      <td>1.53</td>\n",
              "      <td>4.37</td>\n",
              "      <td>4.68</td>\n",
              "      <td>-6.82</td>\n",
              "      <td>-3.42</td>\n",
              "      <td>-0.44</td>\n",
              "      <td>1.56</td>\n",
              "      <td>-0.89</td>\n",
              "      <td>-4.19</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>0.14</td>\n",
              "      <td>-2.58</td>\n",
              "      <td>1.04</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>2.53</td>\n",
              "      <td>4.70</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>1.57</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.02</td>\n",
              "      <td>...</td>\n",
              "      <td>2.29e+00</td>\n",
              "      <td>0.33</td>\n",
              "      <td>-1.23</td>\n",
              "      <td>-1.01</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>-1.14</td>\n",
              "      <td>-0.58</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.88</td>\n",
              "      <td>-0.93</td>\n",
              "      <td>-0.94</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.08</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>1.62</td>\n",
              "      <td>-0.42</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-1.07</td>\n",
              "      <td>3.05e-01</td>\n",
              "      <td>1.25</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.36</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.96</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>0.38</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>0.43</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>9.85e-03</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3768</th>\n",
              "      <td>11.97</td>\n",
              "      <td>-1.23</td>\n",
              "      <td>20.39</td>\n",
              "      <td>5.89</td>\n",
              "      <td>-10.71</td>\n",
              "      <td>-8.86</td>\n",
              "      <td>4.29</td>\n",
              "      <td>8.33</td>\n",
              "      <td>4.21</td>\n",
              "      <td>-2.02</td>\n",
              "      <td>5.71</td>\n",
              "      <td>1.65</td>\n",
              "      <td>3.96</td>\n",
              "      <td>0.60</td>\n",
              "      <td>-0.83</td>\n",
              "      <td>1.95</td>\n",
              "      <td>3.33</td>\n",
              "      <td>-4.39</td>\n",
              "      <td>1.03</td>\n",
              "      <td>-4.60</td>\n",
              "      <td>6.61</td>\n",
              "      <td>1.70</td>\n",
              "      <td>2.48</td>\n",
              "      <td>1.09</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>1.62</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>2.32</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>1.54</td>\n",
              "      <td>-2.26</td>\n",
              "      <td>3.50</td>\n",
              "      <td>-3.12</td>\n",
              "      <td>2.05</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.37</td>\n",
              "      <td>-2.34</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.42e+00</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>3.34</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>1.14</td>\n",
              "      <td>1.96</td>\n",
              "      <td>-1.37</td>\n",
              "      <td>1.83</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>2.79</td>\n",
              "      <td>-1.82</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-0.26</td>\n",
              "      <td>-2.15</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>1.17</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>0.34</td>\n",
              "      <td>-1.76</td>\n",
              "      <td>0.17</td>\n",
              "      <td>-2.30</td>\n",
              "      <td>1.72</td>\n",
              "      <td>-3.65e+00</td>\n",
              "      <td>1.12</td>\n",
              "      <td>-0.84</td>\n",
              "      <td>1.18</td>\n",
              "      <td>-1.40</td>\n",
              "      <td>-0.48</td>\n",
              "      <td>-1.29</td>\n",
              "      <td>2.01</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.21</td>\n",
              "      <td>2.73</td>\n",
              "      <td>-0.84</td>\n",
              "      <td>1.77e+00</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3769</th>\n",
              "      <td>-9.74</td>\n",
              "      <td>5.23</td>\n",
              "      <td>4.25</td>\n",
              "      <td>0.73</td>\n",
              "      <td>-7.95</td>\n",
              "      <td>1.45</td>\n",
              "      <td>-4.57</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-6.50</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-1.01</td>\n",
              "      <td>0.48</td>\n",
              "      <td>-2.34</td>\n",
              "      <td>-2.34</td>\n",
              "      <td>-2.29</td>\n",
              "      <td>1.31</td>\n",
              "      <td>-0.42</td>\n",
              "      <td>-0.63</td>\n",
              "      <td>1.17</td>\n",
              "      <td>1.52</td>\n",
              "      <td>4.51</td>\n",
              "      <td>-1.43</td>\n",
              "      <td>1.30</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-3.53</td>\n",
              "      <td>1.79</td>\n",
              "      <td>1.51</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-2.04</td>\n",
              "      <td>-0.89</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.64</td>\n",
              "      <td>-1.05</td>\n",
              "      <td>2.11</td>\n",
              "      <td>1.36</td>\n",
              "      <td>1.32</td>\n",
              "      <td>-1.77</td>\n",
              "      <td>...</td>\n",
              "      <td>1.06e+00</td>\n",
              "      <td>-1.43</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.53</td>\n",
              "      <td>-0.42</td>\n",
              "      <td>0.38</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>0.27</td>\n",
              "      <td>1.61</td>\n",
              "      <td>-0.91</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>0.11</td>\n",
              "      <td>-0.42</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.62</td>\n",
              "      <td>1.00</td>\n",
              "      <td>-0.28</td>\n",
              "      <td>0.15</td>\n",
              "      <td>3.45e-01</td>\n",
              "      <td>-0.48</td>\n",
              "      <td>1.48</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>0.39</td>\n",
              "      <td>-1.07</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>1.47</td>\n",
              "      <td>1.11</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.23</td>\n",
              "      <td>1.98e-02</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3770</th>\n",
              "      <td>-9.62</td>\n",
              "      <td>10.64</td>\n",
              "      <td>13.45</td>\n",
              "      <td>-3.07</td>\n",
              "      <td>4.33</td>\n",
              "      <td>17.70</td>\n",
              "      <td>-4.50</td>\n",
              "      <td>9.48</td>\n",
              "      <td>9.19</td>\n",
              "      <td>16.09</td>\n",
              "      <td>-5.83</td>\n",
              "      <td>-7.39</td>\n",
              "      <td>16.99</td>\n",
              "      <td>8.73</td>\n",
              "      <td>-2.98</td>\n",
              "      <td>-3.40</td>\n",
              "      <td>-0.26</td>\n",
              "      <td>-0.95</td>\n",
              "      <td>1.88</td>\n",
              "      <td>-3.38</td>\n",
              "      <td>-3.27</td>\n",
              "      <td>-0.46</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>1.84</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.11</td>\n",
              "      <td>2.31</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>-3.67</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>-1.68</td>\n",
              "      <td>-0.51</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>-0.16</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.18</td>\n",
              "      <td>-2.07</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.37e+00</td>\n",
              "      <td>-0.85</td>\n",
              "      <td>-2.42</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>-1.60</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.90</td>\n",
              "      <td>-2.22</td>\n",
              "      <td>-1.51</td>\n",
              "      <td>0.46</td>\n",
              "      <td>1.31</td>\n",
              "      <td>1.11</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.64</td>\n",
              "      <td>-0.91</td>\n",
              "      <td>1.06</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.66</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-2.34e-01</td>\n",
              "      <td>-0.89</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>0.82</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.14</td>\n",
              "      <td>-0.49</td>\n",
              "      <td>0.72</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>0.69</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>4.07e-02</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3771</th>\n",
              "      <td>-12.96</td>\n",
              "      <td>14.81</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-5.60</td>\n",
              "      <td>4.95</td>\n",
              "      <td>2.71</td>\n",
              "      <td>-2.93</td>\n",
              "      <td>4.16</td>\n",
              "      <td>1.45</td>\n",
              "      <td>4.25</td>\n",
              "      <td>0.41</td>\n",
              "      <td>4.18</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>2.16</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>2.43</td>\n",
              "      <td>-1.82</td>\n",
              "      <td>3.88</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.73</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-1.49</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.48</td>\n",
              "      <td>-1.65</td>\n",
              "      <td>1.02</td>\n",
              "      <td>-2.17</td>\n",
              "      <td>1.04</td>\n",
              "      <td>0.33</td>\n",
              "      <td>-2.09</td>\n",
              "      <td>3.51</td>\n",
              "      <td>1.57</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-3.14</td>\n",
              "      <td>3.06</td>\n",
              "      <td>1.11</td>\n",
              "      <td>2.00</td>\n",
              "      <td>-2.83</td>\n",
              "      <td>0.97</td>\n",
              "      <td>1.24</td>\n",
              "      <td>...</td>\n",
              "      <td>3.11e-01</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>1.34</td>\n",
              "      <td>1.38</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>0.66</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>2.73</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>-2.22</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-2.52</td>\n",
              "      <td>1.90</td>\n",
              "      <td>0.18</td>\n",
              "      <td>1.11</td>\n",
              "      <td>1.58</td>\n",
              "      <td>1.42</td>\n",
              "      <td>-2.27</td>\n",
              "      <td>2.48</td>\n",
              "      <td>2.86</td>\n",
              "      <td>0.89</td>\n",
              "      <td>-8.83e-01</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>5.32</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>1.38</td>\n",
              "      <td>1.57</td>\n",
              "      <td>0.56</td>\n",
              "      <td>-0.23</td>\n",
              "      <td>1.18</td>\n",
              "      <td>1.10</td>\n",
              "      <td>2.36</td>\n",
              "      <td>-1.73</td>\n",
              "      <td>6.23e-01</td>\n",
              "      <td>normal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3772 rows × 105 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1      2      3      4      5  ...        99    Disease  0  1  2  3\n",
              "0     -8.97  23.64  -2.83  -6.95  11.87  -3.77  ...  2.32e-01     breast  1  0  0  0\n",
              "1    -11.55  -4.77   2.80 -12.24  10.07  -6.09  ... -3.67e-02     breast  1  0  0  0\n",
              "2      9.57   0.37   0.01  -0.24  -4.87  -6.36  ... -1.47e-01  leukaemia  0  1  0  0\n",
              "3     12.43  -1.04  -1.82  -4.64  -3.14  -7.11  ... -4.77e-01  leukaemia  0  1  0  0\n",
              "4    -13.42  12.00   1.52  -7.41   1.89  20.27  ...  5.57e-01     normal  0  0  0  1\n",
              "...     ...    ...    ...    ...    ...    ...  ...       ...        ... .. .. .. ..\n",
              "3767  15.39   6.05   3.04  -2.51 -11.49 -11.66  ...  9.85e-03     normal  0  0  0  1\n",
              "3768  11.97  -1.23  20.39   5.89 -10.71  -8.86  ...  1.77e+00     normal  0  0  0  1\n",
              "3769  -9.74   5.23   4.25   0.73  -7.95   1.45  ...  1.98e-02     normal  0  0  0  1\n",
              "3770  -9.62  10.64  13.45  -3.07   4.33  17.70  ...  4.07e-02     normal  0  0  0  1\n",
              "3771 -12.96  14.81   0.15  -5.60   4.95   2.71  ...  6.23e-01     normal  0  0  0  1\n",
              "\n",
              "[3772 rows x 105 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHdz8NB5yiKy"
      },
      "source": [
        "[Project presentation](https://)"
      ]
    }
  ]
}